{
  "output_file": "NnamdiOdozi-batch_summary_doubleword.txt",
  "summary": "This repository is a Python-based CLI pipeline for batch document summarization using the Doubleword API and open-weight models. It supports multiple document formats\u2014PDF, DOCX, PPTX, ODP, TXT, and MD\u2014and automates the extraction, submission, and processing of summaries through a three-stage workflow. The pipeline leverages Python 3.12+ and integrates with the OpenAI-compatible Doubleword API, utilizing libraries such as `pypdf`, `pdfplumber`, `python-docx`, `python-pptx`, and `odfpy` for document parsing. The core logic is structured around a modular script design, with each stage encapsulated in a separate Python file, and orchestrated via a main script that runs the entire workflow end-to-end. The system is designed for high-efficiency batch processing, particularly suited for academic or technical literature reviews, regulatory analysis, or compliance tasks.\n\nKey features include:\n- **Document extraction** via `create_batch.py`, which supports multiple formats with fallback mechanisms (e.g., `pypdf` for speed, `pdfplumber` for robustness).\n- **Batch submission** via `submit_batch.py`, which uploads JSONL-formatted requests to the Doubleword API and stores the job ID.\n- **Polling and processing** via `poll_and_process.py`, which monitors job status and triggers `process_results.py` to download and save summaries.\n- **Configurable summarization prompt** in `summarisation_prompt.txt`, allowing customization of output structure, technical depth, and markdown formatting.\n- **Environment-based configuration** via `.env`, enabling control over model selection, polling intervals, and summary length.\n\nTo run the pipeline:\n1. Clone the repository and install dependencies using `uv sync` or `pip install -r requirements.txt`.\n2. Copy `.env.sample` to `.env` and fill in the Doubleword API token and other settings.\n3. Place documents in `data/papers/` or specify a custom directory using `--input-dir`.\n4. Run the full pipeline with `python run_batch_pipeline.py` or execute stages individually using the respective scripts.\n\nThe repository structure includes a minimal set of files: six main Python scripts, a prompt template, configuration files, and a `pyproject.toml` for dependency management. The project consists of 10 files, with a total of approximately 22,000 bytes read across the repository. Languages used are primarily Python (95%) with minimal support from shell scripts and text files.\n\nQuality signals include a clear README with usage instructions, environment configuration, and troubleshooting. However, the repository lacks automated tests, CI/CD workflows, and documentation beyond the README. The license is MIT, and all necessary dependencies are listed in both `requirements.txt` and `pyproject.toml`. The intended audience includes researchers, analysts, and developers working with bulk document processing, particularly in actuarial science, machine learning, or regulatory compliance. Use cases range from literature reviews to compliance analysis.\n\nClear unknowns include the actual performance of the pipeline with large-scale datasets, the exact token cost per request, and the availability of alternative models beyond Qwen. Additionally, the repository does not include error handling for network interruptions or API rate limits, nor does it provide logging beyond console output. The long-term maintenance of the Doubleword API and its compatibility with the OpenAI SDK remains uncertain."
}