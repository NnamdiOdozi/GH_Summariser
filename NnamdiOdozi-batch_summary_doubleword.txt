Directory structure:
└── nnamdiodozi-batch_summary_doubleword/
    ├── README.md
    ├── create_batch.py
    ├── LICENSE
    ├── poll_and_process.py
    ├── process_results.py
    ├── pyproject.toml
    ├── requirements.txt
    ├── run_batch_pipeline.py
    ├── submit_batch.py
    ├── summarisation_prompt.txt
    └── .env.sample

================================================
FILE: README.md
================================================
# Batch Document Summarization with Doubleword API

A simple Python CLI Pipeline for batch processing documents (PDF, DOCX, PPTX, TXT, MD, ODP) into structured summaries using the ultra low-cost Doubleword API and open-weight models. Just load your docs, a prompt, and then the Doubleword API key and you're good to go.

## Overview

This tool extracts text from multiple document formats and generates comprehensive ~2000-word structured summaries using Doubleword's batch inference API. Originally built for literature reviews in actuarial machine learning research, it can be adapted for any bulk document summarization task.

## Use Cases

- **Literature reviews** - Summarize academic papers systematically
- **Regulatory analysis** - Convert 200-page consultation papers into actionable digests
- **Compliance** - Extract structured data from policy documents at scale
- **Sentiment analysis** - Process customer feedback documents in bulk
- **Research synthesis** - Analyze collections of technical reports
- **LLM/Agent Evaluations** - Use LLM as a Judge to evaluate LLM and Agent outputs

## Performance

**Real-world results:**
- **Initial test:** 2 papers processed in ~1 minute
- **Production run:** 33 papers processed in ~30 minutes
- **Total cost:** ~15 pence for 35 papers
- **SLA:** Selected 24-hour window, actual delivery < 30 minutes

## Supported File Formats

- **PDF** (`.pdf`) - Research papers, reports, articles
- **Microsoft Word** (`.docx`) - Documents, proposals
- **Microsoft PowerPoint** (`.pptx`) - Presentations, slide decks
- **OpenDocument Presentation** (`.odp`) - Open format presentations
- **Plain Text** (`.txt`) - Text documents
- **Markdown** (`.md`) - Technical documentation, notes

All formats are processed through the same pipeline with automatic file type detection.

## How It Works

The pipeline consists of three stages:

### Stage 1: Document Extraction & Batch Request Creation
**Script:** `create_batch.py`

- Scans `data/papers/` folder (or custom location via `--input-dir`)
- Extracts text from multiple formats:
  - **PDF:** pypdf (fast) with pdfplumber fallback (robust)
  - **DOCX:** python-docx
  - **PPTX:** python-pptx
  - **ODP:** odfpy
  - **TXT/MD:** Direct text read
- Creates structured JSONL batch requests with custom summarization prompt
- Outputs: `batch_requests_{timestamp}.jsonl`

### Stage 2: Batch Submission
**Script:** `submit_batch.py`

- Uploads `batch_requests.jsonl` to Doubleword API
- Creates batch job with 1-hour completion window
- Saves batch ID to `batch_id.txt` for tracking
- Outputs: Batch ID for monitoring

### Stage 3: Polling & Processing
**Script:** `poll_and_process.py`

- Polls batch job status at configurable intervals (default: 60 seconds)
- Automatically downloads results when completed
- Calls `process_results.py` to extract and save individual summaries
- Outputs: Individual markdown summaries in `data/summaries/`

### Processing Results
**Script:** `process_results.py`

- Downloads batch output file from Doubleword API
- Parses JSONL responses
- Saves each summary as timestamped markdown file
- Format: `{filename}_summary_{timestamp}.md`

## Setup

### 1. Install Dependencies
git clone https://github.com/NnamdiOdozi/batch_summary_doubleword.git

Using uv (recommended):
```bash
uv sync
source .venv/bin/activate  # Linux/macOS
# OR on Windows: .venv\Scripts\activate
```

Or using pip:
```bash
python3 -m venv .venv
source .venv/bin/activate  # Linux/macOS  
# OR on Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

**Requirements** ([requirements.txt](requirements.txt)):
- `pypdf>=6.6.0` - Fast PDF text extraction
- `pdfplumber>=0.11.9` - Robust fallback for complex PDFs
- `python-docx>=1.1.0` - Microsoft Word document extraction
- `python-pptx>=1.0.0` - PowerPoint presentation extraction
- `odfpy>=1.4.1` - OpenDocument format extraction
- `openai>=2.14.0` - API client (compatible with Doubleword API)
- `python-dotenv>=1.1.0` - Environment variable management

### 2. Configure Environment Variables

Copy the sample environment file:
```bash
cp .env.sample .env
```

Edit `.env` and fill in your credentials:
```bash
# Your Doubleword API token
DOUBLEWORD_AUTH_TOKEN=your_api_token_here

# Doubleword API endpoint
DOUBLEWORD_BASE_URL=https://api.doubleword.ai/v1

# API endpoint for chat completions (relative to base URL)
CHAT_COMPLETIONS_ENDPOINT=/v1/chat/completions

# Model to use
DOUBLEWORD_MODEL=Qwen/Qwen3-VL-235B-A22B-Instruct-FP8
or any other model you would like eg the smaller and cheaper Qwen/Qwen3-VL-30B-A3B-Instruct-FP8

# Polling frequency (seconds)
POLLING_INTERVAL=60

# Batch completion window or SLA (how long the API has to complete the job)
# Options: "1h" or "24h"
COMPLETION_WINDOW=1h

# Summary word count (target length for generated summaries)
SUMMARY_WORD_COUNT=2000

# Maximum tokens for model response (includes reasoning + summary)
MAX_TOKENS=5000
```

**Get your API key:**
1. Visit [Doubleword Portal](https://doubleword.ai)
2. Click to join Private Preview
3. Create account or log in
4. Generate API key in settings

### 3. Add Your Documents 
      
Place documents in:
- `data/papers/` folder

Supported formats: PDF, DOCX, PPTX, ODP, TXT, MD     

The pipeline will automatically detect and process all supported files in this directory.

### 4. Customize Summarization (Optional)

**Adjust word count:**
Edit `SUMMARY_WORD_COUNT` in `.env` to change summary length (default: 2000 words)

**Customize prompt template:**
Edit [summarisation_prompt.txt](summarisation_prompt.txt) to adjust:
- Output structure and fields
- Technical complexity level
- Markdown formatting
- Required fields

## Usage

### Quick Start - Run Full Pipeline

```bash
python run_batch_pipeline.py
```

This orchestrator script runs all three stages automatically:
1. Extracts documents and creates batch requests
2. Submits to Doubleword API
3. Polls until complete and downloads summaries

### Command Line Options

**Process all files in default directory:**
```bash
python run_batch_pipeline.py
```

**Process specific files:**
```bash
python run_batch_pipeline.py --files paper1.pdf report.docx slides.pptx
```

**Process files from custom directory:**
```bash
python run_batch_pipeline.py --input-dir /path/to/documents/
```

**View all options:**
```bash
python run_batch_pipeline.py --help
python create_batch.py --help
```

### Manual Step-by-Step

If you prefer to run stages individually:

**Stage 1: Create batch requests (all files in data/papers/)**
```bash
python create_batch.py
```

Or process specific files:
```bash
python create_batch.py --files doc1.pdf doc2.docx
```

Or process custom directory:
```bash
python create_batch.py --input-dir /custom/path/
```

Output: `batch_requests_{timestamp}.jsonl`

**Stage 2: Submit batch**
```bash
python submit_batch.py
```
Output: `batch_id.txt` with job ID

**Stage 3: Poll and process**
```bash
python poll_and_process.py
```
Output: Individual summaries in `data/summaries/`

### Monitoring Progress

The polling script shows real-time status:
```
[2026-01-25 14:32:15] Status: in_progress | Progress: 12/35
[2026-01-25 14:32:45] Status: in_progress | Progress: 24/35
[2026-01-25 14:33:15] Status: completed | Progress: 35/35

✓ Batch completed successfully!
```

Press `Ctrl+C` to stop polling. Run the script again to resume.

## Project Structure

```
batch_summary_doubleword/
├── README.md                           # This file
├── pyproject.toml                      # Python dependencies (uv)
├── requirements.txt                    # Python dependencies (pip)
├── .env.sample                         # Environment variable template
├── .gitignore                          # Git ignore rules
├── run_batch_pipeline.py               # Orchestrator script (Python)
├── summarisation_prompt.txt            # Prompt template for summaries
├── create_batch.py     # Stage 1: PDF extraction
├── submit_batch.py                     # Stage 2: Batch submission
├── poll_and_process.py                 # Stage 3: Polling and processing
├── process_results.py                  # Result processing
└── data/
    ├── papers/                         # Input PDFs
    └── summaries/                      # Output summaries (auto-created)
```

**Generated files (not in git):**
- `batch_requests_YYYYMMDD_HHMMSS.jsonl` - JSONL file with timestamped batch requests
- `batch_id_YYYYMMDD_HHMMSS.txt` - Timestamped batch job ID
- `data/summaries/*.md` - Individual paper summaries

## Configuration Options

### Polling Interval

Adjust how frequently the script checks batch status:

```bash
# In .env file
POLLING_INTERVAL=60  # Check every 60 seconds
```

Lower values = faster notification, more API calls
Higher values = fewer API calls, slower notification

**Recommended:** 30-60 seconds for most use cases

### Model Selection

The default model is `Qwen/Qwen3-VL-235B-A22B-Instruct-FP8`, which supports:
- Long context windows (128K+ tokens)
- Vision capabilities (for PDFs with charts/diagrams)
- Structured output generation

To use a different model, update `DOUBLEWORD_MODEL` in `.env`.

### Completion Window / SLA

The batch job completion window determines how long the API has to complete your job. Configure via `COMPLETION_WINDOW` in `.env`:

```bash
COMPLETION_WINDOW=1h  # Options: "1h" or "24h"
```

Doubleword typically completes jobs much faster than the window:
- 2 papers: ~1 minute
- 35 papers: ~30 minutes

Use `1h` for most cases. Use `24h` if you want even cheaper pricing and if task is not as time critical.

## Cost Estimation

Based on actual usage (Jan 2026):
- **35 papers** (mixed lengths, 45-200 pages each)
- **Model:** Qwen3-VL-235B-A22B-Instruct-FP8
- **Cost:** ~15 pence total (~0.43p per paper)

Cost varies by:
- Document length
- Requested summary length
- Model selected
- Number of requests

## Troubleshooting

### Authentication Errors

```
Error: Unauthorized
```
**Solution:** Check your `DOUBLEWORD_AUTH_TOKEN` in `.env`

### Batch Takes Too Long

**Solution:** Doubleword typically completes in ~1 minute. If waiting longer:
1. Check Doubleword portal for job status
2. Verify your completion window setting
3. Contact Doubleword support if job is stuck

### Process Results Error

```
✗ Error processing results
```
**Solution:** Check that `process_results.py` has correct permissions and paths

## Extending the Pipeline

### Adding New Data Sources

Use the `--input-dir` option to process files from any directory:

```bash
python run_batch_pipeline.py --input-dir /path/to/your/documents/
```

Or process specific files regardless of location:
```bash
python run_batch_pipeline.py --files /path/to/file1.pdf /other/path/file2.docx
```

### Customizing Output Format

Edit `summarisation_prompt.txt` to change:
- Summary structure
- Required fields
- Output length
- Technical depth

### Changing Output Directory

Edit `process_results.py` line 37:
```python
summaries_dir = Path('output/my_summaries')  # Custom location
```

## Technical Stack

- **Python 3.12+** - Core runtime
- **pypdf** - Primary PDF text extraction
- **pdfplumber** - Fallback extraction for complex PDFs
- **python-docx** - Microsoft Word document extraction
- **python-pptx** - PowerPoint presentation extraction
- **odfpy** - OpenDocument format extraction
- **OpenAI SDK** - API client (Doubleword API is OpenAI-compatible)
- **Doubleword API** - Batch inference backend
- **Qwen3-VL-235B** - Vision-language model for document understanding

## Acknowledgments

Built using:
- [Doubleword AI](https://app.doubleword.ai/models?page=1) - Batch inference platform
- [Qwen3-VL] - Open-weight vision-language model provided by Doubleword
- OpenAI-compatible API standard for seamless integration

## License

MIT License - see LICENSE file for details

## Next Steps
- Try out streaming feature
- Test the model's vision capabilities
- LLM as a Judge  - this is often token intensive and async and so a good candidate for batch inference
- Add temperature, top_p, top_k, frequency penalty, presence penalty etc to .env or config file 

## Related Concepts

- **Batch inference** - Processing multiple requests efficiently
- **Open-weight models** - Qwen3, DeepSeek, Llama alternatives to proprietary models
- **Structured output** - JSON/markdown formatted LLM responses
- **Document intelligence** - AI-powered document analysis at scale



================================================
FILE: create_batch.py
================================================
#!/usr/bin/env python3
"""Create JSONL batch requests with support for multiple document formats.

Supported formats: PDF, DOCX, PPTX, ODP, TXT, MD
"""

import json
import os
import argparse
from pathlib import Path
from pypdf import PdfReader
import pdfplumber
from docx import Document
from pptx import Presentation
from odf.opendocument import load as load_odf
from odf.text import P
from odf.draw import Frame
import glob
from dotenv import load_dotenv
from datetime import datetime

# Parse command line arguments
parser = argparse.ArgumentParser(
    description='Create JSONL batch requests from documents',
    formatter_class=argparse.RawDescriptionHelpFormatter,
    epilog='''
Examples:
  # Process all files in default directory (data/papers/)
  python create_batch.py

  # Process specific files
  python create_batch.py --files paper1.pdf paper2.txt report.docx

  # Process all files in a custom directory
  python create_batch.py --input-dir /path/to/documents/
'''
)
parser.add_argument(
    '--files',
    nargs='+',
    metavar='FILE',
    help='Specific file paths to process'
)
parser.add_argument(
    '--input-dir',
    metavar='DIR',
    help='Directory to scan for documents (default: data/papers/)'
)

args = parser.parse_args()

# Load environment variables
load_dotenv()

# Read summarization prompt and substitute word count
with open('summarisation_prompt.txt', 'r') as f:
    prompt_template = f.read()

# Substitute word count from environment variable (default to 2000)
word_count = os.getenv('SUMMARY_WORD_COUNT', '2000')
prompt_template = prompt_template.replace('{WORD_COUNT}', word_count)

# Print environment variables being used
print("Environment Variables:")
print(f"  SUMMARY_WORD_COUNT: {word_count}")
print(f"  CHAT_COMPLETIONS_ENDPOINT: {os.getenv('CHAT_COMPLETIONS_ENDPOINT', '/v1/chat/completions')}")
print(f"  DOUBLEWORD_MODEL: {os.getenv('DOUBLEWORD_MODEL', 'Qwen/Qwen3-VL-235B-A22B-Instruct-FP8')}")
print(f"  MAX_TOKENS: {os.getenv('MAX_TOKENS', '5000')}")
print()

# Collect files based on arguments
supported_extensions = ['*.pdf', '*.txt', '*.md', '*.docx', '*.pptx', '*.odp']
all_files = []

if args.files:
    # Use specific files provided
    all_files = [str(Path(f).resolve()) for f in args.files]
    print(f"Processing {len(all_files)} specified file(s)\n")
elif args.input_dir:
    # Scan custom directory
    input_dir = Path(args.input_dir)
    if not input_dir.exists():
        print(f"Error: Directory '{args.input_dir}' does not exist")
        exit(1)
    for ext in supported_extensions:
        all_files.extend(glob.glob(str(input_dir / ext)))
    all_files.sort()
    print(f"Found {len(all_files)} files in {args.input_dir}\n")
else:
    # Default: scan data/papers directory
    for ext in supported_extensions:
        all_files.extend(glob.glob(f'data/papers/{ext}'))
    all_files.sort()
    print(f"Found {len(all_files)} files in data/papers/\n")

requests = []
failed_files = []
extraction_stats = {'pypdf': 0, 'pdfplumber': 0, 'txt': 0, 'docx': 0, 'pptx': 0, 'odp': 0}

def extract_text_pypdf(pdf_path):
    """Try pypdf first (faster)."""
    with open(pdf_path, 'rb') as f:
        reader = PdfReader(f)
        text = '\n'.join(page.extract_text() for page in reader.pages)
        return text, len(reader.pages)

def extract_text_pdfplumber(pdf_path):
    """Fallback to pdfplumber (more robust but slower)."""
    with pdfplumber.open(pdf_path) as pdf:
        text = '\n'.join((page.extract_text() or '') for page in pdf.pages)
        return text, len(pdf.pages)

def extract_from_text(file_path):
    """Extract text from .txt or .md files."""
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        text = f.read()
        return text, 1

def extract_from_docx(file_path):
    """Extract text from .docx files."""
    doc = Document(file_path)
    paragraphs = [para.text for para in doc.paragraphs]
    text = '\n'.join(paragraphs)
    # Estimate pages (rough: 500 words per page)
    word_count = len(text.split())
    pages = max(1, word_count // 500)
    return text, pages

def extract_from_pptx(file_path):
    """Extract text from .pptx files."""
    prs = Presentation(file_path)
    text_runs = []
    for slide in prs.slides:
        for shape in slide.shapes:
            if hasattr(shape, "text"):
                text_runs.append(shape.text)
    text = '\n'.join(text_runs)
    return text, len(prs.slides)

def extract_from_odp(file_path):
    """Extract text from .odp files."""
    doc = load_odf(file_path)
    text_runs = []
    # Extract all text paragraphs
    for paragraph in doc.getElementsByType(P):
        text_content = ''.join(node.data for node in paragraph.childNodes if hasattr(node, 'data'))
        if text_content.strip():
            text_runs.append(text_content)
    text = '\n'.join(text_runs)
    # Count frames as slide estimate
    frames = doc.getElementsByType(Frame)
    pages = max(1, len(frames))
    return text, pages

for idx, file_path in enumerate(all_files, 1):
    print(f"[{idx}/{len(all_files)}] Processing {file_path}...")

    text = None
    pages = 0
    extraction_method = None
    file_extension = Path(file_path).suffix.lower()

    try:
        # Route to appropriate extraction method based on file type
        if file_extension == '.pdf':
            # Try pypdf first (faster), fallback to pdfplumber
            try:
                text, pages = extract_text_pypdf(file_path)
                extraction_method = 'pypdf'
                extraction_stats['pypdf'] += 1
            except (KeyError, Exception) as e:
                if 'bbox' in str(e) or isinstance(e, KeyError):
                    print(f"  ⚠ pypdf failed ({e}), trying pdfplumber...")
                    text, pages = extract_text_pdfplumber(file_path)
                    extraction_method = 'pdfplumber'
                    extraction_stats['pdfplumber'] += 1
                else:
                    raise

        elif file_extension == '.docx':
            text, pages = extract_from_docx(file_path)
            extraction_method = 'docx'
            extraction_stats['docx'] += 1

        elif file_extension == '.pptx':
            text, pages = extract_from_pptx(file_path)
            extraction_method = 'pptx'
            extraction_stats['pptx'] += 1

        elif file_extension == '.odp':
            text, pages = extract_from_odp(file_path)
            extraction_method = 'odp'
            extraction_stats['odp'] += 1

        elif file_extension in ['.txt', '.md']:
            text, pages = extract_from_text(file_path)
            extraction_method = 'txt'
            extraction_stats['txt'] += 1

        else:
            print(f"  ⚠ Unsupported file type: {file_extension}")
            failed_files.append((file_path, f"unsupported file type: {file_extension}"))
            continue

    except Exception as e:
        print(f"  ✗ Error: {e}")
        failed_files.append((file_path, str(e)))
        continue

    # Skip if no meaningful text extracted
    if not text or len(text.strip()) < 100:
        print(f"  ⚠ Skipped (insufficient text: {len(text)} chars)")
        failed_files.append((file_path, "insufficient text"))
        continue

    print(f"  ✓ Extracted {len(text)} characters from {pages} pages [{extraction_method}]")

    # Create batch request with sanitized custom_id
    # Remove special chars from filename for custom_id (max 64 chars including 'summary-' prefix)
    safe_filename = Path(file_path).stem.replace('%', '_').replace(' ', '_').replace('&', 'and')[:55]

    request = {
        "custom_id": f"summary-{safe_filename}",
        "method": "POST",
        "url": os.getenv('CHAT_COMPLETIONS_ENDPOINT', '/v1/chat/completions'),
        "body": {
            "model": os.getenv('DOUBLEWORD_MODEL', 'Qwen/Qwen3-VL-235B-A22B-Instruct-FP8'),
            "messages": [
                {
                    "role": "user",
                    "content": f"{prompt_template}\n\nDocument text:\n{text}"
                }
            ],
            "max_tokens": int(os.getenv('MAX_TOKENS', '5000'))
        }
    }
    requests.append(request)

# Write JSONL file with timestamp
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
output_file = f'batch_requests_{timestamp}.jsonl'
with open(output_file, 'w') as f:
    for req in requests:
        f.write(json.dumps(req) + '\n')

print(f"\n{'='*60}")
print(f"✓ Created {output_file} with {len(requests)} requests")
print(f"\nExtraction methods used:")
for method, count in extraction_stats.items():
    if count > 0:
        label = "pdfplumber (fallback)" if method == 'pdfplumber' else method
        print(f"  {label}: {count} files")

if failed_files:
    print(f"\n⚠ Failed to process {len(failed_files)} files:")
    for path, reason in failed_files:
        print(f"  - {Path(path).name}: {reason}")

print(f"\nNext step: python submit_batch.py")



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2026 Nnamdi Odozi

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: poll_and_process.py
================================================
#!/usr/bin/env python3
"""Poll batch job status and automatically download results when complete."""

import os
import glob
import time
import subprocess
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize client
client = OpenAI(
    api_key=os.environ['DOUBLEWORD_AUTH_TOKEN'],
    base_url=os.environ['DOUBLEWORD_BASE_URL']
)

# Get polling interval from environment variable (default: 30 seconds)
POLLING_INTERVAL = int(os.environ.get('POLLING_INTERVAL', '30'))

# Find most recent batch_id file
batch_id_files = glob.glob('batch_id_*.txt')
if not batch_id_files:
    print("Error: No batch_id_*.txt files found. Run submit_batch.py first.")
    exit(1)

latest_batch_id_file = max(batch_id_files, key=os.path.getmtime)
with open(latest_batch_id_file, 'r') as f:
    batch_id = f.read().strip()

print(f"Using batch ID from: {latest_batch_id_file}")

print(f"Polling batch job: {batch_id}")
print("Press Ctrl+C to stop polling\n")

try:
    while True:
        batch = client.batches.retrieve(batch_id)
        status = batch.status
        completed = batch.request_counts.completed
        total = batch.request_counts.total

        timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
        print(f"[{timestamp}] Status: {status} | Progress: {completed}/{total}")

        if status == 'completed':
            print("\n✓ Batch completed successfully!")
            print("Downloading and processing results...\n")

            # Run process_results.py
            result = subprocess.run(['.venv/bin/python', 'process_results.py'])

            if result.returncode == 0:
                print("\n✓ All summaries saved to data/summaries/")
            else:
                print("\n✗ Error processing results")
            break

        elif status == 'failed':
            print(f"\n✗ Batch failed!")
            if hasattr(batch, 'errors') and batch.errors:
                print(f"Errors: {batch.errors}")
            break

        elif status == 'expired':
            print(f"\n✗ Batch expired!")
            break

        elif status == 'cancelled':
            print(f"\n✗ Batch was cancelled!")
            break

        # Wait before next check
        time.sleep(POLLING_INTERVAL)

except KeyboardInterrupt:
    print("\n\nPolling stopped by user")
    print(f"Current status: {status}")
    print("Run this script again to resume polling")



================================================
FILE: process_results.py
================================================
#!/usr/bin/env python3
"""Download batch results and save summaries to data/summaries/."""

import os
import glob
import json
from pathlib import Path
from datetime import datetime
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize client
client = OpenAI(
    api_key=os.environ['DOUBLEWORD_AUTH_TOKEN'],
    base_url=os.environ['DOUBLEWORD_BASE_URL']
)

# Find most recent batch_id file
batch_id_files = glob.glob('batch_id_*.txt')
if not batch_id_files:
    print("Error: No batch_id_*.txt files found. Run submit_batch.py first.")
    exit(1)

latest_batch_id_file = max(batch_id_files, key=os.path.getmtime)
with open(latest_batch_id_file, 'r') as f:
    batch_id = f.read().strip()

print(f"Retrieving batch results: {batch_id}\n")

# Get batch status
batch = client.batches.retrieve(batch_id)

if batch.status != 'completed':
    print(f"✗ Batch not completed yet. Status: {batch.status}")
    exit(1)

print(f"✓ Batch completed successfully")
print(f"Output file ID: {batch.output_file_id}\n")

# Download results file
print("Downloading results...")
file_response = client.files.content(batch.output_file_id)

# Create summaries directory
summaries_dir = Path('data/summaries')
summaries_dir.mkdir(parents=True, exist_ok=True)
print(f"Summaries will be saved to: {summaries_dir}/\n")

# Process each result
results_count = 0
for line in file_response.text.split('\n'):
    if not line.strip():
        continue

    result = json.loads(line)
    custom_id = result['custom_id']

    # Extract summary from response
    summary = result['response']['body']['choices'][0]['message']['content']

    # Extract filename from custom_id (e.g., "summary-DGM" -> "DGM")
    filename = custom_id.replace('summary-', '')

    # Generate timestamp
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    # Save summary with timestamp as markdown
    output_path = summaries_dir / f'{filename}_summary_{timestamp}.md'
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(summary)

    print(f"✓ Saved: {output_path}")
    results_count += 1

print(f"\n✓ Successfully processed {results_count} summaries")



================================================
FILE: pyproject.toml
================================================
[tool.uv]
link-mode = "copy"  # Avoid hardlink warnings on Windows/OneDrive

[project]
name = "batch-summary-doubleword"
version = "0.1.0"
description = "Batch PDF summarization using Doubleword API"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "pypdf>=6.6.0",
    "pdfplumber>=0.11.9",
    "openai>=2.14.0",
    "python-dotenv>=1.1.0",
    "python-docx>=1.1.0",
    "python-pptx>=1.0.0",
    "odfpy>=1.4.1",
]



================================================
FILE: requirements.txt
================================================
pypdf>=6.6.0
pdfplumber>=0.11.9
openai>=2.14.0
python-dotenv>=1.1.0
python-docx>=1.1.0
python-pptx>=1.0.0
odfpy>=1.4.1



================================================
FILE: run_batch_pipeline.py
================================================
#!/usr/bin/env python3
"""
Orchestrator script for the batch summarization pipeline.
Runs all three stages: extraction, submission, and polling.

Usage:
  # Process all files in default directory (data/papers/)
  python run_batch_pipeline.py

  # Process specific files
  python run_batch_pipeline.py --files paper1.pdf paper2.txt report.docx

  # Process all files in a custom directory
  python run_batch_pipeline.py --input-dir /path/to/documents/
"""

import os
import sys
import argparse
import subprocess
from pathlib import Path
from dotenv import load_dotenv


def print_header(text):
    """Print a formatted section header."""
    print("\n" + "=" * 50)
    print(text)
    print("=" * 50)


def validate_environment():
    """Validate that all required environment variables are set."""
    required_vars = [
        'DOUBLEWORD_AUTH_TOKEN',
        'DOUBLEWORD_BASE_URL',
        'DOUBLEWORD_MODEL'
    ]

    missing = [var for var in required_vars if not os.getenv(var)]

    if missing:
        print("Error: Missing required environment variables:")
        for var in missing:
            print(f"  - {var}")
        print("\nPlease check your .env file")
        sys.exit(1)

    print("✓ Environment variables loaded")
    print(f"  Base URL: {os.getenv('DOUBLEWORD_BASE_URL')}")
    print(f"  Model: {os.getenv('DOUBLEWORD_MODEL')}")
    print(f"  Polling interval: {os.getenv('POLLING_INTERVAL', '30')} seconds")


def run_stage(stage_num, description, script_name, extra_args=None):
    """Run a pipeline stage."""
    print_header(f"STAGE {stage_num}: {description}")

    cmd = [sys.executable, script_name]
    if extra_args:
        cmd.extend(extra_args)

    try:
        result = subprocess.run(
            cmd,
            check=True,
            capture_output=False
        )
        print(f"\n✓ Stage {stage_num} completed successfully")
        return True
    except subprocess.CalledProcessError as e:
        print(f"\n✗ Error in stage {stage_num}")
        print(f"Script '{script_name}' failed with exit code {e.returncode}")
        return False


def main():
    """Run the complete batch processing pipeline."""
    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description='Run the complete batch summarization pipeline',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
Examples:
  # Process all files in default directory (data/papers/)
  python run_batch_pipeline.py

  # Process specific files
  python run_batch_pipeline.py --files paper1.pdf paper2.txt report.docx

  # Process all files in a custom directory
  python run_batch_pipeline.py --input-dir /path/to/documents/
'''
    )
    parser.add_argument(
        '--files',
        nargs='+',
        metavar='FILE',
        help='Specific file paths to process'
    )
    parser.add_argument(
        '--input-dir',
        metavar='DIR',
        help='Directory to scan for documents (default: data/papers/)'
    )

    args = parser.parse_args()

    print_header("Doubleword Batch Summarization Pipeline")

    # Check for .env file
    env_file = Path('.env')
    if not env_file.exists():
        print("\nError: .env file not found!")
        print("Please copy .env.sample to .env and fill in your credentials")
        sys.exit(1)

    # Load environment variables
    print("\nLoading environment variables from .env...")
    load_dotenv()

    # Validate environment
    validate_environment()

    # Prepare arguments for create_batch.py
    create_batch_args = []
    if args.files:
        create_batch_args.extend(['--files'] + args.files)
    elif args.input_dir:
        create_batch_args.extend(['--input-dir', args.input_dir])

    # Stage 1: Extract documents and create batch requests
    if not run_stage(
        1,
        "Extracting documents and creating batch requests",
        "create_batch.py",
        extra_args=create_batch_args if create_batch_args else None
    ):
        sys.exit(1)

    # Stage 2: Submit batch to Doubleword API
    if not run_stage(
        2,
        "Submitting batch to Doubleword API",
        "submit_batch.py"
    ):
        sys.exit(1)

    # Allow time for batch ID propagation before polling
    print("\nWaiting for batch ID to propagate...")
    import time
    time.sleep(10)  # Wait 10 seconds for the batch to be queryable

    # Stage 3: Poll and process results
    if not run_stage(
        3,
        "Polling for results and processing summaries",
        "poll_and_process.py"
    ):
        sys.exit(1)

    # Success!
    print_header("✓ Pipeline completed successfully!")
    print("\nSummaries saved to: data/summaries/")


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nPipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n✗ Unexpected error: {e}")
        sys.exit(1)



================================================
FILE: submit_batch.py
================================================
#!/usr/bin/env python3
"""Upload batch requests and submit batch job to Doubleword API."""

import os
import glob
from datetime import datetime
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize client with Doubleword credentials
client = OpenAI(
    api_key=os.environ['DOUBLEWORD_AUTH_TOKEN'],
    base_url=os.environ['DOUBLEWORD_BASE_URL']
)

# Print environment variables being used
print("Environment Variables:")
print(f"  DOUBLEWORD_BASE_URL: {os.environ['DOUBLEWORD_BASE_URL']}")
print(f"  DOUBLEWORD_AUTH_TOKEN: {'*' * 20}...{os.environ['DOUBLEWORD_AUTH_TOKEN'][-4:]}")
print(f"  COMPLETION_WINDOW: {os.getenv('COMPLETION_WINDOW', '1h')}")
print(f"  CHAT_COMPLETIONS_ENDPOINT: {os.getenv('CHAT_COMPLETIONS_ENDPOINT', '/v1/chat/completions')}")
print()

# Find most recent batch_requests file
batch_files = glob.glob('batch_requests_*.jsonl')
if not batch_files:
    print("Error: No batch_requests_*.jsonl files found. Run create_batch.py first.")
    exit(1)

latest_batch_file = max(batch_files, key=os.path.getmtime)
print(f"Uploading {latest_batch_file}...")

# Upload batch file
with open(latest_batch_file, "rb") as file:
    batch_file = client.files.create(
        file=file,
        purpose="batch"
    )

print(f"File uploaded successfully!")
print(f"File ID: {batch_file.id}")

# Create batch job
completion_window = os.getenv('COMPLETION_WINDOW', '1h')
print(f"\nCreating batch job (completion window: {completion_window})...")
batch = client.batches.create(
    input_file_id=batch_file.id,
    endpoint=os.getenv('CHAT_COMPLETIONS_ENDPOINT', '/v1/chat/completions'),
    completion_window=completion_window
)

print(f"Batch job created successfully!")
print(f"Batch ID: {batch.id}")
print(f"Status: {batch.status}")

# Save batch ID for later retrieval with timestamp
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
batch_id_file = f'batch_id_{timestamp}.txt'
with open(batch_id_file, 'w') as f:
    f.write(batch.id)

print(f"\nBatch ID saved to {batch_id_file}")
print("Next step: Run poll_and_process.py to monitor progress")



================================================
FILE: summarisation_prompt.txt
================================================
You are an AI research assistant. Your task is to produce a comprehensive {WORD_COUNT}-word technical summary of the provided document in markdown format.

DO NOT ask for clarifications or confirmation. Complete the task as best as you can only using the information provided. Do not make any assumptions that are not supported by the material provided. If certain information is not available, do not make things up!!! Instead write "Not available" or "Not specified" for any of the questions asked. After you have finished please review what you have written for correctness against the instructions.

Pitch the language and complexity of responses at an undergraduate STEM level.

## OUTPUT FORMAT (Markdown)

Use the following markdown structure:

## Document Metadata

**S/N:** [number]
**Title:** [document title]
**Authors:** [author names]
**Date of publication:** [YYYY-MM-DD or as available]
**Topic:** [main topic]
**Sub-topic:** [specific sub-topic]
**URL:** [source URL if available, otherwise "Not available"]

---

## Summary

### 1. Modeling Techniques
Describe specific ML/AI techniques applied (e.g., "Neural networks: deep neural network, convolutional neural network, LSTM, GRU" or "XGBoost with gradient boosting").

### 2. Code Availability
State whether implementation code is available. If yes, provide GitHub or other repository URL.

### 3. Learning Type
Specify if the approach uses supervised, self-supervised, or unsupervised learning.

### 4. Dataset
Describe the dataset used. Indicate if it is real-world data or synthetic data. Include dataset name/source if mentioned.

### 5. Implementation Details
- **Programming language(s):** Python, R, MATLAB, etc.
- **Key libraries/frameworks:** e.g., TensorFlow, PyTorch, scikit-learn, Keras, etc.

### 6. Model Architecture
If non-vanilla or composite models are used, break down into component models and describe the architecture in detail.

### 7. Technical Content
Provide a detailed summary of the modeling approach, methodology, key findings, results, and conclusions. This should form the main body of the summary.

---

**REQUIREMENTS:**
- Output must be in valid markdown format
- Only use the information provided and do not make any assumptions that are not materially justified by the material provided. Do NOT speculatively expand acronyms that are not spelled out in the material. If certain information is not available, do not make things up!!! Instead write "Not available" or "Not specified" for any of the questions asked. After you have finished please review what you have written for correctness against the instructions.
- Produce the summary directly without preamble or meta-commentary
- Do not ask for clarification - work with what is provided
- Target approximately {WORD_COUNT} words in the summary body
- Use markdown formatting: headers (##, ###), bold (**text**), lists (- item), code blocks if needed
- Focus on technical depth and completeness






================================================
FILE: .env.sample
================================================
# Doubleword API Configuration
# Copy this file to .env and fill in your actual values

# Your Doubleword API authentication token
DOUBLEWORD_AUTH_TOKEN=

# Doubleword API base URL
DOUBLEWORD_BASE_URL=https://api.doubleword.ai/v1

# API endpoint for chat completions (relative to base URL)
CHAT_COMPLETIONS_ENDPOINT=/v1/chat/completions

# Model to use for batch processing
DOUBLEWORD_MODEL=Qwen/Qwen3-VL-235B-A22B-Instruct-FP8
#DOUBLEWORD_MODEL=Qwen/Qwen3-VL-30B-A3B-Instruct-FP8

# Polling interval in seconds (how often to check batch status)
POLLING_INTERVAL=60

# Batch completion window or SLA (how long the API has to complete the job)
# Options: "1h" or "24h"
COMPLETION_WINDOW=1h

# Summary word count (target length for generated summaries)
SUMMARY_WORD_COUNT=1500

# Maximum tokens for model response (includes reasoning + summary)
# Adjust based on your SUMMARY_WORD_COUNT: roughly word_count * 1.5 + buffer for reasoning
MAX_TOKENS=5000


