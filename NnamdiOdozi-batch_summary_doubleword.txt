Directory structure:
└── nnamdiodozi-batch_summary_doubleword/
    ├── README.md
    ├── create_batch.py
    ├── LICENSE
    ├── poll_and_process.py
    ├── process_results.py
    ├── pyproject.toml
    ├── requirements.txt
    ├── run_batch_pipeline.py
    ├── submit_batch.py
    ├── summarisation_prompt.txt
    ├── uv.lock
    ├── .env.sample
    └── data/
        ├── summaries_235B_samples/
        │   ├── 0001__IFoA_DSSCC_Part_1_Green_Bonds__5BNov_2023_5D_summary_20260125_143906.md
        │   ├── actuaries-using-data-science-and-artificial-intelligenc_summary_20260125_143906.md
        │   ├── actuary-gpt-applications-of-large-language-models-to-in_summary_20260125_143906.md
        │   ├── AI,_data_science_and_emerging_technologies_summary_20260125_143906.md
        │   ├── DGM_summary_20260125_143906.md
        │   ├── evolution-of-economic-scenario-generators-a-report-by-t_summary_20260125_143906.md
        │   ├── ifoa-response-to-treasury-select-committee-consultation_summary_20260125_143906.md
        │   ├── IFoA_WP_DSSCC_-_Green_Bonds_Paper_2__5BAugust_2024_5D_summary_20260125_143906.md
        │   ├── Key_findings_from_the_data_science_thematic_review_summary_20260125_143906.md
        │   ├── Modular_Framework_of_Machine_Learning_Pipeline__281_29_summary_20260125_143906.md
        │   └── primer-generative-ai_summary_20260125_143906.md
        └── summaries_30B_samples/
            ├── 0001__IFoA_DSSCC_Part_1_Green_Bonds__5BNov_2023_5D_summary_20260125_150113.md
            ├── actuaries-using-data-science-and-artificial-intelligenc_summary_20260125_150113.md
            ├── actuary-gpt-applications-of-large-language-models-to-in_summary_20260125_150113.md
            ├── AI,_data_science_and_emerging_technologies_summary_20260125_150113.md
            ├── DGM_summary_20260125_150113.md
            ├── evolution-of-economic-scenario-generators-a-report-by-t_summary_20260125_150113.md
            ├── ifoa-response-to-treasury-select-committee-consultation_summary_20260125_150113.md
            ├── IFoA_WP_DSSCC_-_Green_Bonds_Paper_2__5BAugust_2024_5D_summary_20260125_150113.md
            ├── Key_findings_from_the_data_science_thematic_review_summary_20260125_150113.md
            ├── Modular_Framework_of_Machine_Learning_Pipeline__281_29_summary_20260125_150113.md
            └── primer-generative-ai_summary_20260125_150113.md

================================================
FILE: README.md
================================================
# Batch Document Summarization with Doubleword API

A simple Python CLI Pipeline for batch processing documents (PDF, DOCX, PPTX, TXT, MD, ODP) into structured summaries using the ultra low-cost Doubleword API and open-weight models. Just load your docs, a prompt, and then the Doubleword API key and you're good to go.

## Overview

This tool extracts text from multiple document formats and generates comprehensive ~2000-word structured summaries using Doubleword's batch inference API. Originally built for literature reviews in actuarial machine learning research, it can be adapted for any bulk document summarization task.

## Use Cases

- **Literature reviews** - Summarize academic papers systematically
- **Regulatory analysis** - Convert 200-page consultation papers into actionable digests
- **Compliance** - Extract structured data from policy documents at scale
- **Sentiment analysis** - Process customer feedback documents in bulk
- **Research synthesis** - Analyze collections of technical reports
- **LLM/Agent Evaluations** - Use LLM as a Judge to evaluate LLM and Agent outputs

## Performance

**Real-world results:**
- **Initial test:** 2 papers processed in ~1 minute
- **Production run:** 33 papers processed in ~30 minutes
- **Total cost:** ~15 pence for 35 papers
- **SLA:** Selected 24-hour window, actual delivery < 30 minutes

## Supported File Formats

- **PDF** (`.pdf`) - Research papers, reports, articles
- **Microsoft Word** (`.docx`) - Documents, proposals
- **Microsoft PowerPoint** (`.pptx`) - Presentations, slide decks
- **OpenDocument Presentation** (`.odp`) - Open format presentations
- **Plain Text** (`.txt`) - Text documents
- **Markdown** (`.md`) - Technical documentation, notes

All formats are processed through the same pipeline with automatic file type detection.

## How It Works

The pipeline consists of three stages:

### Stage 1: Document Extraction & Batch Request Creation
**Script:** `create_batch.py`

- Scans `data/papers/` folder (or custom location via `--input-dir`)
- Extracts text from multiple formats:
  - **PDF:** pypdf (fast) with pdfplumber fallback (robust)
  - **DOCX:** python-docx
  - **PPTX:** python-pptx
  - **ODP:** odfpy
  - **TXT/MD:** Direct text read
- Creates structured JSONL batch requests with custom summarization prompt
- Outputs: `batch_requests_{timestamp}.jsonl`

### Stage 2: Batch Submission
**Script:** `submit_batch.py`

- Uploads `batch_requests.jsonl` to Doubleword API
- Creates batch job with 1-hour completion window
- Saves batch ID to `batch_id.txt` for tracking
- Outputs: Batch ID for monitoring

### Stage 3: Polling & Processing
**Script:** `poll_and_process.py`

- Polls batch job status at configurable intervals (default: 60 seconds)
- Automatically downloads results when completed
- Calls `process_results.py` to extract and save individual summaries
- Outputs: Individual markdown summaries in `data/summaries/`

### Processing Results
**Script:** `process_results.py`

- Downloads batch output file from Doubleword API
- Parses JSONL responses
- Saves each summary as timestamped markdown file
- Format: `{filename}_summary_{timestamp}.md`

## Setup

### 1. Install Dependencies
git clone https://github.com/NnamdiOdozi/batch_summary_doubleword.git

Using uv (recommended):
```bash
uv sync
source .venv/bin/activate  # Linux/macOS
# OR on Windows: .venv\Scripts\activate
```

Or using pip:
```bash
python3 -m venv .venv
source .venv/bin/activate  # Linux/macOS  
# OR on Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

**Requirements** ([requirements.txt](requirements.txt)):
- `pypdf>=6.6.0` - Fast PDF text extraction
- `pdfplumber>=0.11.9` - Robust fallback for complex PDFs
- `python-docx>=1.1.0` - Microsoft Word document extraction
- `python-pptx>=1.0.0` - PowerPoint presentation extraction
- `odfpy>=1.4.1` - OpenDocument format extraction
- `openai>=2.14.0` - API client (compatible with Doubleword API)
- `python-dotenv>=1.1.0` - Environment variable management

### 2. Configure Environment Variables

Copy the sample environment file:
```bash
cp .env.sample .env
```

Edit `.env` and fill in your credentials:
```bash
# Your Doubleword API token
DOUBLEWORD_AUTH_TOKEN=your_api_token_here

# Doubleword API endpoint
DOUBLEWORD_BASE_URL=https://api.doubleword.ai/v1

# API endpoint for chat completions (relative to base URL)
CHAT_COMPLETIONS_ENDPOINT=/v1/chat/completions

# Model to use
DOUBLEWORD_MODEL=Qwen/Qwen3-VL-235B-A22B-Instruct-FP8
or any other model you would like eg the smaller and cheaper Qwen/Qwen3-VL-30B-A3B-Instruct-FP8

# Polling frequency (seconds)
POLLING_INTERVAL=60

# Batch completion window or SLA (how long the API has to complete the job)
# Options: "1h" or "24h"
COMPLETION_WINDOW=1h

# Summary word count (target length for generated summaries)
SUMMARY_WORD_COUNT=2000

# Maximum tokens for model response (includes reasoning + summary)
MAX_TOKENS=5000
```

**Get your API key:**
1. Visit [Doubleword Portal](https://doubleword.ai)
2. Click to join Private Preview
3. Create account or log in
4. Generate API key in settings

### 3. Add Your Documents 
      
Place documents in:
- `data/papers/` folder

Supported formats: PDF, DOCX, PPTX, ODP, TXT, MD     

The pipeline will automatically detect and process all supported files in this directory.

### 4. Customize Summarization (Optional)

**Adjust word count:**
Edit `SUMMARY_WORD_COUNT` in `.env` to change summary length (default: 2000 words)

**Customize prompt template:**
Edit [summarisation_prompt.txt](summarisation_prompt.txt) to adjust:
- Output structure and fields
- Technical complexity level
- Markdown formatting
- Required fields

## Usage

### Quick Start - Run Full Pipeline

```bash
python run_batch_pipeline.py
```

This orchestrator script runs all three stages automatically:
1. Extracts documents and creates batch requests
2. Submits to Doubleword API
3. Polls until complete and downloads summaries

### Command Line Options

**Process all files in default directory:**
```bash
python run_batch_pipeline.py
```

**Process specific files:**
```bash
python run_batch_pipeline.py --files paper1.pdf report.docx slides.pptx
```

**Process files from custom directory:**
```bash
python run_batch_pipeline.py --input-dir /path/to/documents/
```

**View all options:**
```bash
python run_batch_pipeline.py --help
python create_batch.py --help
```

### Manual Step-by-Step

If you prefer to run stages individually:

**Stage 1: Create batch requests (all files in data/papers/)**
```bash
python create_batch.py
```

Or process specific files:
```bash
python create_batch.py --files doc1.pdf doc2.docx
```

Or process custom directory:
```bash
python create_batch.py --input-dir /custom/path/
```

Output: `batch_requests_{timestamp}.jsonl`

**Stage 2: Submit batch**
```bash
python submit_batch.py
```
Output: `batch_id.txt` with job ID

**Stage 3: Poll and process**
```bash
python poll_and_process.py
```
Output: Individual summaries in `data/summaries/`

### Monitoring Progress

The polling script shows real-time status:
```
[2026-01-25 14:32:15] Status: in_progress | Progress: 12/35
[2026-01-25 14:32:45] Status: in_progress | Progress: 24/35
[2026-01-25 14:33:15] Status: completed | Progress: 35/35

✓ Batch completed successfully!
```

Press `Ctrl+C` to stop polling. Run the script again to resume.

## Project Structure

```
batch_summary_doubleword/
├── README.md                           # This file
├── pyproject.toml                      # Python dependencies (uv)
├── requirements.txt                    # Python dependencies (pip)
├── .env.sample                         # Environment variable template
├── .gitignore                          # Git ignore rules
├── run_batch_pipeline.py               # Orchestrator script (Python)
├── summarisation_prompt.txt            # Prompt template for summaries
├── create_batch.py     # Stage 1: PDF extraction
├── submit_batch.py                     # Stage 2: Batch submission
├── poll_and_process.py                 # Stage 3: Polling and processing
├── process_results.py                  # Result processing
└── data/
    ├── papers/                         # Input PDFs
    └── summaries/                      # Output summaries (auto-created)
```

**Generated files (not in git):**
- `batch_requests_YYYYMMDD_HHMMSS.jsonl` - JSONL file with timestamped batch requests
- `batch_id_YYYYMMDD_HHMMSS.txt` - Timestamped batch job ID
- `data/summaries/*.md` - Individual paper summaries

## Configuration Options

### Polling Interval

Adjust how frequently the script checks batch status:

```bash
# In .env file
POLLING_INTERVAL=60  # Check every 60 seconds
```

Lower values = faster notification, more API calls
Higher values = fewer API calls, slower notification

**Recommended:** 30-60 seconds for most use cases

### Model Selection

The default model is `Qwen/Qwen3-VL-235B-A22B-Instruct-FP8`, which supports:
- Long context windows (128K+ tokens)
- Vision capabilities (for PDFs with charts/diagrams)
- Structured output generation

To use a different model, update `DOUBLEWORD_MODEL` in `.env`.

### Completion Window / SLA

The batch job completion window determines how long the API has to complete your job. Configure via `COMPLETION_WINDOW` in `.env`:

```bash
COMPLETION_WINDOW=1h  # Options: "1h" or "24h"
```

Doubleword typically completes jobs much faster than the window:
- 2 papers: ~1 minute
- 35 papers: ~30 minutes

Use `1h` for most cases. Use `24h` if you want even cheaper pricing and if task is not as time critical.

## Cost Estimation

Based on actual usage (Jan 2026):
- **35 papers** (mixed lengths, 45-200 pages each)
- **Model:** Qwen3-VL-235B-A22B-Instruct-FP8
- **Cost:** ~15 pence total (~0.43p per paper)

Cost varies by:
- Document length
- Requested summary length
- Model selected
- Number of requests

## Troubleshooting

### Authentication Errors

```
Error: Unauthorized
```
**Solution:** Check your `DOUBLEWORD_AUTH_TOKEN` in `.env`

### Batch Takes Too Long

**Solution:** Doubleword typically completes in ~1 minute. If waiting longer:
1. Check Doubleword portal for job status
2. Verify your completion window setting
3. Contact Doubleword support if job is stuck

### Process Results Error

```
✗ Error processing results
```
**Solution:** Check that `process_results.py` has correct permissions and paths

## Extending the Pipeline

### Adding New Data Sources

Use the `--input-dir` option to process files from any directory:

```bash
python run_batch_pipeline.py --input-dir /path/to/your/documents/
```

Or process specific files regardless of location:
```bash
python run_batch_pipeline.py --files /path/to/file1.pdf /other/path/file2.docx
```

### Customizing Output Format

Edit `summarisation_prompt.txt` to change:
- Summary structure
- Required fields
- Output length
- Technical depth

### Changing Output Directory

Edit `process_results.py` line 37:
```python
summaries_dir = Path('output/my_summaries')  # Custom location
```

## Technical Stack

- **Python 3.12+** - Core runtime
- **pypdf** - Primary PDF text extraction
- **pdfplumber** - Fallback extraction for complex PDFs
- **python-docx** - Microsoft Word document extraction
- **python-pptx** - PowerPoint presentation extraction
- **odfpy** - OpenDocument format extraction
- **OpenAI SDK** - API client (Doubleword API is OpenAI-compatible)
- **Doubleword API** - Batch inference backend
- **Qwen3-VL-235B** - Vision-language model for document understanding

## Acknowledgments

Built using:
- [Doubleword AI](https://app.doubleword.ai/models?page=1) - Batch inference platform
- [Qwen3-VL] - Open-weight vision-language model provided by Doubleword
- OpenAI-compatible API standard for seamless integration

## License

MIT License - see LICENSE file for details

## Next Steps
- Try out streaming feature
- Test the model's vision capabilities
- LLM as a Judge  - this is often token intensive and async and so a good candidate for batch inference
- Add temperature, top_p, top_k, frequency penalty, presence penalty etc to .env or config file 

## Related Concepts

- **Batch inference** - Processing multiple requests efficiently
- **Open-weight models** - Qwen3, DeepSeek, Llama alternatives to proprietary models
- **Structured output** - JSON/markdown formatted LLM responses
- **Document intelligence** - AI-powered document analysis at scale



================================================
FILE: create_batch.py
================================================
#!/usr/bin/env python3
"""Create JSONL batch requests with support for multiple document formats.

Supported formats: PDF, DOCX, PPTX, ODP, TXT, MD
"""

import json
import os
import argparse
from pathlib import Path
from pypdf import PdfReader
import pdfplumber
from docx import Document
from pptx import Presentation
from odf.opendocument import load as load_odf
from odf.text import P
from odf.draw import Frame
import glob
from dotenv import load_dotenv
from datetime import datetime

# Parse command line arguments
parser = argparse.ArgumentParser(
    description='Create JSONL batch requests from documents',
    formatter_class=argparse.RawDescriptionHelpFormatter,
    epilog='''
Examples:
  # Process all files in default directory (data/papers/)
  python create_batch.py

  # Process specific files
  python create_batch.py --files paper1.pdf paper2.txt report.docx

  # Process all files in a custom directory
  python create_batch.py --input-dir /path/to/documents/
'''
)
parser.add_argument(
    '--files',
    nargs='+',
    metavar='FILE',
    help='Specific file paths to process'
)
parser.add_argument(
    '--input-dir',
    metavar='DIR',
    help='Directory to scan for documents (default: data/papers/)'
)

args = parser.parse_args()

# Load environment variables
load_dotenv()

# Read summarization prompt and substitute word count
with open('summarisation_prompt.txt', 'r') as f:
    prompt_template = f.read()

# Substitute word count from environment variable (default to 2000)
word_count = os.getenv('SUMMARY_WORD_COUNT', '2000')
prompt_template = prompt_template.replace('{WORD_COUNT}', word_count)

# Print environment variables being used
print("Environment Variables:")
print(f"  SUMMARY_WORD_COUNT: {word_count}")
print(f"  CHAT_COMPLETIONS_ENDPOINT: {os.getenv('CHAT_COMPLETIONS_ENDPOINT', '/v1/chat/completions')}")
print(f"  DOUBLEWORD_MODEL: {os.getenv('DOUBLEWORD_MODEL', 'Qwen/Qwen3-VL-235B-A22B-Instruct-FP8')}")
print(f"  MAX_TOKENS: {os.getenv('MAX_TOKENS', '5000')}")
print()

# Collect files based on arguments
supported_extensions = ['*.pdf', '*.txt', '*.md', '*.docx', '*.pptx', '*.odp']
all_files = []

if args.files:
    # Use specific files provided
    all_files = [str(Path(f).resolve()) for f in args.files]
    print(f"Processing {len(all_files)} specified file(s)\n")
elif args.input_dir:
    # Scan custom directory
    input_dir = Path(args.input_dir)
    if not input_dir.exists():
        print(f"Error: Directory '{args.input_dir}' does not exist")
        exit(1)
    for ext in supported_extensions:
        all_files.extend(glob.glob(str(input_dir / ext)))
    all_files.sort()
    print(f"Found {len(all_files)} files in {args.input_dir}\n")
else:
    # Default: scan data/papers directory
    for ext in supported_extensions:
        all_files.extend(glob.glob(f'data/papers/{ext}'))
    all_files.sort()
    print(f"Found {len(all_files)} files in data/papers/\n")

requests = []
failed_files = []
extraction_stats = {'pypdf': 0, 'pdfplumber': 0, 'txt': 0, 'docx': 0, 'pptx': 0, 'odp': 0}

def extract_text_pypdf(pdf_path):
    """Try pypdf first (faster)."""
    with open(pdf_path, 'rb') as f:
        reader = PdfReader(f)
        text = '\n'.join(page.extract_text() for page in reader.pages)
        return text, len(reader.pages)

def extract_text_pdfplumber(pdf_path):
    """Fallback to pdfplumber (more robust but slower)."""
    with pdfplumber.open(pdf_path) as pdf:
        text = '\n'.join((page.extract_text() or '') for page in pdf.pages)
        return text, len(pdf.pages)

def extract_from_text(file_path):
    """Extract text from .txt or .md files."""
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        text = f.read()
        return text, 1

def extract_from_docx(file_path):
    """Extract text from .docx files."""
    doc = Document(file_path)
    paragraphs = [para.text for para in doc.paragraphs]
    text = '\n'.join(paragraphs)
    # Estimate pages (rough: 500 words per page)
    word_count = len(text.split())
    pages = max(1, word_count // 500)
    return text, pages

def extract_from_pptx(file_path):
    """Extract text from .pptx files."""
    prs = Presentation(file_path)
    text_runs = []
    for slide in prs.slides:
        for shape in slide.shapes:
            if hasattr(shape, "text"):
                text_runs.append(shape.text)
    text = '\n'.join(text_runs)
    return text, len(prs.slides)

def extract_from_odp(file_path):
    """Extract text from .odp files."""
    doc = load_odf(file_path)
    text_runs = []
    # Extract all text paragraphs
    for paragraph in doc.getElementsByType(P):
        text_content = ''.join(node.data for node in paragraph.childNodes if hasattr(node, 'data'))
        if text_content.strip():
            text_runs.append(text_content)
    text = '\n'.join(text_runs)
    # Count frames as slide estimate
    frames = doc.getElementsByType(Frame)
    pages = max(1, len(frames))
    return text, pages

for idx, file_path in enumerate(all_files, 1):
    print(f"[{idx}/{len(all_files)}] Processing {file_path}...")

    text = None
    pages = 0
    extraction_method = None
    file_extension = Path(file_path).suffix.lower()

    try:
        # Route to appropriate extraction method based on file type
        if file_extension == '.pdf':
            # Try pypdf first (faster), fallback to pdfplumber
            try:
                text, pages = extract_text_pypdf(file_path)
                extraction_method = 'pypdf'
                extraction_stats['pypdf'] += 1
            except (KeyError, Exception) as e:
                if 'bbox' in str(e) or isinstance(e, KeyError):
                    print(f"  ⚠ pypdf failed ({e}), trying pdfplumber...")
                    text, pages = extract_text_pdfplumber(file_path)
                    extraction_method = 'pdfplumber'
                    extraction_stats['pdfplumber'] += 1
                else:
                    raise

        elif file_extension == '.docx':
            text, pages = extract_from_docx(file_path)
            extraction_method = 'docx'
            extraction_stats['docx'] += 1

        elif file_extension == '.pptx':
            text, pages = extract_from_pptx(file_path)
            extraction_method = 'pptx'
            extraction_stats['pptx'] += 1

        elif file_extension == '.odp':
            text, pages = extract_from_odp(file_path)
            extraction_method = 'odp'
            extraction_stats['odp'] += 1

        elif file_extension in ['.txt', '.md']:
            text, pages = extract_from_text(file_path)
            extraction_method = 'txt'
            extraction_stats['txt'] += 1

        else:
            print(f"  ⚠ Unsupported file type: {file_extension}")
            failed_files.append((file_path, f"unsupported file type: {file_extension}"))
            continue

    except Exception as e:
        print(f"  ✗ Error: {e}")
        failed_files.append((file_path, str(e)))
        continue

    # Skip if no meaningful text extracted
    if not text or len(text.strip()) < 100:
        print(f"  ⚠ Skipped (insufficient text: {len(text)} chars)")
        failed_files.append((file_path, "insufficient text"))
        continue

    print(f"  ✓ Extracted {len(text)} characters from {pages} pages [{extraction_method}]")

    # Create batch request with sanitized custom_id
    # Remove special chars from filename for custom_id (max 64 chars including 'summary-' prefix)
    safe_filename = Path(file_path).stem.replace('%', '_').replace(' ', '_').replace('&', 'and')[:55]

    request = {
        "custom_id": f"summary-{safe_filename}",
        "method": "POST",
        "url": os.getenv('CHAT_COMPLETIONS_ENDPOINT', '/v1/chat/completions'),
        "body": {
            "model": os.getenv('DOUBLEWORD_MODEL', 'Qwen/Qwen3-VL-235B-A22B-Instruct-FP8'),
            "messages": [
                {
                    "role": "user",
                    "content": f"{prompt_template}\n\nDocument text:\n{text}"
                }
            ],
            "max_tokens": int(os.getenv('MAX_TOKENS', '5000'))
        }
    }
    requests.append(request)

# Write JSONL file with timestamp
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
output_file = f'batch_requests_{timestamp}.jsonl'
with open(output_file, 'w') as f:
    for req in requests:
        f.write(json.dumps(req) + '\n')

print(f"\n{'='*60}")
print(f"✓ Created {output_file} with {len(requests)} requests")
print(f"\nExtraction methods used:")
for method, count in extraction_stats.items():
    if count > 0:
        label = "pdfplumber (fallback)" if method == 'pdfplumber' else method
        print(f"  {label}: {count} files")

if failed_files:
    print(f"\n⚠ Failed to process {len(failed_files)} files:")
    for path, reason in failed_files:
        print(f"  - {Path(path).name}: {reason}")

print(f"\nNext step: python submit_batch.py")



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2026 Nnamdi Odozi

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: poll_and_process.py
================================================
#!/usr/bin/env python3
"""Poll batch job status and automatically download results when complete."""

import os
import glob
import time
import subprocess
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize client
client = OpenAI(
    api_key=os.environ['DOUBLEWORD_AUTH_TOKEN'],
    base_url=os.environ['DOUBLEWORD_BASE_URL']
)

# Get polling interval from environment variable (default: 30 seconds)
POLLING_INTERVAL = int(os.environ.get('POLLING_INTERVAL', '30'))

# Find most recent batch_id file
batch_id_files = glob.glob('batch_id_*.txt')
if not batch_id_files:
    print("Error: No batch_id_*.txt files found. Run submit_batch.py first.")
    exit(1)

latest_batch_id_file = max(batch_id_files, key=os.path.getmtime)
with open(latest_batch_id_file, 'r') as f:
    batch_id = f.read().strip()

print(f"Using batch ID from: {latest_batch_id_file}")

print(f"Polling batch job: {batch_id}")
print("Press Ctrl+C to stop polling\n")

try:
    while True:
        batch = client.batches.retrieve(batch_id)
        status = batch.status
        completed = batch.request_counts.completed
        total = batch.request_counts.total

        timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
        print(f"[{timestamp}] Status: {status} | Progress: {completed}/{total}")

        if status == 'completed':
            print("\n✓ Batch completed successfully!")
            print("Downloading and processing results...\n")

            # Run process_results.py
            result = subprocess.run(['.venv/bin/python', 'process_results.py'])

            if result.returncode == 0:
                print("\n✓ All summaries saved to data/summaries/")
            else:
                print("\n✗ Error processing results")
            break

        elif status == 'failed':
            print(f"\n✗ Batch failed!")
            if hasattr(batch, 'errors') and batch.errors:
                print(f"Errors: {batch.errors}")
            break

        elif status == 'expired':
            print(f"\n✗ Batch expired!")
            break

        elif status == 'cancelled':
            print(f"\n✗ Batch was cancelled!")
            break

        # Wait before next check
        time.sleep(POLLING_INTERVAL)

except KeyboardInterrupt:
    print("\n\nPolling stopped by user")
    print(f"Current status: {status}")
    print("Run this script again to resume polling")



================================================
FILE: process_results.py
================================================
#!/usr/bin/env python3
"""Download batch results and save summaries to data/summaries/."""

import os
import glob
import json
from pathlib import Path
from datetime import datetime
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize client
client = OpenAI(
    api_key=os.environ['DOUBLEWORD_AUTH_TOKEN'],
    base_url=os.environ['DOUBLEWORD_BASE_URL']
)

# Find most recent batch_id file
batch_id_files = glob.glob('batch_id_*.txt')
if not batch_id_files:
    print("Error: No batch_id_*.txt files found. Run submit_batch.py first.")
    exit(1)

latest_batch_id_file = max(batch_id_files, key=os.path.getmtime)
with open(latest_batch_id_file, 'r') as f:
    batch_id = f.read().strip()

print(f"Retrieving batch results: {batch_id}\n")

# Get batch status
batch = client.batches.retrieve(batch_id)

if batch.status != 'completed':
    print(f"✗ Batch not completed yet. Status: {batch.status}")
    exit(1)

print(f"✓ Batch completed successfully")
print(f"Output file ID: {batch.output_file_id}\n")

# Download results file
print("Downloading results...")
file_response = client.files.content(batch.output_file_id)

# Create summaries directory
summaries_dir = Path('data/summaries')
summaries_dir.mkdir(parents=True, exist_ok=True)
print(f"Summaries will be saved to: {summaries_dir}/\n")

# Process each result
results_count = 0
for line in file_response.text.split('\n'):
    if not line.strip():
        continue

    result = json.loads(line)
    custom_id = result['custom_id']

    # Extract summary from response
    summary = result['response']['body']['choices'][0]['message']['content']

    # Extract filename from custom_id (e.g., "summary-DGM" -> "DGM")
    filename = custom_id.replace('summary-', '')

    # Generate timestamp
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    # Save summary with timestamp as markdown
    output_path = summaries_dir / f'{filename}_summary_{timestamp}.md'
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(summary)

    print(f"✓ Saved: {output_path}")
    results_count += 1

print(f"\n✓ Successfully processed {results_count} summaries")



================================================
FILE: pyproject.toml
================================================
[tool.uv]
link-mode = "copy"  # Avoid hardlink warnings on Windows/OneDrive

[project]
name = "batch-summary-doubleword"
version = "0.1.0"
description = "Batch PDF summarization using Doubleword API"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "pypdf>=6.6.0",
    "pdfplumber>=0.11.9",
    "openai>=2.14.0",
    "python-dotenv>=1.1.0",
    "python-docx>=1.1.0",
    "python-pptx>=1.0.0",
    "odfpy>=1.4.1",
]



================================================
FILE: requirements.txt
================================================
pypdf>=6.6.0
pdfplumber>=0.11.9
openai>=2.14.0
python-dotenv>=1.1.0
python-docx>=1.1.0
python-pptx>=1.0.0
odfpy>=1.4.1



================================================
FILE: run_batch_pipeline.py
================================================
#!/usr/bin/env python3
"""
Orchestrator script for the batch summarization pipeline.
Runs all three stages: extraction, submission, and polling.

Usage:
  # Process all files in default directory (data/papers/)
  python run_batch_pipeline.py

  # Process specific files
  python run_batch_pipeline.py --files paper1.pdf paper2.txt report.docx

  # Process all files in a custom directory
  python run_batch_pipeline.py --input-dir /path/to/documents/
"""

import os
import sys
import argparse
import subprocess
from pathlib import Path
from dotenv import load_dotenv


def print_header(text):
    """Print a formatted section header."""
    print("\n" + "=" * 50)
    print(text)
    print("=" * 50)


def validate_environment():
    """Validate that all required environment variables are set."""
    required_vars = [
        'DOUBLEWORD_AUTH_TOKEN',
        'DOUBLEWORD_BASE_URL',
        'DOUBLEWORD_MODEL'
    ]

    missing = [var for var in required_vars if not os.getenv(var)]

    if missing:
        print("Error: Missing required environment variables:")
        for var in missing:
            print(f"  - {var}")
        print("\nPlease check your .env file")
        sys.exit(1)

    print("✓ Environment variables loaded")
    print(f"  Base URL: {os.getenv('DOUBLEWORD_BASE_URL')}")
    print(f"  Model: {os.getenv('DOUBLEWORD_MODEL')}")
    print(f"  Polling interval: {os.getenv('POLLING_INTERVAL', '30')} seconds")


def run_stage(stage_num, description, script_name, extra_args=None):
    """Run a pipeline stage."""
    print_header(f"STAGE {stage_num}: {description}")

    cmd = [sys.executable, script_name]
    if extra_args:
        cmd.extend(extra_args)

    try:
        result = subprocess.run(
            cmd,
            check=True,
            capture_output=False
        )
        print(f"\n✓ Stage {stage_num} completed successfully")
        return True
    except subprocess.CalledProcessError as e:
        print(f"\n✗ Error in stage {stage_num}")
        print(f"Script '{script_name}' failed with exit code {e.returncode}")
        return False


def main():
    """Run the complete batch processing pipeline."""
    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description='Run the complete batch summarization pipeline',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
Examples:
  # Process all files in default directory (data/papers/)
  python run_batch_pipeline.py

  # Process specific files
  python run_batch_pipeline.py --files paper1.pdf paper2.txt report.docx

  # Process all files in a custom directory
  python run_batch_pipeline.py --input-dir /path/to/documents/
'''
    )
    parser.add_argument(
        '--files',
        nargs='+',
        metavar='FILE',
        help='Specific file paths to process'
    )
    parser.add_argument(
        '--input-dir',
        metavar='DIR',
        help='Directory to scan for documents (default: data/papers/)'
    )

    args = parser.parse_args()

    print_header("Doubleword Batch Summarization Pipeline")

    # Check for .env file
    env_file = Path('.env')
    if not env_file.exists():
        print("\nError: .env file not found!")
        print("Please copy .env.sample to .env and fill in your credentials")
        sys.exit(1)

    # Load environment variables
    print("\nLoading environment variables from .env...")
    load_dotenv()

    # Validate environment
    validate_environment()

    # Prepare arguments for create_batch.py
    create_batch_args = []
    if args.files:
        create_batch_args.extend(['--files'] + args.files)
    elif args.input_dir:
        create_batch_args.extend(['--input-dir', args.input_dir])

    # Stage 1: Extract documents and create batch requests
    if not run_stage(
        1,
        "Extracting documents and creating batch requests",
        "create_batch.py",
        extra_args=create_batch_args if create_batch_args else None
    ):
        sys.exit(1)

    # Stage 2: Submit batch to Doubleword API
    if not run_stage(
        2,
        "Submitting batch to Doubleword API",
        "submit_batch.py"
    ):
        sys.exit(1)

    # Allow time for batch ID propagation before polling
    print("\nWaiting for batch ID to propagate...")
    import time
    time.sleep(10)  # Wait 10 seconds for the batch to be queryable

    # Stage 3: Poll and process results
    if not run_stage(
        3,
        "Polling for results and processing summaries",
        "poll_and_process.py"
    ):
        sys.exit(1)

    # Success!
    print_header("✓ Pipeline completed successfully!")
    print("\nSummaries saved to: data/summaries/")


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nPipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n✗ Unexpected error: {e}")
        sys.exit(1)



================================================
FILE: submit_batch.py
================================================
#!/usr/bin/env python3
"""Upload batch requests and submit batch job to Doubleword API."""

import os
import glob
from datetime import datetime
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize client with Doubleword credentials
client = OpenAI(
    api_key=os.environ['DOUBLEWORD_AUTH_TOKEN'],
    base_url=os.environ['DOUBLEWORD_BASE_URL']
)

# Print environment variables being used
print("Environment Variables:")
print(f"  DOUBLEWORD_BASE_URL: {os.environ['DOUBLEWORD_BASE_URL']}")
print(f"  DOUBLEWORD_AUTH_TOKEN: {'*' * 20}...{os.environ['DOUBLEWORD_AUTH_TOKEN'][-4:]}")
print(f"  COMPLETION_WINDOW: {os.getenv('COMPLETION_WINDOW', '1h')}")
print(f"  CHAT_COMPLETIONS_ENDPOINT: {os.getenv('CHAT_COMPLETIONS_ENDPOINT', '/v1/chat/completions')}")
print()

# Find most recent batch_requests file
batch_files = glob.glob('batch_requests_*.jsonl')
if not batch_files:
    print("Error: No batch_requests_*.jsonl files found. Run create_batch.py first.")
    exit(1)

latest_batch_file = max(batch_files, key=os.path.getmtime)
print(f"Uploading {latest_batch_file}...")

# Upload batch file
with open(latest_batch_file, "rb") as file:
    batch_file = client.files.create(
        file=file,
        purpose="batch"
    )

print(f"File uploaded successfully!")
print(f"File ID: {batch_file.id}")

# Create batch job
completion_window = os.getenv('COMPLETION_WINDOW', '1h')
print(f"\nCreating batch job (completion window: {completion_window})...")
batch = client.batches.create(
    input_file_id=batch_file.id,
    endpoint=os.getenv('CHAT_COMPLETIONS_ENDPOINT', '/v1/chat/completions'),
    completion_window=completion_window
)

print(f"Batch job created successfully!")
print(f"Batch ID: {batch.id}")
print(f"Status: {batch.status}")

# Save batch ID for later retrieval with timestamp
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
batch_id_file = f'batch_id_{timestamp}.txt'
with open(batch_id_file, 'w') as f:
    f.write(batch.id)

print(f"\nBatch ID saved to {batch_id_file}")
print("Next step: Run poll_and_process.py to monitor progress")



================================================
FILE: summarisation_prompt.txt
================================================
You are an AI research assistant. Your task is to produce a comprehensive {WORD_COUNT}-word technical summary of the provided document in markdown format.

DO NOT ask for clarifications or confirmation. Complete the task as best as you can only using the information provided. Do not make any assumptions that are not supported by the material provided. If certain information is not available, do not make things up!!! Instead write "Not available" or "Not specified" for any of the questions asked. After you have finished please review what you have written for correctness against the instructions.

Pitch the language and complexity of responses at an undergraduate STEM level.

## OUTPUT FORMAT (Markdown)

Use the following markdown structure:

## Document Metadata

**S/N:** [number]
**Title:** [document title]
**Authors:** [author names]
**Date of publication:** [YYYY-MM-DD or as available]
**Topic:** [main topic]
**Sub-topic:** [specific sub-topic]
**URL:** [source URL if available, otherwise "Not available"]

---

## Summary

### 1. Modeling Techniques
Describe specific ML/AI techniques applied (e.g., "Neural networks: deep neural network, convolutional neural network, LSTM, GRU" or "XGBoost with gradient boosting").

### 2. Code Availability
State whether implementation code is available. If yes, provide GitHub or other repository URL.

### 3. Learning Type
Specify if the approach uses supervised, self-supervised, or unsupervised learning.

### 4. Dataset
Describe the dataset used. Indicate if it is real-world data or synthetic data. Include dataset name/source if mentioned.

### 5. Implementation Details
- **Programming language(s):** Python, R, MATLAB, etc.
- **Key libraries/frameworks:** e.g., TensorFlow, PyTorch, scikit-learn, Keras, etc.

### 6. Model Architecture
If non-vanilla or composite models are used, break down into component models and describe the architecture in detail.

### 7. Technical Content
Provide a detailed summary of the modeling approach, methodology, key findings, results, and conclusions. This should form the main body of the summary.

---

**REQUIREMENTS:**
- Output must be in valid markdown format
- Only use the information provided and do not make any assumptions that are not materially justified by the material provided. Do NOT speculatively expand acronyms that are not spelled out in the material. If certain information is not available, do not make things up!!! Instead write "Not available" or "Not specified" for any of the questions asked. After you have finished please review what you have written for correctness against the instructions.
- Produce the summary directly without preamble or meta-commentary
- Do not ask for clarification - work with what is provided
- Target approximately {WORD_COUNT} words in the summary body
- Use markdown formatting: headers (##, ###), bold (**text**), lists (- item), code blocks if needed
- Focus on technical depth and completeness






================================================
FILE: uv.lock
================================================
version = 1
revision = 3
requires-python = ">=3.12"

[[package]]
name = "annotated-types"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ee/67/531ea369ba64dcff5ec9c3402f9f51bf748cec26dde048a2f973a4eea7f5/annotated_types-0.7.0.tar.gz", hash = "sha256:aff07c09a53a08bc8cfccb9c85b05f1aa9a2a6f23728d790723543408344ce89", size = 16081, upload-time = "2024-05-20T21:33:25.928Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl", hash = "sha256:1f02e8b43a8fbbc3f3e0d4f0f4bfc8131bcb4eebe8849b8e5c773f3a1c582a53", size = 13643, upload-time = "2024-05-20T21:33:24.1Z" },
]

[[package]]
name = "anyio"
version = "4.12.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
    { name = "typing-extensions", marker = "python_full_version < '3.13'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/96/f0/5eb65b2bb0d09ac6776f2eb54adee6abe8228ea05b20a5ad0e4945de8aac/anyio-4.12.1.tar.gz", hash = "sha256:41cfcc3a4c85d3f05c932da7c26d0201ac36f72abd4435ba90d0464a3ffed703", size = 228685, upload-time = "2026-01-06T11:45:21.246Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/38/0e/27be9fdef66e72d64c0cdc3cc2823101b80585f8119b5c112c2e8f5f7dab/anyio-4.12.1-py3-none-any.whl", hash = "sha256:d405828884fc140aa80a3c667b8beed277f1dfedec42ba031bd6ac3db606ab6c", size = 113592, upload-time = "2026-01-06T11:45:19.497Z" },
]

[[package]]
name = "batch-summary-doubleword"
version = "0.1.0"
source = { virtual = "." }
dependencies = [
    { name = "odfpy" },
    { name = "openai" },
    { name = "pdfplumber" },
    { name = "pypdf" },
    { name = "python-docx" },
    { name = "python-dotenv" },
    { name = "python-pptx" },
]

[package.metadata]
requires-dist = [
    { name = "odfpy", specifier = ">=1.4.1" },
    { name = "openai", specifier = ">=2.14.0" },
    { name = "pdfplumber", specifier = ">=0.11.9" },
    { name = "pypdf", specifier = ">=6.6.0" },
    { name = "python-docx", specifier = ">=1.1.0" },
    { name = "python-dotenv", specifier = ">=1.1.0" },
    { name = "python-pptx", specifier = ">=1.0.0" },
]

[[package]]
name = "certifi"
version = "2026.1.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e0/2d/a891ca51311197f6ad14a7ef42e2399f36cf2f9bd44752b3dc4eab60fdc5/certifi-2026.1.4.tar.gz", hash = "sha256:ac726dd470482006e014ad384921ed6438c457018f4b3d204aea4281258b2120", size = 154268, upload-time = "2026-01-04T02:42:41.825Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e6/ad/3cc14f097111b4de0040c83a525973216457bbeeb63739ef1ed275c1c021/certifi-2026.1.4-py3-none-any.whl", hash = "sha256:9943707519e4add1115f44c2bc244f782c0249876bf51b6599fee1ffbedd685c", size = 152900, upload-time = "2026-01-04T02:42:40.15Z" },
]

[[package]]
name = "cffi"
version = "2.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pycparser", marker = "implementation_name != 'PyPy'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/eb/56/b1ba7935a17738ae8453301356628e8147c79dbb825bcbc73dc7401f9846/cffi-2.0.0.tar.gz", hash = "sha256:44d1b5909021139fe36001ae048dbdde8214afa20200eda0f64c068cac5d5529", size = 523588, upload-time = "2025-09-08T23:24:04.541Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ea/47/4f61023ea636104d4f16ab488e268b93008c3d0bb76893b1b31db1f96802/cffi-2.0.0-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:6d02d6655b0e54f54c4ef0b94eb6be0607b70853c45ce98bd278dc7de718be5d", size = 185271, upload-time = "2025-09-08T23:22:44.795Z" },
    { url = "https://files.pythonhosted.org/packages/df/a2/781b623f57358e360d62cdd7a8c681f074a71d445418a776eef0aadb4ab4/cffi-2.0.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:8eca2a813c1cb7ad4fb74d368c2ffbbb4789d377ee5bb8df98373c2cc0dee76c", size = 181048, upload-time = "2025-09-08T23:22:45.938Z" },
    { url = "https://files.pythonhosted.org/packages/ff/df/a4f0fbd47331ceeba3d37c2e51e9dfc9722498becbeec2bd8bc856c9538a/cffi-2.0.0-cp312-cp312-manylinux1_i686.manylinux2014_i686.manylinux_2_17_i686.manylinux_2_5_i686.whl", hash = "sha256:21d1152871b019407d8ac3985f6775c079416c282e431a4da6afe7aefd2bccbe", size = 212529, upload-time = "2025-09-08T23:22:47.349Z" },
    { url = "https://files.pythonhosted.org/packages/d5/72/12b5f8d3865bf0f87cf1404d8c374e7487dcf097a1c91c436e72e6badd83/cffi-2.0.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:b21e08af67b8a103c71a250401c78d5e0893beff75e28c53c98f4de42f774062", size = 220097, upload-time = "2025-09-08T23:22:48.677Z" },
    { url = "https://files.pythonhosted.org/packages/c2/95/7a135d52a50dfa7c882ab0ac17e8dc11cec9d55d2c18dda414c051c5e69e/cffi-2.0.0-cp312-cp312-manylinux2014_ppc64le.manylinux_2_17_ppc64le.whl", hash = "sha256:1e3a615586f05fc4065a8b22b8152f0c1b00cdbc60596d187c2a74f9e3036e4e", size = 207983, upload-time = "2025-09-08T23:22:50.06Z" },
    { url = "https://files.pythonhosted.org/packages/3a/c8/15cb9ada8895957ea171c62dc78ff3e99159ee7adb13c0123c001a2546c1/cffi-2.0.0-cp312-cp312-manylinux2014_s390x.manylinux_2_17_s390x.whl", hash = "sha256:81afed14892743bbe14dacb9e36d9e0e504cd204e0b165062c488942b9718037", size = 206519, upload-time = "2025-09-08T23:22:51.364Z" },
    { url = "https://files.pythonhosted.org/packages/78/2d/7fa73dfa841b5ac06c7b8855cfc18622132e365f5b81d02230333ff26e9e/cffi-2.0.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:3e17ed538242334bf70832644a32a7aae3d83b57567f9fd60a26257e992b79ba", size = 219572, upload-time = "2025-09-08T23:22:52.902Z" },
    { url = "https://files.pythonhosted.org/packages/07/e0/267e57e387b4ca276b90f0434ff88b2c2241ad72b16d31836adddfd6031b/cffi-2.0.0-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:3925dd22fa2b7699ed2617149842d2e6adde22b262fcbfada50e3d195e4b3a94", size = 222963, upload-time = "2025-09-08T23:22:54.518Z" },
    { url = "https://files.pythonhosted.org/packages/b6/75/1f2747525e06f53efbd878f4d03bac5b859cbc11c633d0fb81432d98a795/cffi-2.0.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:2c8f814d84194c9ea681642fd164267891702542f028a15fc97d4674b6206187", size = 221361, upload-time = "2025-09-08T23:22:55.867Z" },
    { url = "https://files.pythonhosted.org/packages/7b/2b/2b6435f76bfeb6bbf055596976da087377ede68df465419d192acf00c437/cffi-2.0.0-cp312-cp312-win32.whl", hash = "sha256:da902562c3e9c550df360bfa53c035b2f241fed6d9aef119048073680ace4a18", size = 172932, upload-time = "2025-09-08T23:22:57.188Z" },
    { url = "https://files.pythonhosted.org/packages/f8/ed/13bd4418627013bec4ed6e54283b1959cf6db888048c7cf4b4c3b5b36002/cffi-2.0.0-cp312-cp312-win_amd64.whl", hash = "sha256:da68248800ad6320861f129cd9c1bf96ca849a2771a59e0344e88681905916f5", size = 183557, upload-time = "2025-09-08T23:22:58.351Z" },
    { url = "https://files.pythonhosted.org/packages/95/31/9f7f93ad2f8eff1dbc1c3656d7ca5bfd8fb52c9d786b4dcf19b2d02217fa/cffi-2.0.0-cp312-cp312-win_arm64.whl", hash = "sha256:4671d9dd5ec934cb9a73e7ee9676f9362aba54f7f34910956b84d727b0d73fb6", size = 177762, upload-time = "2025-09-08T23:22:59.668Z" },
    { url = "https://files.pythonhosted.org/packages/4b/8d/a0a47a0c9e413a658623d014e91e74a50cdd2c423f7ccfd44086ef767f90/cffi-2.0.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:00bdf7acc5f795150faa6957054fbbca2439db2f775ce831222b66f192f03beb", size = 185230, upload-time = "2025-09-08T23:23:00.879Z" },
    { url = "https://files.pythonhosted.org/packages/4a/d2/a6c0296814556c68ee32009d9c2ad4f85f2707cdecfd7727951ec228005d/cffi-2.0.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:45d5e886156860dc35862657e1494b9bae8dfa63bf56796f2fb56e1679fc0bca", size = 181043, upload-time = "2025-09-08T23:23:02.231Z" },
    { url = "https://files.pythonhosted.org/packages/b0/1e/d22cc63332bd59b06481ceaac49d6c507598642e2230f201649058a7e704/cffi-2.0.0-cp313-cp313-manylinux1_i686.manylinux2014_i686.manylinux_2_17_i686.manylinux_2_5_i686.whl", hash = "sha256:07b271772c100085dd28b74fa0cd81c8fb1a3ba18b21e03d7c27f3436a10606b", size = 212446, upload-time = "2025-09-08T23:23:03.472Z" },
    { url = "https://files.pythonhosted.org/packages/a9/f5/a2c23eb03b61a0b8747f211eb716446c826ad66818ddc7810cc2cc19b3f2/cffi-2.0.0-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:d48a880098c96020b02d5a1f7d9251308510ce8858940e6fa99ece33f610838b", size = 220101, upload-time = "2025-09-08T23:23:04.792Z" },
    { url = "https://files.pythonhosted.org/packages/f2/7f/e6647792fc5850d634695bc0e6ab4111ae88e89981d35ac269956605feba/cffi-2.0.0-cp313-cp313-manylinux2014_ppc64le.manylinux_2_17_ppc64le.whl", hash = "sha256:f93fd8e5c8c0a4aa1f424d6173f14a892044054871c771f8566e4008eaa359d2", size = 207948, upload-time = "2025-09-08T23:23:06.127Z" },
    { url = "https://files.pythonhosted.org/packages/cb/1e/a5a1bd6f1fb30f22573f76533de12a00bf274abcdc55c8edab639078abb6/cffi-2.0.0-cp313-cp313-manylinux2014_s390x.manylinux_2_17_s390x.whl", hash = "sha256:dd4f05f54a52fb558f1ba9f528228066954fee3ebe629fc1660d874d040ae5a3", size = 206422, upload-time = "2025-09-08T23:23:07.753Z" },
    { url = "https://files.pythonhosted.org/packages/98/df/0a1755e750013a2081e863e7cd37e0cdd02664372c754e5560099eb7aa44/cffi-2.0.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:c8d3b5532fc71b7a77c09192b4a5a200ea992702734a2e9279a37f2478236f26", size = 219499, upload-time = "2025-09-08T23:23:09.648Z" },
    { url = "https://files.pythonhosted.org/packages/50/e1/a969e687fcf9ea58e6e2a928ad5e2dd88cc12f6f0ab477e9971f2309b57c/cffi-2.0.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:d9b29c1f0ae438d5ee9acb31cadee00a58c46cc9c0b2f9038c6b0b3470877a8c", size = 222928, upload-time = "2025-09-08T23:23:10.928Z" },
    { url = "https://files.pythonhosted.org/packages/36/54/0362578dd2c9e557a28ac77698ed67323ed5b9775ca9d3fe73fe191bb5d8/cffi-2.0.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:6d50360be4546678fc1b79ffe7a66265e28667840010348dd69a314145807a1b", size = 221302, upload-time = "2025-09-08T23:23:12.42Z" },
    { url = "https://files.pythonhosted.org/packages/eb/6d/bf9bda840d5f1dfdbf0feca87fbdb64a918a69bca42cfa0ba7b137c48cb8/cffi-2.0.0-cp313-cp313-win32.whl", hash = "sha256:74a03b9698e198d47562765773b4a8309919089150a0bb17d829ad7b44b60d27", size = 172909, upload-time = "2025-09-08T23:23:14.32Z" },
    { url = "https://files.pythonhosted.org/packages/37/18/6519e1ee6f5a1e579e04b9ddb6f1676c17368a7aba48299c3759bbc3c8b3/cffi-2.0.0-cp313-cp313-win_amd64.whl", hash = "sha256:19f705ada2530c1167abacb171925dd886168931e0a7b78f5bffcae5c6b5be75", size = 183402, upload-time = "2025-09-08T23:23:15.535Z" },
    { url = "https://files.pythonhosted.org/packages/cb/0e/02ceeec9a7d6ee63bb596121c2c8e9b3a9e150936f4fbef6ca1943e6137c/cffi-2.0.0-cp313-cp313-win_arm64.whl", hash = "sha256:256f80b80ca3853f90c21b23ee78cd008713787b1b1e93eae9f3d6a7134abd91", size = 177780, upload-time = "2025-09-08T23:23:16.761Z" },
    { url = "https://files.pythonhosted.org/packages/92/c4/3ce07396253a83250ee98564f8d7e9789fab8e58858f35d07a9a2c78de9f/cffi-2.0.0-cp314-cp314-macosx_10_13_x86_64.whl", hash = "sha256:fc33c5141b55ed366cfaad382df24fe7dcbc686de5be719b207bb248e3053dc5", size = 185320, upload-time = "2025-09-08T23:23:18.087Z" },
    { url = "https://files.pythonhosted.org/packages/59/dd/27e9fa567a23931c838c6b02d0764611c62290062a6d4e8ff7863daf9730/cffi-2.0.0-cp314-cp314-macosx_11_0_arm64.whl", hash = "sha256:c654de545946e0db659b3400168c9ad31b5d29593291482c43e3564effbcee13", size = 181487, upload-time = "2025-09-08T23:23:19.622Z" },
    { url = "https://files.pythonhosted.org/packages/d6/43/0e822876f87ea8a4ef95442c3d766a06a51fc5298823f884ef87aaad168c/cffi-2.0.0-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:24b6f81f1983e6df8db3adc38562c83f7d4a0c36162885ec7f7b77c7dcbec97b", size = 220049, upload-time = "2025-09-08T23:23:20.853Z" },
    { url = "https://files.pythonhosted.org/packages/b4/89/76799151d9c2d2d1ead63c2429da9ea9d7aac304603de0c6e8764e6e8e70/cffi-2.0.0-cp314-cp314-manylinux2014_ppc64le.manylinux_2_17_ppc64le.whl", hash = "sha256:12873ca6cb9b0f0d3a0da705d6086fe911591737a59f28b7936bdfed27c0d47c", size = 207793, upload-time = "2025-09-08T23:23:22.08Z" },
    { url = "https://files.pythonhosted.org/packages/bb/dd/3465b14bb9e24ee24cb88c9e3730f6de63111fffe513492bf8c808a3547e/cffi-2.0.0-cp314-cp314-manylinux2014_s390x.manylinux_2_17_s390x.whl", hash = "sha256:d9b97165e8aed9272a6bb17c01e3cc5871a594a446ebedc996e2397a1c1ea8ef", size = 206300, upload-time = "2025-09-08T23:23:23.314Z" },
    { url = "https://files.pythonhosted.org/packages/47/d9/d83e293854571c877a92da46fdec39158f8d7e68da75bf73581225d28e90/cffi-2.0.0-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:afb8db5439b81cf9c9d0c80404b60c3cc9c3add93e114dcae767f1477cb53775", size = 219244, upload-time = "2025-09-08T23:23:24.541Z" },
    { url = "https://files.pythonhosted.org/packages/2b/0f/1f177e3683aead2bb00f7679a16451d302c436b5cbf2505f0ea8146ef59e/cffi-2.0.0-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:737fe7d37e1a1bffe70bd5754ea763a62a066dc5913ca57e957824b72a85e205", size = 222828, upload-time = "2025-09-08T23:23:26.143Z" },
    { url = "https://files.pythonhosted.org/packages/c6/0f/cafacebd4b040e3119dcb32fed8bdef8dfe94da653155f9d0b9dc660166e/cffi-2.0.0-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:38100abb9d1b1435bc4cc340bb4489635dc2f0da7456590877030c9b3d40b0c1", size = 220926, upload-time = "2025-09-08T23:23:27.873Z" },
    { url = "https://files.pythonhosted.org/packages/3e/aa/df335faa45b395396fcbc03de2dfcab242cd61a9900e914fe682a59170b1/cffi-2.0.0-cp314-cp314-win32.whl", hash = "sha256:087067fa8953339c723661eda6b54bc98c5625757ea62e95eb4898ad5e776e9f", size = 175328, upload-time = "2025-09-08T23:23:44.61Z" },
    { url = "https://files.pythonhosted.org/packages/bb/92/882c2d30831744296ce713f0feb4c1cd30f346ef747b530b5318715cc367/cffi-2.0.0-cp314-cp314-win_amd64.whl", hash = "sha256:203a48d1fb583fc7d78a4c6655692963b860a417c0528492a6bc21f1aaefab25", size = 185650, upload-time = "2025-09-08T23:23:45.848Z" },
    { url = "https://files.pythonhosted.org/packages/9f/2c/98ece204b9d35a7366b5b2c6539c350313ca13932143e79dc133ba757104/cffi-2.0.0-cp314-cp314-win_arm64.whl", hash = "sha256:dbd5c7a25a7cb98f5ca55d258b103a2054f859a46ae11aaf23134f9cc0d356ad", size = 180687, upload-time = "2025-09-08T23:23:47.105Z" },
    { url = "https://files.pythonhosted.org/packages/3e/61/c768e4d548bfa607abcda77423448df8c471f25dbe64fb2ef6d555eae006/cffi-2.0.0-cp314-cp314t-macosx_10_13_x86_64.whl", hash = "sha256:9a67fc9e8eb39039280526379fb3a70023d77caec1852002b4da7e8b270c4dd9", size = 188773, upload-time = "2025-09-08T23:23:29.347Z" },
    { url = "https://files.pythonhosted.org/packages/2c/ea/5f76bce7cf6fcd0ab1a1058b5af899bfbef198bea4d5686da88471ea0336/cffi-2.0.0-cp314-cp314t-macosx_11_0_arm64.whl", hash = "sha256:7a66c7204d8869299919db4d5069a82f1561581af12b11b3c9f48c584eb8743d", size = 185013, upload-time = "2025-09-08T23:23:30.63Z" },
    { url = "https://files.pythonhosted.org/packages/be/b4/c56878d0d1755cf9caa54ba71e5d049479c52f9e4afc230f06822162ab2f/cffi-2.0.0-cp314-cp314t-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:7cc09976e8b56f8cebd752f7113ad07752461f48a58cbba644139015ac24954c", size = 221593, upload-time = "2025-09-08T23:23:31.91Z" },
    { url = "https://files.pythonhosted.org/packages/e0/0d/eb704606dfe8033e7128df5e90fee946bbcb64a04fcdaa97321309004000/cffi-2.0.0-cp314-cp314t-manylinux2014_ppc64le.manylinux_2_17_ppc64le.whl", hash = "sha256:92b68146a71df78564e4ef48af17551a5ddd142e5190cdf2c5624d0c3ff5b2e8", size = 209354, upload-time = "2025-09-08T23:23:33.214Z" },
    { url = "https://files.pythonhosted.org/packages/d8/19/3c435d727b368ca475fb8742ab97c9cb13a0de600ce86f62eab7fa3eea60/cffi-2.0.0-cp314-cp314t-manylinux2014_s390x.manylinux_2_17_s390x.whl", hash = "sha256:b1e74d11748e7e98e2f426ab176d4ed720a64412b6a15054378afdb71e0f37dc", size = 208480, upload-time = "2025-09-08T23:23:34.495Z" },
    { url = "https://files.pythonhosted.org/packages/d0/44/681604464ed9541673e486521497406fadcc15b5217c3e326b061696899a/cffi-2.0.0-cp314-cp314t-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:28a3a209b96630bca57cce802da70c266eb08c6e97e5afd61a75611ee6c64592", size = 221584, upload-time = "2025-09-08T23:23:36.096Z" },
    { url = "https://files.pythonhosted.org/packages/25/8e/342a504ff018a2825d395d44d63a767dd8ebc927ebda557fecdaca3ac33a/cffi-2.0.0-cp314-cp314t-musllinux_1_2_aarch64.whl", hash = "sha256:7553fb2090d71822f02c629afe6042c299edf91ba1bf94951165613553984512", size = 224443, upload-time = "2025-09-08T23:23:37.328Z" },
    { url = "https://files.pythonhosted.org/packages/e1/5e/b666bacbbc60fbf415ba9988324a132c9a7a0448a9a8f125074671c0f2c3/cffi-2.0.0-cp314-cp314t-musllinux_1_2_x86_64.whl", hash = "sha256:6c6c373cfc5c83a975506110d17457138c8c63016b563cc9ed6e056a82f13ce4", size = 223437, upload-time = "2025-09-08T23:23:38.945Z" },
    { url = "https://files.pythonhosted.org/packages/a0/1d/ec1a60bd1a10daa292d3cd6bb0b359a81607154fb8165f3ec95fe003b85c/cffi-2.0.0-cp314-cp314t-win32.whl", hash = "sha256:1fc9ea04857caf665289b7a75923f2c6ed559b8298a1b8c49e59f7dd95c8481e", size = 180487, upload-time = "2025-09-08T23:23:40.423Z" },
    { url = "https://files.pythonhosted.org/packages/bf/41/4c1168c74fac325c0c8156f04b6749c8b6a8f405bbf91413ba088359f60d/cffi-2.0.0-cp314-cp314t-win_amd64.whl", hash = "sha256:d68b6cef7827e8641e8ef16f4494edda8b36104d79773a334beaa1e3521430f6", size = 191726, upload-time = "2025-09-08T23:23:41.742Z" },
    { url = "https://files.pythonhosted.org/packages/ae/3a/dbeec9d1ee0844c679f6bb5d6ad4e9f198b1224f4e7a32825f47f6192b0c/cffi-2.0.0-cp314-cp314t-win_arm64.whl", hash = "sha256:0a1527a803f0a659de1af2e1fd700213caba79377e27e4693648c2923da066f9", size = 184195, upload-time = "2025-09-08T23:23:43.004Z" },
]

[[package]]
name = "charset-normalizer"
version = "3.4.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/13/69/33ddede1939fdd074bce5434295f38fae7136463422fe4fd3e0e89b98062/charset_normalizer-3.4.4.tar.gz", hash = "sha256:94537985111c35f28720e43603b8e7b43a6ecfb2ce1d3058bbe955b73404e21a", size = 129418, upload-time = "2025-10-14T04:42:32.879Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f3/85/1637cd4af66fa687396e757dec650f28025f2a2f5a5531a3208dc0ec43f2/charset_normalizer-3.4.4-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:0a98e6759f854bd25a58a73fa88833fba3b7c491169f86ce1180c948ab3fd394", size = 208425, upload-time = "2025-10-14T04:40:53.353Z" },
    { url = "https://files.pythonhosted.org/packages/9d/6a/04130023fef2a0d9c62d0bae2649b69f7b7d8d24ea5536feef50551029df/charset_normalizer-3.4.4-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:b5b290ccc2a263e8d185130284f8501e3e36c5e02750fc6b6bdeb2e9e96f1e25", size = 148162, upload-time = "2025-10-14T04:40:54.558Z" },
    { url = "https://files.pythonhosted.org/packages/78/29/62328d79aa60da22c9e0b9a66539feae06ca0f5a4171ac4f7dc285b83688/charset_normalizer-3.4.4-cp312-cp312-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:74bb723680f9f7a6234dcf67aea57e708ec1fbdf5699fb91dfd6f511b0a320ef", size = 144558, upload-time = "2025-10-14T04:40:55.677Z" },
    { url = "https://files.pythonhosted.org/packages/86/bb/b32194a4bf15b88403537c2e120b817c61cd4ecffa9b6876e941c3ee38fe/charset_normalizer-3.4.4-cp312-cp312-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:f1e34719c6ed0b92f418c7c780480b26b5d9c50349e9a9af7d76bf757530350d", size = 161497, upload-time = "2025-10-14T04:40:57.217Z" },
    { url = "https://files.pythonhosted.org/packages/19/89/a54c82b253d5b9b111dc74aca196ba5ccfcca8242d0fb64146d4d3183ff1/charset_normalizer-3.4.4-cp312-cp312-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:2437418e20515acec67d86e12bf70056a33abdacb5cb1655042f6538d6b085a8", size = 159240, upload-time = "2025-10-14T04:40:58.358Z" },
    { url = "https://files.pythonhosted.org/packages/c0/10/d20b513afe03acc89ec33948320a5544d31f21b05368436d580dec4e234d/charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:11d694519d7f29d6cd09f6ac70028dba10f92f6cdd059096db198c283794ac86", size = 153471, upload-time = "2025-10-14T04:40:59.468Z" },
    { url = "https://files.pythonhosted.org/packages/61/fa/fbf177b55bdd727010f9c0a3c49eefa1d10f960e5f09d1d887bf93c2e698/charset_normalizer-3.4.4-cp312-cp312-manylinux_2_31_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:ac1c4a689edcc530fc9d9aa11f5774b9e2f33f9a0c6a57864e90908f5208d30a", size = 150864, upload-time = "2025-10-14T04:41:00.623Z" },
    { url = "https://files.pythonhosted.org/packages/05/12/9fbc6a4d39c0198adeebbde20b619790e9236557ca59fc40e0e3cebe6f40/charset_normalizer-3.4.4-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:21d142cc6c0ec30d2efee5068ca36c128a30b0f2c53c1c07bd78cb6bc1d3be5f", size = 150647, upload-time = "2025-10-14T04:41:01.754Z" },
    { url = "https://files.pythonhosted.org/packages/ad/1f/6a9a593d52e3e8c5d2b167daf8c6b968808efb57ef4c210acb907c365bc4/charset_normalizer-3.4.4-cp312-cp312-musllinux_1_2_armv7l.whl", hash = "sha256:5dbe56a36425d26d6cfb40ce79c314a2e4dd6211d51d6d2191c00bed34f354cc", size = 145110, upload-time = "2025-10-14T04:41:03.231Z" },
    { url = "https://files.pythonhosted.org/packages/30/42/9a52c609e72471b0fc54386dc63c3781a387bb4fe61c20231a4ebcd58bdd/charset_normalizer-3.4.4-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:5bfbb1b9acf3334612667b61bd3002196fe2a1eb4dd74d247e0f2a4d50ec9bbf", size = 162839, upload-time = "2025-10-14T04:41:04.715Z" },
    { url = "https://files.pythonhosted.org/packages/c4/5b/c0682bbf9f11597073052628ddd38344a3d673fda35a36773f7d19344b23/charset_normalizer-3.4.4-cp312-cp312-musllinux_1_2_riscv64.whl", hash = "sha256:d055ec1e26e441f6187acf818b73564e6e6282709e9bcb5b63f5b23068356a15", size = 150667, upload-time = "2025-10-14T04:41:05.827Z" },
    { url = "https://files.pythonhosted.org/packages/e4/24/a41afeab6f990cf2daf6cb8c67419b63b48cf518e4f56022230840c9bfb2/charset_normalizer-3.4.4-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:af2d8c67d8e573d6de5bc30cdb27e9b95e49115cd9baad5ddbd1a6207aaa82a9", size = 160535, upload-time = "2025-10-14T04:41:06.938Z" },
    { url = "https://files.pythonhosted.org/packages/2a/e5/6a4ce77ed243c4a50a1fecca6aaaab419628c818a49434be428fe24c9957/charset_normalizer-3.4.4-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:780236ac706e66881f3b7f2f32dfe90507a09e67d1d454c762cf642e6e1586e0", size = 154816, upload-time = "2025-10-14T04:41:08.101Z" },
    { url = "https://files.pythonhosted.org/packages/a8/ef/89297262b8092b312d29cdb2517cb1237e51db8ecef2e9af5edbe7b683b1/charset_normalizer-3.4.4-cp312-cp312-win32.whl", hash = "sha256:5833d2c39d8896e4e19b689ffc198f08ea58116bee26dea51e362ecc7cd3ed26", size = 99694, upload-time = "2025-10-14T04:41:09.23Z" },
    { url = "https://files.pythonhosted.org/packages/3d/2d/1e5ed9dd3b3803994c155cd9aacb60c82c331bad84daf75bcb9c91b3295e/charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl", hash = "sha256:a79cfe37875f822425b89a82333404539ae63dbdddf97f84dcbc3d339aae9525", size = 107131, upload-time = "2025-10-14T04:41:10.467Z" },
    { url = "https://files.pythonhosted.org/packages/d0/d9/0ed4c7098a861482a7b6a95603edce4c0d9db2311af23da1fb2b75ec26fc/charset_normalizer-3.4.4-cp312-cp312-win_arm64.whl", hash = "sha256:376bec83a63b8021bb5c8ea75e21c4ccb86e7e45ca4eb81146091b56599b80c3", size = 100390, upload-time = "2025-10-14T04:41:11.915Z" },
    { url = "https://files.pythonhosted.org/packages/97/45/4b3a1239bbacd321068ea6e7ac28875b03ab8bc0aa0966452db17cd36714/charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:e1f185f86a6f3403aa2420e815904c67b2f9ebc443f045edd0de921108345794", size = 208091, upload-time = "2025-10-14T04:41:13.346Z" },
    { url = "https://files.pythonhosted.org/packages/7d/62/73a6d7450829655a35bb88a88fca7d736f9882a27eacdca2c6d505b57e2e/charset_normalizer-3.4.4-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:6b39f987ae8ccdf0d2642338faf2abb1862340facc796048b604ef14919e55ed", size = 147936, upload-time = "2025-10-14T04:41:14.461Z" },
    { url = "https://files.pythonhosted.org/packages/89/c5/adb8c8b3d6625bef6d88b251bbb0d95f8205831b987631ab0c8bb5d937c2/charset_normalizer-3.4.4-cp313-cp313-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:3162d5d8ce1bb98dd51af660f2121c55d0fa541b46dff7bb9b9f86ea1d87de72", size = 144180, upload-time = "2025-10-14T04:41:15.588Z" },
    { url = "https://files.pythonhosted.org/packages/91/ed/9706e4070682d1cc219050b6048bfd293ccf67b3d4f5a4f39207453d4b99/charset_normalizer-3.4.4-cp313-cp313-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:81d5eb2a312700f4ecaa977a8235b634ce853200e828fbadf3a9c50bab278328", size = 161346, upload-time = "2025-10-14T04:41:16.738Z" },
    { url = "https://files.pythonhosted.org/packages/d5/0d/031f0d95e4972901a2f6f09ef055751805ff541511dc1252ba3ca1f80cf5/charset_normalizer-3.4.4-cp313-cp313-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:5bd2293095d766545ec1a8f612559f6b40abc0eb18bb2f5d1171872d34036ede", size = 158874, upload-time = "2025-10-14T04:41:17.923Z" },
    { url = "https://files.pythonhosted.org/packages/f5/83/6ab5883f57c9c801ce5e5677242328aa45592be8a00644310a008d04f922/charset_normalizer-3.4.4-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:a8a8b89589086a25749f471e6a900d3f662d1d3b6e2e59dcecf787b1cc3a1894", size = 153076, upload-time = "2025-10-14T04:41:19.106Z" },
    { url = "https://files.pythonhosted.org/packages/75/1e/5ff781ddf5260e387d6419959ee89ef13878229732732ee73cdae01800f2/charset_normalizer-3.4.4-cp313-cp313-manylinux_2_31_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:bc7637e2f80d8530ee4a78e878bce464f70087ce73cf7c1caf142416923b98f1", size = 150601, upload-time = "2025-10-14T04:41:20.245Z" },
    { url = "https://files.pythonhosted.org/packages/d7/57/71be810965493d3510a6ca79b90c19e48696fb1ff964da319334b12677f0/charset_normalizer-3.4.4-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:f8bf04158c6b607d747e93949aa60618b61312fe647a6369f88ce2ff16043490", size = 150376, upload-time = "2025-10-14T04:41:21.398Z" },
    { url = "https://files.pythonhosted.org/packages/e5/d5/c3d057a78c181d007014feb7e9f2e65905a6c4ef182c0ddf0de2924edd65/charset_normalizer-3.4.4-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:554af85e960429cf30784dd47447d5125aaa3b99a6f0683589dbd27e2f45da44", size = 144825, upload-time = "2025-10-14T04:41:22.583Z" },
    { url = "https://files.pythonhosted.org/packages/e6/8c/d0406294828d4976f275ffbe66f00266c4b3136b7506941d87c00cab5272/charset_normalizer-3.4.4-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:74018750915ee7ad843a774364e13a3db91682f26142baddf775342c3f5b1133", size = 162583, upload-time = "2025-10-14T04:41:23.754Z" },
    { url = "https://files.pythonhosted.org/packages/d7/24/e2aa1f18c8f15c4c0e932d9287b8609dd30ad56dbe41d926bd846e22fb8d/charset_normalizer-3.4.4-cp313-cp313-musllinux_1_2_riscv64.whl", hash = "sha256:c0463276121fdee9c49b98908b3a89c39be45d86d1dbaa22957e38f6321d4ce3", size = 150366, upload-time = "2025-10-14T04:41:25.27Z" },
    { url = "https://files.pythonhosted.org/packages/e4/5b/1e6160c7739aad1e2df054300cc618b06bf784a7a164b0f238360721ab86/charset_normalizer-3.4.4-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:362d61fd13843997c1c446760ef36f240cf81d3ebf74ac62652aebaf7838561e", size = 160300, upload-time = "2025-10-14T04:41:26.725Z" },
    { url = "https://files.pythonhosted.org/packages/7a/10/f882167cd207fbdd743e55534d5d9620e095089d176d55cb22d5322f2afd/charset_normalizer-3.4.4-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:9a26f18905b8dd5d685d6d07b0cdf98a79f3c7a918906af7cc143ea2e164c8bc", size = 154465, upload-time = "2025-10-14T04:41:28.322Z" },
    { url = "https://files.pythonhosted.org/packages/89/66/c7a9e1b7429be72123441bfdbaf2bc13faab3f90b933f664db506dea5915/charset_normalizer-3.4.4-cp313-cp313-win32.whl", hash = "sha256:9b35f4c90079ff2e2edc5b26c0c77925e5d2d255c42c74fdb70fb49b172726ac", size = 99404, upload-time = "2025-10-14T04:41:29.95Z" },
    { url = "https://files.pythonhosted.org/packages/c4/26/b9924fa27db384bdcd97ab83b4f0a8058d96ad9626ead570674d5e737d90/charset_normalizer-3.4.4-cp313-cp313-win_amd64.whl", hash = "sha256:b435cba5f4f750aa6c0a0d92c541fb79f69a387c91e61f1795227e4ed9cece14", size = 107092, upload-time = "2025-10-14T04:41:31.188Z" },
    { url = "https://files.pythonhosted.org/packages/af/8f/3ed4bfa0c0c72a7ca17f0380cd9e4dd842b09f664e780c13cff1dcf2ef1b/charset_normalizer-3.4.4-cp313-cp313-win_arm64.whl", hash = "sha256:542d2cee80be6f80247095cc36c418f7bddd14f4a6de45af91dfad36d817bba2", size = 100408, upload-time = "2025-10-14T04:41:32.624Z" },
    { url = "https://files.pythonhosted.org/packages/2a/35/7051599bd493e62411d6ede36fd5af83a38f37c4767b92884df7301db25d/charset_normalizer-3.4.4-cp314-cp314-macosx_10_13_universal2.whl", hash = "sha256:da3326d9e65ef63a817ecbcc0df6e94463713b754fe293eaa03da99befb9a5bd", size = 207746, upload-time = "2025-10-14T04:41:33.773Z" },
    { url = "https://files.pythonhosted.org/packages/10/9a/97c8d48ef10d6cd4fcead2415523221624bf58bcf68a802721a6bc807c8f/charset_normalizer-3.4.4-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:8af65f14dc14a79b924524b1e7fffe304517b2bff5a58bf64f30b98bbc5079eb", size = 147889, upload-time = "2025-10-14T04:41:34.897Z" },
    { url = "https://files.pythonhosted.org/packages/10/bf/979224a919a1b606c82bd2c5fa49b5c6d5727aa47b4312bb27b1734f53cd/charset_normalizer-3.4.4-cp314-cp314-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:74664978bb272435107de04e36db5a9735e78232b85b77d45cfb38f758efd33e", size = 143641, upload-time = "2025-10-14T04:41:36.116Z" },
    { url = "https://files.pythonhosted.org/packages/ba/33/0ad65587441fc730dc7bd90e9716b30b4702dc7b617e6ba4997dc8651495/charset_normalizer-3.4.4-cp314-cp314-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:752944c7ffbfdd10c074dc58ec2d5a8a4cd9493b314d367c14d24c17684ddd14", size = 160779, upload-time = "2025-10-14T04:41:37.229Z" },
    { url = "https://files.pythonhosted.org/packages/67/ed/331d6b249259ee71ddea93f6f2f0a56cfebd46938bde6fcc6f7b9a3d0e09/charset_normalizer-3.4.4-cp314-cp314-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:d1f13550535ad8cff21b8d757a3257963e951d96e20ec82ab44bc64aeb62a191", size = 159035, upload-time = "2025-10-14T04:41:38.368Z" },
    { url = "https://files.pythonhosted.org/packages/67/ff/f6b948ca32e4f2a4576aa129d8bed61f2e0543bf9f5f2b7fc3758ed005c9/charset_normalizer-3.4.4-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:ecaae4149d99b1c9e7b88bb03e3221956f68fd6d50be2ef061b2381b61d20838", size = 152542, upload-time = "2025-10-14T04:41:39.862Z" },
    { url = "https://files.pythonhosted.org/packages/16/85/276033dcbcc369eb176594de22728541a925b2632f9716428c851b149e83/charset_normalizer-3.4.4-cp314-cp314-manylinux_2_31_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:cb6254dc36b47a990e59e1068afacdcd02958bdcce30bb50cc1700a8b9d624a6", size = 149524, upload-time = "2025-10-14T04:41:41.319Z" },
    { url = "https://files.pythonhosted.org/packages/9e/f2/6a2a1f722b6aba37050e626530a46a68f74e63683947a8acff92569f979a/charset_normalizer-3.4.4-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:c8ae8a0f02f57a6e61203a31428fa1d677cbe50c93622b4149d5c0f319c1d19e", size = 150395, upload-time = "2025-10-14T04:41:42.539Z" },
    { url = "https://files.pythonhosted.org/packages/60/bb/2186cb2f2bbaea6338cad15ce23a67f9b0672929744381e28b0592676824/charset_normalizer-3.4.4-cp314-cp314-musllinux_1_2_armv7l.whl", hash = "sha256:47cc91b2f4dd2833fddaedd2893006b0106129d4b94fdb6af1f4ce5a9965577c", size = 143680, upload-time = "2025-10-14T04:41:43.661Z" },
    { url = "https://files.pythonhosted.org/packages/7d/a5/bf6f13b772fbb2a90360eb620d52ed8f796f3c5caee8398c3b2eb7b1c60d/charset_normalizer-3.4.4-cp314-cp314-musllinux_1_2_ppc64le.whl", hash = "sha256:82004af6c302b5d3ab2cfc4cc5f29db16123b1a8417f2e25f9066f91d4411090", size = 162045, upload-time = "2025-10-14T04:41:44.821Z" },
    { url = "https://files.pythonhosted.org/packages/df/c5/d1be898bf0dc3ef9030c3825e5d3b83f2c528d207d246cbabe245966808d/charset_normalizer-3.4.4-cp314-cp314-musllinux_1_2_riscv64.whl", hash = "sha256:2b7d8f6c26245217bd2ad053761201e9f9680f8ce52f0fcd8d0755aeae5b2152", size = 149687, upload-time = "2025-10-14T04:41:46.442Z" },
    { url = "https://files.pythonhosted.org/packages/a5/42/90c1f7b9341eef50c8a1cb3f098ac43b0508413f33affd762855f67a410e/charset_normalizer-3.4.4-cp314-cp314-musllinux_1_2_s390x.whl", hash = "sha256:799a7a5e4fb2d5898c60b640fd4981d6a25f1c11790935a44ce38c54e985f828", size = 160014, upload-time = "2025-10-14T04:41:47.631Z" },
    { url = "https://files.pythonhosted.org/packages/76/be/4d3ee471e8145d12795ab655ece37baed0929462a86e72372fd25859047c/charset_normalizer-3.4.4-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:99ae2cffebb06e6c22bdc25801d7b30f503cc87dbd283479e7b606f70aff57ec", size = 154044, upload-time = "2025-10-14T04:41:48.81Z" },
    { url = "https://files.pythonhosted.org/packages/b0/6f/8f7af07237c34a1defe7defc565a9bc1807762f672c0fde711a4b22bf9c0/charset_normalizer-3.4.4-cp314-cp314-win32.whl", hash = "sha256:f9d332f8c2a2fcbffe1378594431458ddbef721c1769d78e2cbc06280d8155f9", size = 99940, upload-time = "2025-10-14T04:41:49.946Z" },
    { url = "https://files.pythonhosted.org/packages/4b/51/8ade005e5ca5b0d80fb4aff72a3775b325bdc3d27408c8113811a7cbe640/charset_normalizer-3.4.4-cp314-cp314-win_amd64.whl", hash = "sha256:8a6562c3700cce886c5be75ade4a5db4214fda19fede41d9792d100288d8f94c", size = 107104, upload-time = "2025-10-14T04:41:51.051Z" },
    { url = "https://files.pythonhosted.org/packages/da/5f/6b8f83a55bb8278772c5ae54a577f3099025f9ade59d0136ac24a0df4bde/charset_normalizer-3.4.4-cp314-cp314-win_arm64.whl", hash = "sha256:de00632ca48df9daf77a2c65a484531649261ec9f25489917f09e455cb09ddb2", size = 100743, upload-time = "2025-10-14T04:41:52.122Z" },
    { url = "https://files.pythonhosted.org/packages/0a/4c/925909008ed5a988ccbb72dcc897407e5d6d3bd72410d69e051fc0c14647/charset_normalizer-3.4.4-py3-none-any.whl", hash = "sha256:7a32c560861a02ff789ad905a2fe94e3f840803362c84fecf1851cb4cf3dc37f", size = 53402, upload-time = "2025-10-14T04:42:31.76Z" },
]

[[package]]
name = "colorama"
version = "0.4.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d8/53/6f443c9a4a8358a93a6792e2acffb9d9d5cb0a5cfd8802644b7b1c9a02e4/colorama-0.4.6.tar.gz", hash = "sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44", size = 27697, upload-time = "2022-10-25T02:36:22.414Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl", hash = "sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6", size = 25335, upload-time = "2022-10-25T02:36:20.889Z" },
]

[[package]]
name = "cryptography"
version = "46.0.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "cffi", marker = "platform_python_implementation != 'PyPy'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/9f/33/c00162f49c0e2fe8064a62cb92b93e50c74a72bc370ab92f86112b33ff62/cryptography-46.0.3.tar.gz", hash = "sha256:a8b17438104fed022ce745b362294d9ce35b4c2e45c1d958ad4a4b019285f4a1", size = 749258, upload-time = "2025-10-15T23:18:31.74Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/1d/42/9c391dd801d6cf0d561b5890549d4b27bafcc53b39c31a817e69d87c625b/cryptography-46.0.3-cp311-abi3-macosx_10_9_universal2.whl", hash = "sha256:109d4ddfadf17e8e7779c39f9b18111a09efb969a301a31e987416a0191ed93a", size = 7225004, upload-time = "2025-10-15T23:16:52.239Z" },
    { url = "https://files.pythonhosted.org/packages/1c/67/38769ca6b65f07461eb200e85fc1639b438bdc667be02cf7f2cd6a64601c/cryptography-46.0.3-cp311-abi3-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:09859af8466b69bc3c27bdf4f5d84a665e0f7ab5088412e9e2ec49758eca5cbc", size = 4296667, upload-time = "2025-10-15T23:16:54.369Z" },
    { url = "https://files.pythonhosted.org/packages/5c/49/498c86566a1d80e978b42f0d702795f69887005548c041636df6ae1ca64c/cryptography-46.0.3-cp311-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:01ca9ff2885f3acc98c29f1860552e37f6d7c7d013d7334ff2a9de43a449315d", size = 4450807, upload-time = "2025-10-15T23:16:56.414Z" },
    { url = "https://files.pythonhosted.org/packages/4b/0a/863a3604112174c8624a2ac3c038662d9e59970c7f926acdcfaed8d61142/cryptography-46.0.3-cp311-abi3-manylinux_2_28_aarch64.whl", hash = "sha256:6eae65d4c3d33da080cff9c4ab1f711b15c1d9760809dad6ea763f3812d254cb", size = 4299615, upload-time = "2025-10-15T23:16:58.442Z" },
    { url = "https://files.pythonhosted.org/packages/64/02/b73a533f6b64a69f3cd3872acb6ebc12aef924d8d103133bb3ea750dc703/cryptography-46.0.3-cp311-abi3-manylinux_2_28_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:e5bf0ed4490068a2e72ac03d786693adeb909981cc596425d09032d372bcc849", size = 4016800, upload-time = "2025-10-15T23:17:00.378Z" },
    { url = "https://files.pythonhosted.org/packages/25/d5/16e41afbfa450cde85a3b7ec599bebefaef16b5c6ba4ec49a3532336ed72/cryptography-46.0.3-cp311-abi3-manylinux_2_28_ppc64le.whl", hash = "sha256:5ecfccd2329e37e9b7112a888e76d9feca2347f12f37918facbb893d7bb88ee8", size = 4984707, upload-time = "2025-10-15T23:17:01.98Z" },
    { url = "https://files.pythonhosted.org/packages/c9/56/e7e69b427c3878352c2fb9b450bd0e19ed552753491d39d7d0a2f5226d41/cryptography-46.0.3-cp311-abi3-manylinux_2_28_x86_64.whl", hash = "sha256:a2c0cd47381a3229c403062f764160d57d4d175e022c1df84e168c6251a22eec", size = 4482541, upload-time = "2025-10-15T23:17:04.078Z" },
    { url = "https://files.pythonhosted.org/packages/78/f6/50736d40d97e8483172f1bb6e698895b92a223dba513b0ca6f06b2365339/cryptography-46.0.3-cp311-abi3-manylinux_2_34_aarch64.whl", hash = "sha256:549e234ff32571b1f4076ac269fcce7a808d3bf98b76c8dd560e42dbc66d7d91", size = 4299464, upload-time = "2025-10-15T23:17:05.483Z" },
    { url = "https://files.pythonhosted.org/packages/00/de/d8e26b1a855f19d9994a19c702fa2e93b0456beccbcfe437eda00e0701f2/cryptography-46.0.3-cp311-abi3-manylinux_2_34_ppc64le.whl", hash = "sha256:c0a7bb1a68a5d3471880e264621346c48665b3bf1c3759d682fc0864c540bd9e", size = 4950838, upload-time = "2025-10-15T23:17:07.425Z" },
    { url = "https://files.pythonhosted.org/packages/8f/29/798fc4ec461a1c9e9f735f2fc58741b0daae30688f41b2497dcbc9ed1355/cryptography-46.0.3-cp311-abi3-manylinux_2_34_x86_64.whl", hash = "sha256:10b01676fc208c3e6feeb25a8b83d81767e8059e1fe86e1dc62d10a3018fa926", size = 4481596, upload-time = "2025-10-15T23:17:09.343Z" },
    { url = "https://files.pythonhosted.org/packages/15/8d/03cd48b20a573adfff7652b76271078e3045b9f49387920e7f1f631d125e/cryptography-46.0.3-cp311-abi3-musllinux_1_2_aarch64.whl", hash = "sha256:0abf1ffd6e57c67e92af68330d05760b7b7efb243aab8377e583284dbab72c71", size = 4426782, upload-time = "2025-10-15T23:17:11.22Z" },
    { url = "https://files.pythonhosted.org/packages/fa/b1/ebacbfe53317d55cf33165bda24c86523497a6881f339f9aae5c2e13e57b/cryptography-46.0.3-cp311-abi3-musllinux_1_2_x86_64.whl", hash = "sha256:a04bee9ab6a4da801eb9b51f1b708a1b5b5c9eb48c03f74198464c66f0d344ac", size = 4698381, upload-time = "2025-10-15T23:17:12.829Z" },
    { url = "https://files.pythonhosted.org/packages/96/92/8a6a9525893325fc057a01f654d7efc2c64b9de90413adcf605a85744ff4/cryptography-46.0.3-cp311-abi3-win32.whl", hash = "sha256:f260d0d41e9b4da1ed1e0f1ce571f97fe370b152ab18778e9e8f67d6af432018", size = 3055988, upload-time = "2025-10-15T23:17:14.65Z" },
    { url = "https://files.pythonhosted.org/packages/7e/bf/80fbf45253ea585a1e492a6a17efcb93467701fa79e71550a430c5e60df0/cryptography-46.0.3-cp311-abi3-win_amd64.whl", hash = "sha256:a9a3008438615669153eb86b26b61e09993921ebdd75385ddd748702c5adfddb", size = 3514451, upload-time = "2025-10-15T23:17:16.142Z" },
    { url = "https://files.pythonhosted.org/packages/2e/af/9b302da4c87b0beb9db4e756386a7c6c5b8003cd0e742277888d352ae91d/cryptography-46.0.3-cp311-abi3-win_arm64.whl", hash = "sha256:5d7f93296ee28f68447397bf5198428c9aeeab45705a55d53a6343455dcb2c3c", size = 2928007, upload-time = "2025-10-15T23:17:18.04Z" },
    { url = "https://files.pythonhosted.org/packages/f5/e2/a510aa736755bffa9d2f75029c229111a1d02f8ecd5de03078f4c18d91a3/cryptography-46.0.3-cp314-cp314t-macosx_10_9_universal2.whl", hash = "sha256:00a5e7e87938e5ff9ff5447ab086a5706a957137e6e433841e9d24f38a065217", size = 7158012, upload-time = "2025-10-15T23:17:19.982Z" },
    { url = "https://files.pythonhosted.org/packages/73/dc/9aa866fbdbb95b02e7f9d086f1fccfeebf8953509b87e3f28fff927ff8a0/cryptography-46.0.3-cp314-cp314t-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:c8daeb2d2174beb4575b77482320303f3d39b8e81153da4f0fb08eb5fe86a6c5", size = 4288728, upload-time = "2025-10-15T23:17:21.527Z" },
    { url = "https://files.pythonhosted.org/packages/c5/fd/bc1daf8230eaa075184cbbf5f8cd00ba9db4fd32d63fb83da4671b72ed8a/cryptography-46.0.3-cp314-cp314t-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:39b6755623145ad5eff1dab323f4eae2a32a77a7abef2c5089a04a3d04366715", size = 4435078, upload-time = "2025-10-15T23:17:23.042Z" },
    { url = "https://files.pythonhosted.org/packages/82/98/d3bd5407ce4c60017f8ff9e63ffee4200ab3e23fe05b765cab805a7db008/cryptography-46.0.3-cp314-cp314t-manylinux_2_28_aarch64.whl", hash = "sha256:db391fa7c66df6762ee3f00c95a89e6d428f4d60e7abc8328f4fe155b5ac6e54", size = 4293460, upload-time = "2025-10-15T23:17:24.885Z" },
    { url = "https://files.pythonhosted.org/packages/26/e9/e23e7900983c2b8af7a08098db406cf989d7f09caea7897e347598d4cd5b/cryptography-46.0.3-cp314-cp314t-manylinux_2_28_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:78a97cf6a8839a48c49271cdcbd5cf37ca2c1d6b7fdd86cc864f302b5e9bf459", size = 3995237, upload-time = "2025-10-15T23:17:26.449Z" },
    { url = "https://files.pythonhosted.org/packages/91/15/af68c509d4a138cfe299d0d7ddb14afba15233223ebd933b4bbdbc7155d3/cryptography-46.0.3-cp314-cp314t-manylinux_2_28_ppc64le.whl", hash = "sha256:dfb781ff7eaa91a6f7fd41776ec37c5853c795d3b358d4896fdbb5df168af422", size = 4967344, upload-time = "2025-10-15T23:17:28.06Z" },
    { url = "https://files.pythonhosted.org/packages/ca/e3/8643d077c53868b681af077edf6b3cb58288b5423610f21c62aadcbe99f4/cryptography-46.0.3-cp314-cp314t-manylinux_2_28_x86_64.whl", hash = "sha256:6f61efb26e76c45c4a227835ddeae96d83624fb0d29eb5df5b96e14ed1a0afb7", size = 4466564, upload-time = "2025-10-15T23:17:29.665Z" },
    { url = "https://files.pythonhosted.org/packages/0e/43/c1e8726fa59c236ff477ff2b5dc071e54b21e5a1e51aa2cee1676f1c986f/cryptography-46.0.3-cp314-cp314t-manylinux_2_34_aarch64.whl", hash = "sha256:23b1a8f26e43f47ceb6d6a43115f33a5a37d57df4ea0ca295b780ae8546e8044", size = 4292415, upload-time = "2025-10-15T23:17:31.686Z" },
    { url = "https://files.pythonhosted.org/packages/42/f9/2f8fefdb1aee8a8e3256a0568cffc4e6d517b256a2fe97a029b3f1b9fe7e/cryptography-46.0.3-cp314-cp314t-manylinux_2_34_ppc64le.whl", hash = "sha256:b419ae593c86b87014b9be7396b385491ad7f320bde96826d0dd174459e54665", size = 4931457, upload-time = "2025-10-15T23:17:33.478Z" },
    { url = "https://files.pythonhosted.org/packages/79/30/9b54127a9a778ccd6d27c3da7563e9f2d341826075ceab89ae3b41bf5be2/cryptography-46.0.3-cp314-cp314t-manylinux_2_34_x86_64.whl", hash = "sha256:50fc3343ac490c6b08c0cf0d704e881d0d660be923fd3076db3e932007e726e3", size = 4466074, upload-time = "2025-10-15T23:17:35.158Z" },
    { url = "https://files.pythonhosted.org/packages/ac/68/b4f4a10928e26c941b1b6a179143af9f4d27d88fe84a6a3c53592d2e76bf/cryptography-46.0.3-cp314-cp314t-musllinux_1_2_aarch64.whl", hash = "sha256:22d7e97932f511d6b0b04f2bfd818d73dcd5928db509460aaf48384778eb6d20", size = 4420569, upload-time = "2025-10-15T23:17:37.188Z" },
    { url = "https://files.pythonhosted.org/packages/a3/49/3746dab4c0d1979888f125226357d3262a6dd40e114ac29e3d2abdf1ec55/cryptography-46.0.3-cp314-cp314t-musllinux_1_2_x86_64.whl", hash = "sha256:d55f3dffadd674514ad19451161118fd010988540cee43d8bc20675e775925de", size = 4681941, upload-time = "2025-10-15T23:17:39.236Z" },
    { url = "https://files.pythonhosted.org/packages/fd/30/27654c1dbaf7e4a3531fa1fc77986d04aefa4d6d78259a62c9dc13d7ad36/cryptography-46.0.3-cp314-cp314t-win32.whl", hash = "sha256:8a6e050cb6164d3f830453754094c086ff2d0b2f3a897a1d9820f6139a1f0914", size = 3022339, upload-time = "2025-10-15T23:17:40.888Z" },
    { url = "https://files.pythonhosted.org/packages/f6/30/640f34ccd4d2a1bc88367b54b926b781b5a018d65f404d409aba76a84b1c/cryptography-46.0.3-cp314-cp314t-win_amd64.whl", hash = "sha256:760f83faa07f8b64e9c33fc963d790a2edb24efb479e3520c14a45741cd9b2db", size = 3494315, upload-time = "2025-10-15T23:17:42.769Z" },
    { url = "https://files.pythonhosted.org/packages/ba/8b/88cc7e3bd0a8e7b861f26981f7b820e1f46aa9d26cc482d0feba0ecb4919/cryptography-46.0.3-cp314-cp314t-win_arm64.whl", hash = "sha256:516ea134e703e9fe26bcd1277a4b59ad30586ea90c365a87781d7887a646fe21", size = 2919331, upload-time = "2025-10-15T23:17:44.468Z" },
    { url = "https://files.pythonhosted.org/packages/fd/23/45fe7f376a7df8daf6da3556603b36f53475a99ce4faacb6ba2cf3d82021/cryptography-46.0.3-cp38-abi3-macosx_10_9_universal2.whl", hash = "sha256:cb3d760a6117f621261d662bccc8ef5bc32ca673e037c83fbe565324f5c46936", size = 7218248, upload-time = "2025-10-15T23:17:46.294Z" },
    { url = "https://files.pythonhosted.org/packages/27/32/b68d27471372737054cbd34c84981f9edbc24fe67ca225d389799614e27f/cryptography-46.0.3-cp38-abi3-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:4b7387121ac7d15e550f5cb4a43aef2559ed759c35df7336c402bb8275ac9683", size = 4294089, upload-time = "2025-10-15T23:17:48.269Z" },
    { url = "https://files.pythonhosted.org/packages/26/42/fa8389d4478368743e24e61eea78846a0006caffaf72ea24a15159215a14/cryptography-46.0.3-cp38-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:15ab9b093e8f09daab0f2159bb7e47532596075139dd74365da52ecc9cb46c5d", size = 4440029, upload-time = "2025-10-15T23:17:49.837Z" },
    { url = "https://files.pythonhosted.org/packages/5f/eb/f483db0ec5ac040824f269e93dd2bd8a21ecd1027e77ad7bdf6914f2fd80/cryptography-46.0.3-cp38-abi3-manylinux_2_28_aarch64.whl", hash = "sha256:46acf53b40ea38f9c6c229599a4a13f0d46a6c3fa9ef19fc1a124d62e338dfa0", size = 4297222, upload-time = "2025-10-15T23:17:51.357Z" },
    { url = "https://files.pythonhosted.org/packages/fd/cf/da9502c4e1912cb1da3807ea3618a6829bee8207456fbbeebc361ec38ba3/cryptography-46.0.3-cp38-abi3-manylinux_2_28_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:10ca84c4668d066a9878890047f03546f3ae0a6b8b39b697457b7757aaf18dbc", size = 4012280, upload-time = "2025-10-15T23:17:52.964Z" },
    { url = "https://files.pythonhosted.org/packages/6b/8f/9adb86b93330e0df8b3dcf03eae67c33ba89958fc2e03862ef1ac2b42465/cryptography-46.0.3-cp38-abi3-manylinux_2_28_ppc64le.whl", hash = "sha256:36e627112085bb3b81b19fed209c05ce2a52ee8b15d161b7c643a7d5a88491f3", size = 4978958, upload-time = "2025-10-15T23:17:54.965Z" },
    { url = "https://files.pythonhosted.org/packages/d1/a0/5fa77988289c34bdb9f913f5606ecc9ada1adb5ae870bd0d1054a7021cc4/cryptography-46.0.3-cp38-abi3-manylinux_2_28_x86_64.whl", hash = "sha256:1000713389b75c449a6e979ffc7dcc8ac90b437048766cef052d4d30b8220971", size = 4473714, upload-time = "2025-10-15T23:17:56.754Z" },
    { url = "https://files.pythonhosted.org/packages/14/e5/fc82d72a58d41c393697aa18c9abe5ae1214ff6f2a5c18ac470f92777895/cryptography-46.0.3-cp38-abi3-manylinux_2_34_aarch64.whl", hash = "sha256:b02cf04496f6576afffef5ddd04a0cb7d49cf6be16a9059d793a30b035f6b6ac", size = 4296970, upload-time = "2025-10-15T23:17:58.588Z" },
    { url = "https://files.pythonhosted.org/packages/78/06/5663ed35438d0b09056973994f1aec467492b33bd31da36e468b01ec1097/cryptography-46.0.3-cp38-abi3-manylinux_2_34_ppc64le.whl", hash = "sha256:71e842ec9bc7abf543b47cf86b9a743baa95f4677d22baa4c7d5c69e49e9bc04", size = 4940236, upload-time = "2025-10-15T23:18:00.897Z" },
    { url = "https://files.pythonhosted.org/packages/fc/59/873633f3f2dcd8a053b8dd1d38f783043b5fce589c0f6988bf55ef57e43e/cryptography-46.0.3-cp38-abi3-manylinux_2_34_x86_64.whl", hash = "sha256:402b58fc32614f00980b66d6e56a5b4118e6cb362ae8f3fda141ba4689bd4506", size = 4472642, upload-time = "2025-10-15T23:18:02.749Z" },
    { url = "https://files.pythonhosted.org/packages/3d/39/8e71f3930e40f6877737d6f69248cf74d4e34b886a3967d32f919cc50d3b/cryptography-46.0.3-cp38-abi3-musllinux_1_2_aarch64.whl", hash = "sha256:ef639cb3372f69ec44915fafcd6698b6cc78fbe0c2ea41be867f6ed612811963", size = 4423126, upload-time = "2025-10-15T23:18:04.85Z" },
    { url = "https://files.pythonhosted.org/packages/cd/c7/f65027c2810e14c3e7268353b1681932b87e5a48e65505d8cc17c99e36ae/cryptography-46.0.3-cp38-abi3-musllinux_1_2_x86_64.whl", hash = "sha256:3b51b8ca4f1c6453d8829e1eb7299499ca7f313900dd4d89a24b8b87c0a780d4", size = 4686573, upload-time = "2025-10-15T23:18:06.908Z" },
    { url = "https://files.pythonhosted.org/packages/0a/6e/1c8331ddf91ca4730ab3086a0f1be19c65510a33b5a441cb334e7a2d2560/cryptography-46.0.3-cp38-abi3-win32.whl", hash = "sha256:6276eb85ef938dc035d59b87c8a7dc559a232f954962520137529d77b18ff1df", size = 3036695, upload-time = "2025-10-15T23:18:08.672Z" },
    { url = "https://files.pythonhosted.org/packages/90/45/b0d691df20633eff80955a0fc7695ff9051ffce8b69741444bd9ed7bd0db/cryptography-46.0.3-cp38-abi3-win_amd64.whl", hash = "sha256:416260257577718c05135c55958b674000baef9a1c7d9e8f306ec60d71db850f", size = 3501720, upload-time = "2025-10-15T23:18:10.632Z" },
    { url = "https://files.pythonhosted.org/packages/e8/cb/2da4cc83f5edb9c3257d09e1e7ab7b23f049c7962cae8d842bbef0a9cec9/cryptography-46.0.3-cp38-abi3-win_arm64.whl", hash = "sha256:d89c3468de4cdc4f08a57e214384d0471911a3830fcdaf7a8cc587e42a866372", size = 2918740, upload-time = "2025-10-15T23:18:12.277Z" },
]

[[package]]
name = "defusedxml"
version = "0.7.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/0f/d5/c66da9b79e5bdb124974bfe172b4daf3c984ebd9c2a06e2b8a4dc7331c72/defusedxml-0.7.1.tar.gz", hash = "sha256:1bb3032db185915b62d7c6209c5a8792be6a32ab2fedacc84e01b52c51aa3e69", size = 75520, upload-time = "2021-03-08T10:59:26.269Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/07/6c/aa3f2f849e01cb6a001cd8554a88d4c77c5c1a31c95bdf1cf9301e6d9ef4/defusedxml-0.7.1-py2.py3-none-any.whl", hash = "sha256:a352e7e428770286cc899e2542b6cdaedb2b4953ff269a210103ec58f6198a61", size = 25604, upload-time = "2021-03-08T10:59:24.45Z" },
]

[[package]]
name = "distro"
version = "1.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fc/f8/98eea607f65de6527f8a2e8885fc8015d3e6f5775df186e443e0964a11c3/distro-1.9.0.tar.gz", hash = "sha256:2fa77c6fd8940f116ee1d6b94a2f90b13b5ea8d019b98bc8bafdcabcdd9bdbed", size = 60722, upload-time = "2023-12-24T09:54:32.31Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl", hash = "sha256:7bffd925d65168f85027d8da9af6bddab658135b840670a223589bc0c8ef02b2", size = 20277, upload-time = "2023-12-24T09:54:30.421Z" },
]

[[package]]
name = "h11"
version = "0.16.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/01/ee/02a2c011bdab74c6fb3c75474d40b3052059d95df7e73351460c8588d963/h11-0.16.0.tar.gz", hash = "sha256:4e35b956cf45792e4caa5885e69fba00bdbc6ffafbfa020300e549b208ee5ff1", size = 101250, upload-time = "2025-04-24T03:35:25.427Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl", hash = "sha256:63cf8bbe7522de3bf65932fda1d9c2772064ffb3dae62d55932da54b31cb6c86", size = 37515, upload-time = "2025-04-24T03:35:24.344Z" },
]

[[package]]
name = "httpcore"
version = "1.0.9"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/06/94/82699a10bca87a5556c9c59b5963f2d039dbd239f25bc2a63907a05a14cb/httpcore-1.0.9.tar.gz", hash = "sha256:6e34463af53fd2ab5d807f399a9b45ea31c3dfa2276f15a2c3f00afff6e176e8", size = 85484, upload-time = "2025-04-24T22:06:22.219Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl", hash = "sha256:2d400746a40668fc9dec9810239072b40b4484b640a8c38fd654a024c7a1bf55", size = 78784, upload-time = "2025-04-24T22:06:20.566Z" },
]

[[package]]
name = "httpx"
version = "0.28.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "certifi" },
    { name = "httpcore" },
    { name = "idna" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b1/df/48c586a5fe32a0f01324ee087459e112ebb7224f646c0b5023f5e79e9956/httpx-0.28.1.tar.gz", hash = "sha256:75e98c5f16b0f35b567856f597f06ff2270a374470a5c2392242528e3e3e42fc", size = 141406, upload-time = "2024-12-06T15:37:23.222Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl", hash = "sha256:d909fcccc110f8c7faf814ca82a9a4d816bc5a6dbfea25d6591d6985b8ba59ad", size = 73517, upload-time = "2024-12-06T15:37:21.509Z" },
]

[[package]]
name = "idna"
version = "3.11"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/6f/6d/0703ccc57f3a7233505399edb88de3cbd678da106337b9fcde432b65ed60/idna-3.11.tar.gz", hash = "sha256:795dafcc9c04ed0c1fb032c2aa73654d8e8c5023a7df64a53f39190ada629902", size = 194582, upload-time = "2025-10-12T14:55:20.501Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0e/61/66938bbb5fc52dbdf84594873d5b51fb1f7c7794e9c0f5bd885f30bc507b/idna-3.11-py3-none-any.whl", hash = "sha256:771a87f49d9defaf64091e6e6fe9c18d4833f140bd19464795bc32d966ca37ea", size = 71008, upload-time = "2025-10-12T14:55:18.883Z" },
]

[[package]]
name = "jiter"
version = "0.12.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/45/9d/e0660989c1370e25848bb4c52d061c71837239738ad937e83edca174c273/jiter-0.12.0.tar.gz", hash = "sha256:64dfcd7d5c168b38d3f9f8bba7fc639edb3418abcc74f22fdbe6b8938293f30b", size = 168294, upload-time = "2025-11-09T20:49:23.302Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/92/c9/5b9f7b4983f1b542c64e84165075335e8a236fa9e2ea03a0c79780062be8/jiter-0.12.0-cp312-cp312-macosx_10_12_x86_64.whl", hash = "sha256:305e061fa82f4680607a775b2e8e0bcb071cd2205ac38e6ef48c8dd5ebe1cf37", size = 314449, upload-time = "2025-11-09T20:47:22.999Z" },
    { url = "https://files.pythonhosted.org/packages/98/6e/e8efa0e78de00db0aee82c0cf9e8b3f2027efd7f8a71f859d8f4be8e98ef/jiter-0.12.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:5c1860627048e302a528333c9307c818c547f214d8659b0705d2195e1a94b274", size = 319855, upload-time = "2025-11-09T20:47:24.779Z" },
    { url = "https://files.pythonhosted.org/packages/20/26/894cd88e60b5d58af53bec5c6759d1292bd0b37a8b5f60f07abf7a63ae5f/jiter-0.12.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:df37577a4f8408f7e0ec3205d2a8f87672af8f17008358063a4d6425b6081ce3", size = 350171, upload-time = "2025-11-09T20:47:26.469Z" },
    { url = "https://files.pythonhosted.org/packages/f5/27/a7b818b9979ac31b3763d25f3653ec3a954044d5e9f5d87f2f247d679fd1/jiter-0.12.0-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:75fdd787356c1c13a4f40b43c2156276ef7a71eb487d98472476476d803fb2cf", size = 365590, upload-time = "2025-11-09T20:47:27.918Z" },
    { url = "https://files.pythonhosted.org/packages/ba/7e/e46195801a97673a83746170b17984aa8ac4a455746354516d02ca5541b4/jiter-0.12.0-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:1eb5db8d9c65b112aacf14fcd0faae9913d07a8afea5ed06ccdd12b724e966a1", size = 479462, upload-time = "2025-11-09T20:47:29.654Z" },
    { url = "https://files.pythonhosted.org/packages/ca/75/f833bfb009ab4bd11b1c9406d333e3b4357709ed0570bb48c7c06d78c7dd/jiter-0.12.0-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:73c568cc27c473f82480abc15d1301adf333a7ea4f2e813d6a2c7d8b6ba8d0df", size = 378983, upload-time = "2025-11-09T20:47:31.026Z" },
    { url = "https://files.pythonhosted.org/packages/71/b3/7a69d77943cc837d30165643db753471aff5df39692d598da880a6e51c24/jiter-0.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4321e8a3d868919bcb1abb1db550d41f2b5b326f72df29e53b2df8b006eb9403", size = 361328, upload-time = "2025-11-09T20:47:33.286Z" },
    { url = "https://files.pythonhosted.org/packages/b0/ac/a78f90caf48d65ba70d8c6efc6f23150bc39dc3389d65bbec2a95c7bc628/jiter-0.12.0-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:0a51bad79f8cc9cac2b4b705039f814049142e0050f30d91695a2d9a6611f126", size = 386740, upload-time = "2025-11-09T20:47:34.703Z" },
    { url = "https://files.pythonhosted.org/packages/39/b6/5d31c2cc8e1b6a6bcf3c5721e4ca0a3633d1ab4754b09bc7084f6c4f5327/jiter-0.12.0-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:2a67b678f6a5f1dd6c36d642d7db83e456bc8b104788262aaefc11a22339f5a9", size = 520875, upload-time = "2025-11-09T20:47:36.058Z" },
    { url = "https://files.pythonhosted.org/packages/30/b5/4df540fae4e9f68c54b8dab004bd8c943a752f0b00efd6e7d64aa3850339/jiter-0.12.0-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:efe1a211fe1fd14762adea941e3cfd6c611a136e28da6c39272dbb7a1bbe6a86", size = 511457, upload-time = "2025-11-09T20:47:37.932Z" },
    { url = "https://files.pythonhosted.org/packages/07/65/86b74010e450a1a77b2c1aabb91d4a91dd3cd5afce99f34d75fd1ac64b19/jiter-0.12.0-cp312-cp312-win32.whl", hash = "sha256:d779d97c834b4278276ec703dc3fc1735fca50af63eb7262f05bdb4e62203d44", size = 204546, upload-time = "2025-11-09T20:47:40.47Z" },
    { url = "https://files.pythonhosted.org/packages/1c/c7/6659f537f9562d963488e3e55573498a442503ced01f7e169e96a6110383/jiter-0.12.0-cp312-cp312-win_amd64.whl", hash = "sha256:e8269062060212b373316fe69236096aaf4c49022d267c6736eebd66bbbc60bb", size = 205196, upload-time = "2025-11-09T20:47:41.794Z" },
    { url = "https://files.pythonhosted.org/packages/21/f4/935304f5169edadfec7f9c01eacbce4c90bb9a82035ac1de1f3bd2d40be6/jiter-0.12.0-cp312-cp312-win_arm64.whl", hash = "sha256:06cb970936c65de926d648af0ed3d21857f026b1cf5525cb2947aa5e01e05789", size = 186100, upload-time = "2025-11-09T20:47:43.007Z" },
    { url = "https://files.pythonhosted.org/packages/3d/a6/97209693b177716e22576ee1161674d1d58029eb178e01866a0422b69224/jiter-0.12.0-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:6cc49d5130a14b732e0612bc76ae8db3b49898732223ef8b7599aa8d9810683e", size = 313658, upload-time = "2025-11-09T20:47:44.424Z" },
    { url = "https://files.pythonhosted.org/packages/06/4d/125c5c1537c7d8ee73ad3d530a442d6c619714b95027143f1b61c0b4dfe0/jiter-0.12.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:37f27a32ce36364d2fa4f7fdc507279db604d27d239ea2e044c8f148410defe1", size = 318605, upload-time = "2025-11-09T20:47:45.973Z" },
    { url = "https://files.pythonhosted.org/packages/99/bf/a840b89847885064c41a5f52de6e312e91fa84a520848ee56c97e4fa0205/jiter-0.12.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:bbc0944aa3d4b4773e348cda635252824a78f4ba44328e042ef1ff3f6080d1cf", size = 349803, upload-time = "2025-11-09T20:47:47.535Z" },
    { url = "https://files.pythonhosted.org/packages/8a/88/e63441c28e0db50e305ae23e19c1d8fae012d78ed55365da392c1f34b09c/jiter-0.12.0-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:da25c62d4ee1ffbacb97fac6dfe4dcd6759ebdc9015991e92a6eae5816287f44", size = 365120, upload-time = "2025-11-09T20:47:49.284Z" },
    { url = "https://files.pythonhosted.org/packages/0a/7c/49b02714af4343970eb8aca63396bc1c82fa01197dbb1e9b0d274b550d4e/jiter-0.12.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:048485c654b838140b007390b8182ba9774621103bd4d77c9c3f6f117474ba45", size = 479918, upload-time = "2025-11-09T20:47:50.807Z" },
    { url = "https://files.pythonhosted.org/packages/69/ba/0a809817fdd5a1db80490b9150645f3aae16afad166960bcd562be194f3b/jiter-0.12.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:635e737fbb7315bef0037c19b88b799143d2d7d3507e61a76751025226b3ac87", size = 379008, upload-time = "2025-11-09T20:47:52.211Z" },
    { url = "https://files.pythonhosted.org/packages/5f/c3/c9fc0232e736c8877d9e6d83d6eeb0ba4e90c6c073835cc2e8f73fdeef51/jiter-0.12.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4e017c417b1ebda911bd13b1e40612704b1f5420e30695112efdbed8a4b389ed", size = 361785, upload-time = "2025-11-09T20:47:53.512Z" },
    { url = "https://files.pythonhosted.org/packages/96/61/61f69b7e442e97ca6cd53086ddc1cf59fb830549bc72c0a293713a60c525/jiter-0.12.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:89b0bfb8b2bf2351fba36bb211ef8bfceba73ef58e7f0c68fb67b5a2795ca2f9", size = 386108, upload-time = "2025-11-09T20:47:54.893Z" },
    { url = "https://files.pythonhosted.org/packages/e9/2e/76bb3332f28550c8f1eba3bf6e5efe211efda0ddbbaf24976bc7078d42a5/jiter-0.12.0-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:f5aa5427a629a824a543672778c9ce0c5e556550d1569bb6ea28a85015287626", size = 519937, upload-time = "2025-11-09T20:47:56.253Z" },
    { url = "https://files.pythonhosted.org/packages/84/d6/fa96efa87dc8bff2094fb947f51f66368fa56d8d4fc9e77b25d7fbb23375/jiter-0.12.0-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:ed53b3d6acbcb0fd0b90f20c7cb3b24c357fe82a3518934d4edfa8c6898e498c", size = 510853, upload-time = "2025-11-09T20:47:58.32Z" },
    { url = "https://files.pythonhosted.org/packages/8a/28/93f67fdb4d5904a708119a6ab58a8f1ec226ff10a94a282e0215402a8462/jiter-0.12.0-cp313-cp313-win32.whl", hash = "sha256:4747de73d6b8c78f2e253a2787930f4fffc68da7fa319739f57437f95963c4de", size = 204699, upload-time = "2025-11-09T20:47:59.686Z" },
    { url = "https://files.pythonhosted.org/packages/c4/1f/30b0eb087045a0abe2a5c9c0c0c8da110875a1d3be83afd4a9a4e548be3c/jiter-0.12.0-cp313-cp313-win_amd64.whl", hash = "sha256:e25012eb0c456fcc13354255d0338cd5397cce26c77b2832b3c4e2e255ea5d9a", size = 204258, upload-time = "2025-11-09T20:48:01.01Z" },
    { url = "https://files.pythonhosted.org/packages/2c/f4/2b4daf99b96bce6fc47971890b14b2a36aef88d7beb9f057fafa032c6141/jiter-0.12.0-cp313-cp313-win_arm64.whl", hash = "sha256:c97b92c54fe6110138c872add030a1f99aea2401ddcdaa21edf74705a646dd60", size = 185503, upload-time = "2025-11-09T20:48:02.35Z" },
    { url = "https://files.pythonhosted.org/packages/39/ca/67bb15a7061d6fe20b9b2a2fd783e296a1e0f93468252c093481a2f00efa/jiter-0.12.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:53839b35a38f56b8be26a7851a48b89bc47e5d88e900929df10ed93b95fea3d6", size = 317965, upload-time = "2025-11-09T20:48:03.783Z" },
    { url = "https://files.pythonhosted.org/packages/18/af/1788031cd22e29c3b14bc6ca80b16a39a0b10e611367ffd480c06a259831/jiter-0.12.0-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:94f669548e55c91ab47fef8bddd9c954dab1938644e715ea49d7e117015110a4", size = 345831, upload-time = "2025-11-09T20:48:05.55Z" },
    { url = "https://files.pythonhosted.org/packages/05/17/710bf8472d1dff0d3caf4ced6031060091c1320f84ee7d5dcbed1f352417/jiter-0.12.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:351d54f2b09a41600ffea43d081522d792e81dcfb915f6d2d242744c1cc48beb", size = 361272, upload-time = "2025-11-09T20:48:06.951Z" },
    { url = "https://files.pythonhosted.org/packages/fb/f1/1dcc4618b59761fef92d10bcbb0b038b5160be653b003651566a185f1a5c/jiter-0.12.0-cp313-cp313t-win_amd64.whl", hash = "sha256:2a5e90604620f94bf62264e7c2c038704d38217b7465b863896c6d7c902b06c7", size = 204604, upload-time = "2025-11-09T20:48:08.328Z" },
    { url = "https://files.pythonhosted.org/packages/d9/32/63cb1d9f1c5c6632a783c0052cde9ef7ba82688f7065e2f0d5f10a7e3edb/jiter-0.12.0-cp313-cp313t-win_arm64.whl", hash = "sha256:88ef757017e78d2860f96250f9393b7b577b06a956ad102c29c8237554380db3", size = 185628, upload-time = "2025-11-09T20:48:09.572Z" },
    { url = "https://files.pythonhosted.org/packages/a8/99/45c9f0dbe4a1416b2b9a8a6d1236459540f43d7fb8883cff769a8db0612d/jiter-0.12.0-cp314-cp314-macosx_10_12_x86_64.whl", hash = "sha256:c46d927acd09c67a9fb1416df45c5a04c27e83aae969267e98fba35b74e99525", size = 312478, upload-time = "2025-11-09T20:48:10.898Z" },
    { url = "https://files.pythonhosted.org/packages/4c/a7/54ae75613ba9e0f55fcb0bc5d1f807823b5167cc944e9333ff322e9f07dd/jiter-0.12.0-cp314-cp314-macosx_11_0_arm64.whl", hash = "sha256:774ff60b27a84a85b27b88cd5583899c59940bcc126caca97eb2a9df6aa00c49", size = 318706, upload-time = "2025-11-09T20:48:12.266Z" },
    { url = "https://files.pythonhosted.org/packages/59/31/2aa241ad2c10774baf6c37f8b8e1f39c07db358f1329f4eb40eba179c2a2/jiter-0.12.0-cp314-cp314-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c5433fab222fb072237df3f637d01b81f040a07dcac1cb4a5c75c7aa9ed0bef1", size = 351894, upload-time = "2025-11-09T20:48:13.673Z" },
    { url = "https://files.pythonhosted.org/packages/54/4f/0f2759522719133a9042781b18cc94e335b6d290f5e2d3e6899d6af933e3/jiter-0.12.0-cp314-cp314-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:f8c593c6e71c07866ec6bfb790e202a833eeec885022296aff6b9e0b92d6a70e", size = 365714, upload-time = "2025-11-09T20:48:15.083Z" },
    { url = "https://files.pythonhosted.org/packages/dc/6f/806b895f476582c62a2f52c453151edd8a0fde5411b0497baaa41018e878/jiter-0.12.0-cp314-cp314-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:90d32894d4c6877a87ae00c6b915b609406819dce8bc0d4e962e4de2784e567e", size = 478989, upload-time = "2025-11-09T20:48:16.706Z" },
    { url = "https://files.pythonhosted.org/packages/86/6c/012d894dc6e1033acd8db2b8346add33e413ec1c7c002598915278a37f79/jiter-0.12.0-cp314-cp314-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:798e46eed9eb10c3adbbacbd3bdb5ecd4cf7064e453d00dbef08802dae6937ff", size = 378615, upload-time = "2025-11-09T20:48:18.614Z" },
    { url = "https://files.pythonhosted.org/packages/87/30/d718d599f6700163e28e2c71c0bbaf6dace692e7df2592fd793ac9276717/jiter-0.12.0-cp314-cp314-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b3f1368f0a6719ea80013a4eb90ba72e75d7ea67cfc7846db2ca504f3df0169a", size = 364745, upload-time = "2025-11-09T20:48:20.117Z" },
    { url = "https://files.pythonhosted.org/packages/8f/85/315b45ce4b6ddc7d7fceca24068543b02bdc8782942f4ee49d652e2cc89f/jiter-0.12.0-cp314-cp314-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:65f04a9d0b4406f7e51279710b27484af411896246200e461d80d3ba0caa901a", size = 386502, upload-time = "2025-11-09T20:48:21.543Z" },
    { url = "https://files.pythonhosted.org/packages/74/0b/ce0434fb40c5b24b368fe81b17074d2840748b4952256bab451b72290a49/jiter-0.12.0-cp314-cp314-musllinux_1_1_aarch64.whl", hash = "sha256:fd990541982a24281d12b67a335e44f117e4c6cbad3c3b75c7dea68bf4ce3a67", size = 519845, upload-time = "2025-11-09T20:48:22.964Z" },
    { url = "https://files.pythonhosted.org/packages/e8/a3/7a7a4488ba052767846b9c916d208b3ed114e3eb670ee984e4c565b9cf0d/jiter-0.12.0-cp314-cp314-musllinux_1_1_x86_64.whl", hash = "sha256:b111b0e9152fa7df870ecaebb0bd30240d9f7fff1f2003bcb4ed0f519941820b", size = 510701, upload-time = "2025-11-09T20:48:24.483Z" },
    { url = "https://files.pythonhosted.org/packages/c3/16/052ffbf9d0467b70af24e30f91e0579e13ded0c17bb4a8eb2aed3cb60131/jiter-0.12.0-cp314-cp314-win32.whl", hash = "sha256:a78befb9cc0a45b5a5a0d537b06f8544c2ebb60d19d02c41ff15da28a9e22d42", size = 205029, upload-time = "2025-11-09T20:48:25.749Z" },
    { url = "https://files.pythonhosted.org/packages/e4/18/3cf1f3f0ccc789f76b9a754bdb7a6977e5d1d671ee97a9e14f7eb728d80e/jiter-0.12.0-cp314-cp314-win_amd64.whl", hash = "sha256:e1fe01c082f6aafbe5c8faf0ff074f38dfb911d53f07ec333ca03f8f6226debf", size = 204960, upload-time = "2025-11-09T20:48:27.415Z" },
    { url = "https://files.pythonhosted.org/packages/02/68/736821e52ecfdeeb0f024b8ab01b5a229f6b9293bbdb444c27efade50b0f/jiter-0.12.0-cp314-cp314-win_arm64.whl", hash = "sha256:d72f3b5a432a4c546ea4bedc84cce0c3404874f1d1676260b9c7f048a9855451", size = 185529, upload-time = "2025-11-09T20:48:29.125Z" },
    { url = "https://files.pythonhosted.org/packages/30/61/12ed8ee7a643cce29ac97c2281f9ce3956eb76b037e88d290f4ed0d41480/jiter-0.12.0-cp314-cp314t-macosx_11_0_arm64.whl", hash = "sha256:e6ded41aeba3603f9728ed2b6196e4df875348ab97b28fc8afff115ed42ba7a7", size = 318974, upload-time = "2025-11-09T20:48:30.87Z" },
    { url = "https://files.pythonhosted.org/packages/2d/c6/f3041ede6d0ed5e0e79ff0de4c8f14f401bbf196f2ef3971cdbe5fd08d1d/jiter-0.12.0-cp314-cp314t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a947920902420a6ada6ad51892082521978e9dd44a802663b001436e4b771684", size = 345932, upload-time = "2025-11-09T20:48:32.658Z" },
    { url = "https://files.pythonhosted.org/packages/d5/5d/4d94835889edd01ad0e2dbfc05f7bdfaed46292e7b504a6ac7839aa00edb/jiter-0.12.0-cp314-cp314t-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:add5e227e0554d3a52cf390a7635edaffdf4f8fce4fdbcef3cc2055bb396a30c", size = 367243, upload-time = "2025-11-09T20:48:34.093Z" },
    { url = "https://files.pythonhosted.org/packages/fd/76/0051b0ac2816253a99d27baf3dda198663aff882fa6ea7deeb94046da24e/jiter-0.12.0-cp314-cp314t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:3f9b1cda8fcb736250d7e8711d4580ebf004a46771432be0ae4796944b5dfa5d", size = 479315, upload-time = "2025-11-09T20:48:35.507Z" },
    { url = "https://files.pythonhosted.org/packages/70/ae/83f793acd68e5cb24e483f44f482a1a15601848b9b6f199dacb970098f77/jiter-0.12.0-cp314-cp314t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:deeb12a2223fe0135c7ff1356a143d57f95bbf1f4a66584f1fc74df21d86b993", size = 380714, upload-time = "2025-11-09T20:48:40.014Z" },
    { url = "https://files.pythonhosted.org/packages/b1/5e/4808a88338ad2c228b1126b93fcd8ba145e919e886fe910d578230dabe3b/jiter-0.12.0-cp314-cp314t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c596cc0f4cb574877550ce4ecd51f8037469146addd676d7c1a30ebe6391923f", size = 365168, upload-time = "2025-11-09T20:48:41.462Z" },
    { url = "https://files.pythonhosted.org/packages/0c/d4/04619a9e8095b42aef436b5aeb4c0282b4ff1b27d1db1508df9f5dc82750/jiter-0.12.0-cp314-cp314t-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:5ab4c823b216a4aeab3fdbf579c5843165756bd9ad87cc6b1c65919c4715f783", size = 387893, upload-time = "2025-11-09T20:48:42.921Z" },
    { url = "https://files.pythonhosted.org/packages/17/ea/d3c7e62e4546fdc39197fa4a4315a563a89b95b6d54c0d25373842a59cbe/jiter-0.12.0-cp314-cp314t-musllinux_1_1_aarch64.whl", hash = "sha256:e427eee51149edf962203ff8db75a7514ab89be5cb623fb9cea1f20b54f1107b", size = 520828, upload-time = "2025-11-09T20:48:44.278Z" },
    { url = "https://files.pythonhosted.org/packages/cc/0b/c6d3562a03fd767e31cb119d9041ea7958c3c80cb3d753eafb19b3b18349/jiter-0.12.0-cp314-cp314t-musllinux_1_1_x86_64.whl", hash = "sha256:edb868841f84c111255ba5e80339d386d937ec1fdce419518ce1bd9370fac5b6", size = 511009, upload-time = "2025-11-09T20:48:45.726Z" },
    { url = "https://files.pythonhosted.org/packages/aa/51/2cb4468b3448a8385ebcd15059d325c9ce67df4e2758d133ab9442b19834/jiter-0.12.0-cp314-cp314t-win32.whl", hash = "sha256:8bbcfe2791dfdb7c5e48baf646d37a6a3dcb5a97a032017741dea9f817dca183", size = 205110, upload-time = "2025-11-09T20:48:47.033Z" },
    { url = "https://files.pythonhosted.org/packages/b2/c5/ae5ec83dec9c2d1af805fd5fe8f74ebded9c8670c5210ec7820ce0dbeb1e/jiter-0.12.0-cp314-cp314t-win_amd64.whl", hash = "sha256:2fa940963bf02e1d8226027ef461e36af472dea85d36054ff835aeed944dd873", size = 205223, upload-time = "2025-11-09T20:48:49.076Z" },
    { url = "https://files.pythonhosted.org/packages/97/9a/3c5391907277f0e55195550cf3fa8e293ae9ee0c00fb402fec1e38c0c82f/jiter-0.12.0-cp314-cp314t-win_arm64.whl", hash = "sha256:506c9708dd29b27288f9f8f1140c3cb0e3d8ddb045956d7757b1fa0e0f39a473", size = 185564, upload-time = "2025-11-09T20:48:50.376Z" },
    { url = "https://files.pythonhosted.org/packages/cb/f5/12efb8ada5f5c9edc1d4555fe383c1fb2eac05ac5859258a72d61981d999/jiter-0.12.0-graalpy312-graalpy250_312_native-macosx_10_12_x86_64.whl", hash = "sha256:e8547883d7b96ef2e5fe22b88f8a4c8725a56e7f4abafff20fd5272d634c7ecb", size = 309974, upload-time = "2025-11-09T20:49:17.187Z" },
    { url = "https://files.pythonhosted.org/packages/85/15/d6eb3b770f6a0d332675141ab3962fd4a7c270ede3515d9f3583e1d28276/jiter-0.12.0-graalpy312-graalpy250_312_native-macosx_11_0_arm64.whl", hash = "sha256:89163163c0934854a668ed783a2546a0617f71706a2551a4a0666d91ab365d6b", size = 304233, upload-time = "2025-11-09T20:49:18.734Z" },
    { url = "https://files.pythonhosted.org/packages/8c/3e/e7e06743294eea2cf02ced6aa0ff2ad237367394e37a0e2b4a1108c67a36/jiter-0.12.0-graalpy312-graalpy250_312_native-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d96b264ab7d34bbb2312dedc47ce07cd53f06835eacbc16dde3761f47c3a9e7f", size = 338537, upload-time = "2025-11-09T20:49:20.317Z" },
    { url = "https://files.pythonhosted.org/packages/2f/9c/6753e6522b8d0ef07d3a3d239426669e984fb0eba15a315cdbc1253904e4/jiter-0.12.0-graalpy312-graalpy250_312_native-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c24e864cb30ab82311c6425655b0cdab0a98c5d973b065c66a3f020740c2324c", size = 346110, upload-time = "2025-11-09T20:49:21.817Z" },
]

[[package]]
name = "lxml"
version = "6.0.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/aa/88/262177de60548e5a2bfc46ad28232c9e9cbde697bd94132aeb80364675cb/lxml-6.0.2.tar.gz", hash = "sha256:cd79f3367bd74b317dda655dc8fcfa304d9eb6e4fb06b7168c5cf27f96e0cd62", size = 4073426, upload-time = "2025-09-22T04:04:59.287Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f3/c8/8ff2bc6b920c84355146cd1ab7d181bc543b89241cfb1ebee824a7c81457/lxml-6.0.2-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:a59f5448ba2ceccd06995c95ea59a7674a10de0810f2ce90c9006f3cbc044456", size = 8661887, upload-time = "2025-09-22T04:01:17.265Z" },
    { url = "https://files.pythonhosted.org/packages/37/6f/9aae1008083bb501ef63284220ce81638332f9ccbfa53765b2b7502203cf/lxml-6.0.2-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:e8113639f3296706fbac34a30813929e29247718e88173ad849f57ca59754924", size = 4667818, upload-time = "2025-09-22T04:01:19.688Z" },
    { url = "https://files.pythonhosted.org/packages/f1/ca/31fb37f99f37f1536c133476674c10b577e409c0a624384147653e38baf2/lxml-6.0.2-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:a8bef9b9825fa8bc816a6e641bb67219489229ebc648be422af695f6e7a4fa7f", size = 4950807, upload-time = "2025-09-22T04:01:21.487Z" },
    { url = "https://files.pythonhosted.org/packages/da/87/f6cb9442e4bada8aab5ae7e1046264f62fdbeaa6e3f6211b93f4c0dd97f1/lxml-6.0.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:65ea18d710fd14e0186c2f973dc60bb52039a275f82d3c44a0e42b43440ea534", size = 5109179, upload-time = "2025-09-22T04:01:23.32Z" },
    { url = "https://files.pythonhosted.org/packages/c8/20/a7760713e65888db79bbae4f6146a6ae5c04e4a204a3c48896c408cd6ed2/lxml-6.0.2-cp312-cp312-manylinux_2_26_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:c371aa98126a0d4c739ca93ceffa0fd7a5d732e3ac66a46e74339acd4d334564", size = 5023044, upload-time = "2025-09-22T04:01:25.118Z" },
    { url = "https://files.pythonhosted.org/packages/a2/b0/7e64e0460fcb36471899f75831509098f3fd7cd02a3833ac517433cb4f8f/lxml-6.0.2-cp312-cp312-manylinux_2_26_i686.manylinux_2_28_i686.whl", hash = "sha256:700efd30c0fa1a3581d80a748157397559396090a51d306ea59a70020223d16f", size = 5359685, upload-time = "2025-09-22T04:01:27.398Z" },
    { url = "https://files.pythonhosted.org/packages/b9/e1/e5df362e9ca4e2f48ed6411bd4b3a0ae737cc842e96877f5bf9428055ab4/lxml-6.0.2-cp312-cp312-manylinux_2_26_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:c33e66d44fe60e72397b487ee92e01da0d09ba2d66df8eae42d77b6d06e5eba0", size = 5654127, upload-time = "2025-09-22T04:01:29.629Z" },
    { url = "https://files.pythonhosted.org/packages/c6/d1/232b3309a02d60f11e71857778bfcd4acbdb86c07db8260caf7d008b08f8/lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:90a345bbeaf9d0587a3aaffb7006aa39ccb6ff0e96a57286c0cb2fd1520ea192", size = 5253958, upload-time = "2025-09-22T04:01:31.535Z" },
    { url = "https://files.pythonhosted.org/packages/35/35/d955a070994725c4f7d80583a96cab9c107c57a125b20bb5f708fe941011/lxml-6.0.2-cp312-cp312-manylinux_2_31_armv7l.whl", hash = "sha256:064fdadaf7a21af3ed1dcaa106b854077fbeada827c18f72aec9346847cd65d0", size = 4711541, upload-time = "2025-09-22T04:01:33.801Z" },
    { url = "https://files.pythonhosted.org/packages/1e/be/667d17363b38a78c4bd63cfd4b4632029fd68d2c2dc81f25ce9eb5224dd5/lxml-6.0.2-cp312-cp312-manylinux_2_38_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:fbc74f42c3525ac4ffa4b89cbdd00057b6196bcefe8bce794abd42d33a018092", size = 5267426, upload-time = "2025-09-22T04:01:35.639Z" },
    { url = "https://files.pythonhosted.org/packages/ea/47/62c70aa4a1c26569bc958c9ca86af2bb4e1f614e8c04fb2989833874f7ae/lxml-6.0.2-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:6ddff43f702905a4e32bc24f3f2e2edfe0f8fde3277d481bffb709a4cced7a1f", size = 5064917, upload-time = "2025-09-22T04:01:37.448Z" },
    { url = "https://files.pythonhosted.org/packages/bd/55/6ceddaca353ebd0f1908ef712c597f8570cc9c58130dbb89903198e441fd/lxml-6.0.2-cp312-cp312-musllinux_1_2_armv7l.whl", hash = "sha256:6da5185951d72e6f5352166e3da7b0dc27aa70bd1090b0eb3f7f7212b53f1bb8", size = 4788795, upload-time = "2025-09-22T04:01:39.165Z" },
    { url = "https://files.pythonhosted.org/packages/cf/e8/fd63e15da5e3fd4c2146f8bbb3c14e94ab850589beab88e547b2dbce22e1/lxml-6.0.2-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:57a86e1ebb4020a38d295c04fc79603c7899e0df71588043eb218722dabc087f", size = 5676759, upload-time = "2025-09-22T04:01:41.506Z" },
    { url = "https://files.pythonhosted.org/packages/76/47/b3ec58dc5c374697f5ba37412cd2728f427d056315d124dd4b61da381877/lxml-6.0.2-cp312-cp312-musllinux_1_2_riscv64.whl", hash = "sha256:2047d8234fe735ab77802ce5f2297e410ff40f5238aec569ad7c8e163d7b19a6", size = 5255666, upload-time = "2025-09-22T04:01:43.363Z" },
    { url = "https://files.pythonhosted.org/packages/19/93/03ba725df4c3d72afd9596eef4a37a837ce8e4806010569bedfcd2cb68fd/lxml-6.0.2-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:6f91fd2b2ea15a6800c8e24418c0775a1694eefc011392da73bc6cef2623b322", size = 5277989, upload-time = "2025-09-22T04:01:45.215Z" },
    { url = "https://files.pythonhosted.org/packages/c6/80/c06de80bfce881d0ad738576f243911fccf992687ae09fd80b734712b39c/lxml-6.0.2-cp312-cp312-win32.whl", hash = "sha256:3ae2ce7d6fedfb3414a2b6c5e20b249c4c607f72cb8d2bb7cc9c6ec7c6f4e849", size = 3611456, upload-time = "2025-09-22T04:01:48.243Z" },
    { url = "https://files.pythonhosted.org/packages/f7/d7/0cdfb6c3e30893463fb3d1e52bc5f5f99684a03c29a0b6b605cfae879cd5/lxml-6.0.2-cp312-cp312-win_amd64.whl", hash = "sha256:72c87e5ee4e58a8354fb9c7c84cbf95a1c8236c127a5d1b7683f04bed8361e1f", size = 4011793, upload-time = "2025-09-22T04:01:50.042Z" },
    { url = "https://files.pythonhosted.org/packages/ea/7b/93c73c67db235931527301ed3785f849c78991e2e34f3fd9a6663ffda4c5/lxml-6.0.2-cp312-cp312-win_arm64.whl", hash = "sha256:61cb10eeb95570153e0c0e554f58df92ecf5109f75eacad4a95baa709e26c3d6", size = 3672836, upload-time = "2025-09-22T04:01:52.145Z" },
    { url = "https://files.pythonhosted.org/packages/53/fd/4e8f0540608977aea078bf6d79f128e0e2c2bba8af1acf775c30baa70460/lxml-6.0.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:9b33d21594afab46f37ae58dfadd06636f154923c4e8a4d754b0127554eb2e77", size = 8648494, upload-time = "2025-09-22T04:01:54.242Z" },
    { url = "https://files.pythonhosted.org/packages/5d/f4/2a94a3d3dfd6c6b433501b8d470a1960a20ecce93245cf2db1706adf6c19/lxml-6.0.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:6c8963287d7a4c5c9a432ff487c52e9c5618667179c18a204bdedb27310f022f", size = 4661146, upload-time = "2025-09-22T04:01:56.282Z" },
    { url = "https://files.pythonhosted.org/packages/25/2e/4efa677fa6b322013035d38016f6ae859d06cac67437ca7dc708a6af7028/lxml-6.0.2-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:1941354d92699fb5ffe6ed7b32f9649e43c2feb4b97205f75866f7d21aa91452", size = 4946932, upload-time = "2025-09-22T04:01:58.989Z" },
    { url = "https://files.pythonhosted.org/packages/ce/0f/526e78a6d38d109fdbaa5049c62e1d32fdd70c75fb61c4eadf3045d3d124/lxml-6.0.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:bb2f6ca0ae2d983ded09357b84af659c954722bbf04dea98030064996d156048", size = 5100060, upload-time = "2025-09-22T04:02:00.812Z" },
    { url = "https://files.pythonhosted.org/packages/81/76/99de58d81fa702cc0ea7edae4f4640416c2062813a00ff24bd70ac1d9c9b/lxml-6.0.2-cp313-cp313-manylinux_2_26_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:eb2a12d704f180a902d7fa778c6d71f36ceb7b0d317f34cdc76a5d05aa1dd1df", size = 5019000, upload-time = "2025-09-22T04:02:02.671Z" },
    { url = "https://files.pythonhosted.org/packages/b5/35/9e57d25482bc9a9882cb0037fdb9cc18f4b79d85df94fa9d2a89562f1d25/lxml-6.0.2-cp313-cp313-manylinux_2_26_i686.manylinux_2_28_i686.whl", hash = "sha256:6ec0e3f745021bfed19c456647f0298d60a24c9ff86d9d051f52b509663feeb1", size = 5348496, upload-time = "2025-09-22T04:02:04.904Z" },
    { url = "https://files.pythonhosted.org/packages/a6/8e/cb99bd0b83ccc3e8f0f528e9aa1f7a9965dfec08c617070c5db8d63a87ce/lxml-6.0.2-cp313-cp313-manylinux_2_26_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:846ae9a12d54e368933b9759052d6206a9e8b250291109c48e350c1f1f49d916", size = 5643779, upload-time = "2025-09-22T04:02:06.689Z" },
    { url = "https://files.pythonhosted.org/packages/d0/34/9e591954939276bb679b73773836c6684c22e56d05980e31d52a9a8deb18/lxml-6.0.2-cp313-cp313-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:ef9266d2aa545d7374938fb5c484531ef5a2ec7f2d573e62f8ce722c735685fd", size = 5244072, upload-time = "2025-09-22T04:02:08.587Z" },
    { url = "https://files.pythonhosted.org/packages/8d/27/b29ff065f9aaca443ee377aff699714fcbffb371b4fce5ac4ca759e436d5/lxml-6.0.2-cp313-cp313-manylinux_2_31_armv7l.whl", hash = "sha256:4077b7c79f31755df33b795dc12119cb557a0106bfdab0d2c2d97bd3cf3dffa6", size = 4718675, upload-time = "2025-09-22T04:02:10.783Z" },
    { url = "https://files.pythonhosted.org/packages/2b/9f/f756f9c2cd27caa1a6ef8c32ae47aadea697f5c2c6d07b0dae133c244fbe/lxml-6.0.2-cp313-cp313-manylinux_2_38_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:a7c5d5e5f1081955358533be077166ee97ed2571d6a66bdba6ec2f609a715d1a", size = 5255171, upload-time = "2025-09-22T04:02:12.631Z" },
    { url = "https://files.pythonhosted.org/packages/61/46/bb85ea42d2cb1bd8395484fd72f38e3389611aa496ac7772da9205bbda0e/lxml-6.0.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:8f8d0cbd0674ee89863a523e6994ac25fd5be9c8486acfc3e5ccea679bad2679", size = 5057175, upload-time = "2025-09-22T04:02:14.718Z" },
    { url = "https://files.pythonhosted.org/packages/95/0c/443fc476dcc8e41577f0af70458c50fe299a97bb6b7505bb1ae09aa7f9ac/lxml-6.0.2-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:2cbcbf6d6e924c28f04a43f3b6f6e272312a090f269eff68a2982e13e5d57659", size = 4785688, upload-time = "2025-09-22T04:02:16.957Z" },
    { url = "https://files.pythonhosted.org/packages/48/78/6ef0b359d45bb9697bc5a626e1992fa5d27aa3f8004b137b2314793b50a0/lxml-6.0.2-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:dfb874cfa53340009af6bdd7e54ebc0d21012a60a4e65d927c2e477112e63484", size = 5660655, upload-time = "2025-09-22T04:02:18.815Z" },
    { url = "https://files.pythonhosted.org/packages/ff/ea/e1d33808f386bc1339d08c0dcada6e4712d4ed8e93fcad5f057070b7988a/lxml-6.0.2-cp313-cp313-musllinux_1_2_riscv64.whl", hash = "sha256:fb8dae0b6b8b7f9e96c26fdd8121522ce5de9bb5538010870bd538683d30e9a2", size = 5247695, upload-time = "2025-09-22T04:02:20.593Z" },
    { url = "https://files.pythonhosted.org/packages/4f/47/eba75dfd8183673725255247a603b4ad606f4ae657b60c6c145b381697da/lxml-6.0.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:358d9adae670b63e95bc59747c72f4dc97c9ec58881d4627fe0120da0f90d314", size = 5269841, upload-time = "2025-09-22T04:02:22.489Z" },
    { url = "https://files.pythonhosted.org/packages/76/04/5c5e2b8577bc936e219becb2e98cdb1aca14a4921a12995b9d0c523502ae/lxml-6.0.2-cp313-cp313-win32.whl", hash = "sha256:e8cd2415f372e7e5a789d743d133ae474290a90b9023197fd78f32e2dc6873e2", size = 3610700, upload-time = "2025-09-22T04:02:24.465Z" },
    { url = "https://files.pythonhosted.org/packages/fe/0a/4643ccc6bb8b143e9f9640aa54e38255f9d3b45feb2cbe7ae2ca47e8782e/lxml-6.0.2-cp313-cp313-win_amd64.whl", hash = "sha256:b30d46379644fbfc3ab81f8f82ae4de55179414651f110a1514f0b1f8f6cb2d7", size = 4010347, upload-time = "2025-09-22T04:02:26.286Z" },
    { url = "https://files.pythonhosted.org/packages/31/ef/dcf1d29c3f530577f61e5fe2f1bd72929acf779953668a8a47a479ae6f26/lxml-6.0.2-cp313-cp313-win_arm64.whl", hash = "sha256:13dcecc9946dca97b11b7c40d29fba63b55ab4170d3c0cf8c0c164343b9bfdcf", size = 3671248, upload-time = "2025-09-22T04:02:27.918Z" },
    { url = "https://files.pythonhosted.org/packages/03/15/d4a377b385ab693ce97b472fe0c77c2b16ec79590e688b3ccc71fba19884/lxml-6.0.2-cp314-cp314-macosx_10_13_universal2.whl", hash = "sha256:b0c732aa23de8f8aec23f4b580d1e52905ef468afb4abeafd3fec77042abb6fe", size = 8659801, upload-time = "2025-09-22T04:02:30.113Z" },
    { url = "https://files.pythonhosted.org/packages/c8/e8/c128e37589463668794d503afaeb003987373c5f94d667124ffd8078bbd9/lxml-6.0.2-cp314-cp314-macosx_10_13_x86_64.whl", hash = "sha256:4468e3b83e10e0317a89a33d28f7aeba1caa4d1a6fd457d115dd4ffe90c5931d", size = 4659403, upload-time = "2025-09-22T04:02:32.119Z" },
    { url = "https://files.pythonhosted.org/packages/00/ce/74903904339decdf7da7847bb5741fc98a5451b42fc419a86c0c13d26fe2/lxml-6.0.2-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:abd44571493973bad4598a3be7e1d807ed45aa2adaf7ab92ab7c62609569b17d", size = 4966974, upload-time = "2025-09-22T04:02:34.155Z" },
    { url = "https://files.pythonhosted.org/packages/1f/d3/131dec79ce61c5567fecf82515bd9bc36395df42501b50f7f7f3bd065df0/lxml-6.0.2-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:370cd78d5855cfbffd57c422851f7d3864e6ae72d0da615fca4dad8c45d375a5", size = 5102953, upload-time = "2025-09-22T04:02:36.054Z" },
    { url = "https://files.pythonhosted.org/packages/3a/ea/a43ba9bb750d4ffdd885f2cd333572f5bb900cd2408b67fdda07e85978a0/lxml-6.0.2-cp314-cp314-manylinux_2_26_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:901e3b4219fa04ef766885fb40fa516a71662a4c61b80c94d25336b4934b71c0", size = 5055054, upload-time = "2025-09-22T04:02:38.154Z" },
    { url = "https://files.pythonhosted.org/packages/60/23/6885b451636ae286c34628f70a7ed1fcc759f8d9ad382d132e1c8d3d9bfd/lxml-6.0.2-cp314-cp314-manylinux_2_26_i686.manylinux_2_28_i686.whl", hash = "sha256:a4bf42d2e4cf52c28cc1812d62426b9503cdb0c87a6de81442626aa7d69707ba", size = 5352421, upload-time = "2025-09-22T04:02:40.413Z" },
    { url = "https://files.pythonhosted.org/packages/48/5b/fc2ddfc94ddbe3eebb8e9af6e3fd65e2feba4967f6a4e9683875c394c2d8/lxml-6.0.2-cp314-cp314-manylinux_2_26_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:b2c7fdaa4d7c3d886a42534adec7cfac73860b89b4e5298752f60aa5984641a0", size = 5673684, upload-time = "2025-09-22T04:02:42.288Z" },
    { url = "https://files.pythonhosted.org/packages/29/9c/47293c58cc91769130fbf85531280e8cc7868f7fbb6d92f4670071b9cb3e/lxml-6.0.2-cp314-cp314-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:98a5e1660dc7de2200b00d53fa00bcd3c35a3608c305d45a7bbcaf29fa16e83d", size = 5252463, upload-time = "2025-09-22T04:02:44.165Z" },
    { url = "https://files.pythonhosted.org/packages/9b/da/ba6eceb830c762b48e711ded880d7e3e89fc6c7323e587c36540b6b23c6b/lxml-6.0.2-cp314-cp314-manylinux_2_31_armv7l.whl", hash = "sha256:dc051506c30b609238d79eda75ee9cab3e520570ec8219844a72a46020901e37", size = 4698437, upload-time = "2025-09-22T04:02:46.524Z" },
    { url = "https://files.pythonhosted.org/packages/a5/24/7be3f82cb7990b89118d944b619e53c656c97dc89c28cfb143fdb7cd6f4d/lxml-6.0.2-cp314-cp314-manylinux_2_38_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:8799481bbdd212470d17513a54d568f44416db01250f49449647b5ab5b5dccb9", size = 5269890, upload-time = "2025-09-22T04:02:48.812Z" },
    { url = "https://files.pythonhosted.org/packages/1b/bd/dcfb9ea1e16c665efd7538fc5d5c34071276ce9220e234217682e7d2c4a5/lxml-6.0.2-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:9261bb77c2dab42f3ecd9103951aeca2c40277701eb7e912c545c1b16e0e4917", size = 5097185, upload-time = "2025-09-22T04:02:50.746Z" },
    { url = "https://files.pythonhosted.org/packages/21/04/a60b0ff9314736316f28316b694bccbbabe100f8483ad83852d77fc7468e/lxml-6.0.2-cp314-cp314-musllinux_1_2_armv7l.whl", hash = "sha256:65ac4a01aba353cfa6d5725b95d7aed6356ddc0a3cd734de00124d285b04b64f", size = 4745895, upload-time = "2025-09-22T04:02:52.968Z" },
    { url = "https://files.pythonhosted.org/packages/d6/bd/7d54bd1846e5a310d9c715921c5faa71cf5c0853372adf78aee70c8d7aa2/lxml-6.0.2-cp314-cp314-musllinux_1_2_ppc64le.whl", hash = "sha256:b22a07cbb82fea98f8a2fd814f3d1811ff9ed76d0fc6abc84eb21527596e7cc8", size = 5695246, upload-time = "2025-09-22T04:02:54.798Z" },
    { url = "https://files.pythonhosted.org/packages/fd/32/5643d6ab947bc371da21323acb2a6e603cedbe71cb4c99c8254289ab6f4e/lxml-6.0.2-cp314-cp314-musllinux_1_2_riscv64.whl", hash = "sha256:d759cdd7f3e055d6bc8d9bec3ad905227b2e4c785dc16c372eb5b5e83123f48a", size = 5260797, upload-time = "2025-09-22T04:02:57.058Z" },
    { url = "https://files.pythonhosted.org/packages/33/da/34c1ec4cff1eea7d0b4cd44af8411806ed943141804ac9c5d565302afb78/lxml-6.0.2-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:945da35a48d193d27c188037a05fec5492937f66fb1958c24fc761fb9d40d43c", size = 5277404, upload-time = "2025-09-22T04:02:58.966Z" },
    { url = "https://files.pythonhosted.org/packages/82/57/4eca3e31e54dc89e2c3507e1cd411074a17565fa5ffc437c4ae0a00d439e/lxml-6.0.2-cp314-cp314-win32.whl", hash = "sha256:be3aaa60da67e6153eb15715cc2e19091af5dc75faef8b8a585aea372507384b", size = 3670072, upload-time = "2025-09-22T04:03:38.05Z" },
    { url = "https://files.pythonhosted.org/packages/e3/e0/c96cf13eccd20c9421ba910304dae0f619724dcf1702864fd59dd386404d/lxml-6.0.2-cp314-cp314-win_amd64.whl", hash = "sha256:fa25afbadead523f7001caf0c2382afd272c315a033a7b06336da2637d92d6ed", size = 4080617, upload-time = "2025-09-22T04:03:39.835Z" },
    { url = "https://files.pythonhosted.org/packages/d5/5d/b3f03e22b3d38d6f188ef044900a9b29b2fe0aebb94625ce9fe244011d34/lxml-6.0.2-cp314-cp314-win_arm64.whl", hash = "sha256:063eccf89df5b24e361b123e257e437f9e9878f425ee9aae3144c77faf6da6d8", size = 3754930, upload-time = "2025-09-22T04:03:41.565Z" },
    { url = "https://files.pythonhosted.org/packages/5e/5c/42c2c4c03554580708fc738d13414801f340c04c3eff90d8d2d227145275/lxml-6.0.2-cp314-cp314t-macosx_10_13_universal2.whl", hash = "sha256:6162a86d86893d63084faaf4ff937b3daea233e3682fb4474db07395794fa80d", size = 8910380, upload-time = "2025-09-22T04:03:01.645Z" },
    { url = "https://files.pythonhosted.org/packages/bf/4f/12df843e3e10d18d468a7557058f8d3733e8b6e12401f30b1ef29360740f/lxml-6.0.2-cp314-cp314t-macosx_10_13_x86_64.whl", hash = "sha256:414aaa94e974e23a3e92e7ca5b97d10c0cf37b6481f50911032c69eeb3991bba", size = 4775632, upload-time = "2025-09-22T04:03:03.814Z" },
    { url = "https://files.pythonhosted.org/packages/e4/0c/9dc31e6c2d0d418483cbcb469d1f5a582a1cd00a1f4081953d44051f3c50/lxml-6.0.2-cp314-cp314t-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:48461bd21625458dd01e14e2c38dd0aea69addc3c4f960c30d9f59d7f93be601", size = 4975171, upload-time = "2025-09-22T04:03:05.651Z" },
    { url = "https://files.pythonhosted.org/packages/e7/2b/9b870c6ca24c841bdd887504808f0417aa9d8d564114689266f19ddf29c8/lxml-6.0.2-cp314-cp314t-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:25fcc59afc57d527cfc78a58f40ab4c9b8fd096a9a3f964d2781ffb6eb33f4ed", size = 5110109, upload-time = "2025-09-22T04:03:07.452Z" },
    { url = "https://files.pythonhosted.org/packages/bf/0c/4f5f2a4dd319a178912751564471355d9019e220c20d7db3fb8307ed8582/lxml-6.0.2-cp314-cp314t-manylinux_2_26_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:5179c60288204e6ddde3f774a93350177e08876eaf3ab78aa3a3649d43eb7d37", size = 5041061, upload-time = "2025-09-22T04:03:09.297Z" },
    { url = "https://files.pythonhosted.org/packages/12/64/554eed290365267671fe001a20d72d14f468ae4e6acef1e179b039436967/lxml-6.0.2-cp314-cp314t-manylinux_2_26_i686.manylinux_2_28_i686.whl", hash = "sha256:967aab75434de148ec80597b75062d8123cadf2943fb4281f385141e18b21338", size = 5306233, upload-time = "2025-09-22T04:03:11.651Z" },
    { url = "https://files.pythonhosted.org/packages/7a/31/1d748aa275e71802ad9722df32a7a35034246b42c0ecdd8235412c3396ef/lxml-6.0.2-cp314-cp314t-manylinux_2_26_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:d100fcc8930d697c6561156c6810ab4a508fb264c8b6779e6e61e2ed5e7558f9", size = 5604739, upload-time = "2025-09-22T04:03:13.592Z" },
    { url = "https://files.pythonhosted.org/packages/8f/41/2c11916bcac09ed561adccacceaedd2bf0e0b25b297ea92aab99fd03d0fa/lxml-6.0.2-cp314-cp314t-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:2ca59e7e13e5981175b8b3e4ab84d7da57993eeff53c07764dcebda0d0e64ecd", size = 5225119, upload-time = "2025-09-22T04:03:15.408Z" },
    { url = "https://files.pythonhosted.org/packages/99/05/4e5c2873d8f17aa018e6afde417c80cc5d0c33be4854cce3ef5670c49367/lxml-6.0.2-cp314-cp314t-manylinux_2_31_armv7l.whl", hash = "sha256:957448ac63a42e2e49531b9d6c0fa449a1970dbc32467aaad46f11545be9af1d", size = 4633665, upload-time = "2025-09-22T04:03:17.262Z" },
    { url = "https://files.pythonhosted.org/packages/0f/c9/dcc2da1bebd6275cdc723b515f93edf548b82f36a5458cca3578bc899332/lxml-6.0.2-cp314-cp314t-manylinux_2_38_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:b7fc49c37f1786284b12af63152fe1d0990722497e2d5817acfe7a877522f9a9", size = 5234997, upload-time = "2025-09-22T04:03:19.14Z" },
    { url = "https://files.pythonhosted.org/packages/9c/e2/5172e4e7468afca64a37b81dba152fc5d90e30f9c83c7c3213d6a02a5ce4/lxml-6.0.2-cp314-cp314t-musllinux_1_2_aarch64.whl", hash = "sha256:e19e0643cc936a22e837f79d01a550678da8377d7d801a14487c10c34ee49c7e", size = 5090957, upload-time = "2025-09-22T04:03:21.436Z" },
    { url = "https://files.pythonhosted.org/packages/a5/b3/15461fd3e5cd4ddcb7938b87fc20b14ab113b92312fc97afe65cd7c85de1/lxml-6.0.2-cp314-cp314t-musllinux_1_2_armv7l.whl", hash = "sha256:1db01e5cf14345628e0cbe71067204db658e2fb8e51e7f33631f5f4735fefd8d", size = 4764372, upload-time = "2025-09-22T04:03:23.27Z" },
    { url = "https://files.pythonhosted.org/packages/05/33/f310b987c8bf9e61c4dd8e8035c416bd3230098f5e3cfa69fc4232de7059/lxml-6.0.2-cp314-cp314t-musllinux_1_2_ppc64le.whl", hash = "sha256:875c6b5ab39ad5291588aed6925fac99d0097af0dd62f33c7b43736043d4a2ec", size = 5634653, upload-time = "2025-09-22T04:03:25.767Z" },
    { url = "https://files.pythonhosted.org/packages/70/ff/51c80e75e0bc9382158133bdcf4e339b5886c6ee2418b5199b3f1a61ed6d/lxml-6.0.2-cp314-cp314t-musllinux_1_2_riscv64.whl", hash = "sha256:cdcbed9ad19da81c480dfd6dd161886db6096083c9938ead313d94b30aadf272", size = 5233795, upload-time = "2025-09-22T04:03:27.62Z" },
    { url = "https://files.pythonhosted.org/packages/56/4d/4856e897df0d588789dd844dbed9d91782c4ef0b327f96ce53c807e13128/lxml-6.0.2-cp314-cp314t-musllinux_1_2_x86_64.whl", hash = "sha256:80dadc234ebc532e09be1975ff538d154a7fa61ea5031c03d25178855544728f", size = 5257023, upload-time = "2025-09-22T04:03:30.056Z" },
    { url = "https://files.pythonhosted.org/packages/0f/85/86766dfebfa87bea0ab78e9ff7a4b4b45225df4b4d3b8cc3c03c5cd68464/lxml-6.0.2-cp314-cp314t-win32.whl", hash = "sha256:da08e7bb297b04e893d91087df19638dc7a6bb858a954b0cc2b9f5053c922312", size = 3911420, upload-time = "2025-09-22T04:03:32.198Z" },
    { url = "https://files.pythonhosted.org/packages/fe/1a/b248b355834c8e32614650b8008c69ffeb0ceb149c793961dd8c0b991bb3/lxml-6.0.2-cp314-cp314t-win_amd64.whl", hash = "sha256:252a22982dca42f6155125ac76d3432e548a7625d56f5a273ee78a5057216eca", size = 4406837, upload-time = "2025-09-22T04:03:34.027Z" },
    { url = "https://files.pythonhosted.org/packages/92/aa/df863bcc39c5e0946263454aba394de8a9084dbaff8ad143846b0d844739/lxml-6.0.2-cp314-cp314t-win_arm64.whl", hash = "sha256:bb4c1847b303835d89d785a18801a883436cdfd5dc3d62947f9c49e24f0f5a2c", size = 3822205, upload-time = "2025-09-22T04:03:36.249Z" },
]

[[package]]
name = "odfpy"
version = "1.4.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "defusedxml" },
]
sdist = { url = "https://files.pythonhosted.org/packages/97/73/8ade73f6749177003f7ce3304f524774adda96e6aaab30ea79fd8fda7934/odfpy-1.4.1.tar.gz", hash = "sha256:db766a6e59c5103212f3cc92ec8dd50a0f3a02790233ed0b52148b70d3c438ec", size = 717045, upload-time = "2020-01-18T16:55:48.852Z" }

[[package]]
name = "openai"
version = "2.15.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "distro" },
    { name = "httpx" },
    { name = "jiter" },
    { name = "pydantic" },
    { name = "sniffio" },
    { name = "tqdm" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/94/f4/4690ecb5d70023ce6bfcfeabfe717020f654bde59a775058ec6ac4692463/openai-2.15.0.tar.gz", hash = "sha256:42eb8cbb407d84770633f31bf727d4ffb4138711c670565a41663d9439174fba", size = 627383, upload-time = "2026-01-09T22:10:08.603Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b5/df/c306f7375d42bafb379934c2df4c2fa3964656c8c782bac75ee10c102818/openai-2.15.0-py3-none-any.whl", hash = "sha256:6ae23b932cd7230f7244e52954daa6602716d6b9bf235401a107af731baea6c3", size = 1067879, upload-time = "2026-01-09T22:10:06.446Z" },
]

[[package]]
name = "pdfminer-six"
version = "20251230"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "charset-normalizer" },
    { name = "cryptography" },
]
sdist = { url = "https://files.pythonhosted.org/packages/46/9a/d79d8fa6d47a0338846bb558b39b9963b8eb2dfedec61867c138c1b17eeb/pdfminer_six-20251230.tar.gz", hash = "sha256:e8f68a14c57e00c2d7276d26519ea64be1b48f91db1cdc776faa80528ca06c1e", size = 8511285, upload-time = "2025-12-30T15:49:13.104Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/65/d7/b288ea32deb752a09aab73c75e1e7572ab2a2b56c3124a5d1eb24c62ceb3/pdfminer_six-20251230-py3-none-any.whl", hash = "sha256:9ff2e3466a7dfc6de6fd779478850b6b7c2d9e9405aa2a5869376a822771f485", size = 6591909, upload-time = "2025-12-30T15:49:10.76Z" },
]

[[package]]
name = "pdfplumber"
version = "0.11.9"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pdfminer-six" },
    { name = "pillow" },
    { name = "pypdfium2" },
]
sdist = { url = "https://files.pythonhosted.org/packages/38/37/9ca3519e92a8434eb93be570b131476cc0a4e840bb39c62ddb7813a39d53/pdfplumber-0.11.9.tar.gz", hash = "sha256:481224b678b2bbdbf376e2c39bf914144eef7c3d301b4a28eebf0f7f6109d6dc", size = 102768, upload-time = "2026-01-05T08:10:29.072Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/c8/cdbc975f5b634e249cfa6597e37c50f3078412474f21c015e508bfbfe3c3/pdfplumber-0.11.9-py3-none-any.whl", hash = "sha256:33ec5580959ba524e9100138746e090879504c42955df1b8a997604dd326c443", size = 60045, upload-time = "2026-01-05T08:10:27.512Z" },
]

[[package]]
name = "pillow"
version = "12.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d0/02/d52c733a2452ef1ffcc123b68e6606d07276b0e358db70eabad7e40042b7/pillow-12.1.0.tar.gz", hash = "sha256:5c5ae0a06e9ea030ab786b0251b32c7e4ce10e58d983c0d5c56029455180b5b9", size = 46977283, upload-time = "2026-01-02T09:13:29.892Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/20/31/dc53fe21a2f2996e1b7d92bf671cdb157079385183ef7c1ae08b485db510/pillow-12.1.0-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:a332ac4ccb84b6dde65dbace8431f3af08874bf9770719d32a635c4ef411b18b", size = 5262642, upload-time = "2026-01-02T09:11:10.138Z" },
    { url = "https://files.pythonhosted.org/packages/ab/c1/10e45ac9cc79419cedf5121b42dcca5a50ad2b601fa080f58c22fb27626e/pillow-12.1.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:907bfa8a9cb790748a9aa4513e37c88c59660da3bcfffbd24a7d9e6abf224551", size = 4657464, upload-time = "2026-01-02T09:11:12.319Z" },
    { url = "https://files.pythonhosted.org/packages/ad/26/7b82c0ab7ef40ebede7a97c72d473bda5950f609f8e0c77b04af574a0ddb/pillow-12.1.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:efdc140e7b63b8f739d09a99033aa430accce485ff78e6d311973a67b6bf3208", size = 6234878, upload-time = "2026-01-02T09:11:14.096Z" },
    { url = "https://files.pythonhosted.org/packages/76/25/27abc9792615b5e886ca9411ba6637b675f1b77af3104710ac7353fe5605/pillow-12.1.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:bef9768cab184e7ae6e559c032e95ba8d07b3023c289f79a2bd36e8bf85605a5", size = 8044868, upload-time = "2026-01-02T09:11:15.903Z" },
    { url = "https://files.pythonhosted.org/packages/0a/ea/f200a4c36d836100e7bc738fc48cd963d3ba6372ebc8298a889e0cfc3359/pillow-12.1.0-cp312-cp312-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:742aea052cf5ab5034a53c3846165bc3ce88d7c38e954120db0ab867ca242661", size = 6349468, upload-time = "2026-01-02T09:11:17.631Z" },
    { url = "https://files.pythonhosted.org/packages/11/8f/48d0b77ab2200374c66d344459b8958c86693be99526450e7aee714e03e4/pillow-12.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:a6dfc2af5b082b635af6e08e0d1f9f1c4e04d17d4e2ca0ef96131e85eda6eb17", size = 7041518, upload-time = "2026-01-02T09:11:19.389Z" },
    { url = "https://files.pythonhosted.org/packages/1d/23/c281182eb986b5d31f0a76d2a2c8cd41722d6fb8ed07521e802f9bba52de/pillow-12.1.0-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:609e89d9f90b581c8d16358c9087df76024cf058fa693dd3e1e1620823f39670", size = 6462829, upload-time = "2026-01-02T09:11:21.28Z" },
    { url = "https://files.pythonhosted.org/packages/25/ef/7018273e0faac099d7b00982abdcc39142ae6f3bd9ceb06de09779c4a9d6/pillow-12.1.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:43b4899cfd091a9693a1278c4982f3e50f7fb7cff5153b05174b4afc9593b616", size = 7166756, upload-time = "2026-01-02T09:11:23.559Z" },
    { url = "https://files.pythonhosted.org/packages/8f/c8/993d4b7ab2e341fe02ceef9576afcf5830cdec640be2ac5bee1820d693d4/pillow-12.1.0-cp312-cp312-win32.whl", hash = "sha256:aa0c9cc0b82b14766a99fbe6084409972266e82f459821cd26997a488a7261a7", size = 6328770, upload-time = "2026-01-02T09:11:25.661Z" },
    { url = "https://files.pythonhosted.org/packages/a7/87/90b358775a3f02765d87655237229ba64a997b87efa8ccaca7dd3e36e7a7/pillow-12.1.0-cp312-cp312-win_amd64.whl", hash = "sha256:d70534cea9e7966169ad29a903b99fc507e932069a881d0965a1a84bb57f6c6d", size = 7033406, upload-time = "2026-01-02T09:11:27.474Z" },
    { url = "https://files.pythonhosted.org/packages/5d/cf/881b457eccacac9e5b2ddd97d5071fb6d668307c57cbf4e3b5278e06e536/pillow-12.1.0-cp312-cp312-win_arm64.whl", hash = "sha256:65b80c1ee7e14a87d6a068dd3b0aea268ffcabfe0498d38661b00c5b4b22e74c", size = 2452612, upload-time = "2026-01-02T09:11:29.309Z" },
    { url = "https://files.pythonhosted.org/packages/dd/c7/2530a4aa28248623e9d7f27316b42e27c32ec410f695929696f2e0e4a778/pillow-12.1.0-cp313-cp313-ios_13_0_arm64_iphoneos.whl", hash = "sha256:7b5dd7cbae20285cdb597b10eb5a2c13aa9de6cde9bb64a3c1317427b1db1ae1", size = 4062543, upload-time = "2026-01-02T09:11:31.566Z" },
    { url = "https://files.pythonhosted.org/packages/8f/1f/40b8eae823dc1519b87d53c30ed9ef085506b05281d313031755c1705f73/pillow-12.1.0-cp313-cp313-ios_13_0_arm64_iphonesimulator.whl", hash = "sha256:29a4cef9cb672363926f0470afc516dbf7305a14d8c54f7abbb5c199cd8f8179", size = 4138373, upload-time = "2026-01-02T09:11:33.367Z" },
    { url = "https://files.pythonhosted.org/packages/d4/77/6fa60634cf06e52139fd0e89e5bbf055e8166c691c42fb162818b7fda31d/pillow-12.1.0-cp313-cp313-ios_13_0_x86_64_iphonesimulator.whl", hash = "sha256:681088909d7e8fa9e31b9799aaa59ba5234c58e5e4f1951b4c4d1082a2e980e0", size = 3601241, upload-time = "2026-01-02T09:11:35.011Z" },
    { url = "https://files.pythonhosted.org/packages/4f/bf/28ab865de622e14b747f0cd7877510848252d950e43002e224fb1c9ababf/pillow-12.1.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:983976c2ab753166dc66d36af6e8ec15bb511e4a25856e2227e5f7e00a160587", size = 5262410, upload-time = "2026-01-02T09:11:36.682Z" },
    { url = "https://files.pythonhosted.org/packages/1c/34/583420a1b55e715937a85bd48c5c0991598247a1fd2eb5423188e765ea02/pillow-12.1.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:db44d5c160a90df2d24a24760bbd37607d53da0b34fb546c4c232af7192298ac", size = 4657312, upload-time = "2026-01-02T09:11:38.535Z" },
    { url = "https://files.pythonhosted.org/packages/1d/fd/f5a0896839762885b3376ff04878f86ab2b097c2f9a9cdccf4eda8ba8dc0/pillow-12.1.0-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:6b7a9d1db5dad90e2991645874f708e87d9a3c370c243c2d7684d28f7e133e6b", size = 6232605, upload-time = "2026-01-02T09:11:40.602Z" },
    { url = "https://files.pythonhosted.org/packages/98/aa/938a09d127ac1e70e6ed467bd03834350b33ef646b31edb7452d5de43792/pillow-12.1.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:6258f3260986990ba2fa8a874f8b6e808cf5abb51a94015ca3dc3c68aa4f30ea", size = 8041617, upload-time = "2026-01-02T09:11:42.721Z" },
    { url = "https://files.pythonhosted.org/packages/17/e8/538b24cb426ac0186e03f80f78bc8dc7246c667f58b540bdd57c71c9f79d/pillow-12.1.0-cp313-cp313-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:e115c15e3bc727b1ca3e641a909f77f8ca72a64fff150f666fcc85e57701c26c", size = 6346509, upload-time = "2026-01-02T09:11:44.955Z" },
    { url = "https://files.pythonhosted.org/packages/01/9a/632e58ec89a32738cabfd9ec418f0e9898a2b4719afc581f07c04a05e3c9/pillow-12.1.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:6741e6f3074a35e47c77b23a4e4f2d90db3ed905cb1c5e6e0d49bff2045632bc", size = 7038117, upload-time = "2026-01-02T09:11:46.736Z" },
    { url = "https://files.pythonhosted.org/packages/c7/a2/d40308cf86eada842ca1f3ffa45d0ca0df7e4ab33c83f81e73f5eaed136d/pillow-12.1.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:935b9d1aed48fcfb3f838caac506f38e29621b44ccc4f8a64d575cb1b2a88644", size = 6460151, upload-time = "2026-01-02T09:11:48.625Z" },
    { url = "https://files.pythonhosted.org/packages/f1/88/f5b058ad6453a085c5266660a1417bdad590199da1b32fb4efcff9d33b05/pillow-12.1.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:5fee4c04aad8932da9f8f710af2c1a15a83582cfb884152a9caa79d4efcdbf9c", size = 7164534, upload-time = "2026-01-02T09:11:50.445Z" },
    { url = "https://files.pythonhosted.org/packages/19/ce/c17334caea1db789163b5d855a5735e47995b0b5dc8745e9a3605d5f24c0/pillow-12.1.0-cp313-cp313-win32.whl", hash = "sha256:a786bf667724d84aa29b5db1c61b7bfdde380202aaca12c3461afd6b71743171", size = 6332551, upload-time = "2026-01-02T09:11:52.234Z" },
    { url = "https://files.pythonhosted.org/packages/e5/07/74a9d941fa45c90a0d9465098fe1ec85de3e2afbdc15cc4766622d516056/pillow-12.1.0-cp313-cp313-win_amd64.whl", hash = "sha256:461f9dfdafa394c59cd6d818bdfdbab4028b83b02caadaff0ffd433faf4c9a7a", size = 7040087, upload-time = "2026-01-02T09:11:54.822Z" },
    { url = "https://files.pythonhosted.org/packages/88/09/c99950c075a0e9053d8e880595926302575bc742b1b47fe1bbcc8d388d50/pillow-12.1.0-cp313-cp313-win_arm64.whl", hash = "sha256:9212d6b86917a2300669511ed094a9406888362e085f2431a7da985a6b124f45", size = 2452470, upload-time = "2026-01-02T09:11:56.522Z" },
    { url = "https://files.pythonhosted.org/packages/b5/ba/970b7d85ba01f348dee4d65412476321d40ee04dcb51cd3735b9dc94eb58/pillow-12.1.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:00162e9ca6d22b7c3ee8e61faa3c3253cd19b6a37f126cad04f2f88b306f557d", size = 5264816, upload-time = "2026-01-02T09:11:58.227Z" },
    { url = "https://files.pythonhosted.org/packages/10/60/650f2fb55fdba7a510d836202aa52f0baac633e50ab1cf18415d332188fb/pillow-12.1.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:7d6daa89a00b58c37cb1747ec9fb7ac3bc5ffd5949f5888657dfddde6d1312e0", size = 4660472, upload-time = "2026-01-02T09:12:00.798Z" },
    { url = "https://files.pythonhosted.org/packages/2b/c0/5273a99478956a099d533c4f46cbaa19fd69d606624f4334b85e50987a08/pillow-12.1.0-cp313-cp313t-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:e2479c7f02f9d505682dc47df8c0ea1fc5e264c4d1629a5d63fe3e2334b89554", size = 6268974, upload-time = "2026-01-02T09:12:02.572Z" },
    { url = "https://files.pythonhosted.org/packages/b4/26/0bf714bc2e73d5267887d47931d53c4ceeceea6978148ed2ab2a4e6463c4/pillow-12.1.0-cp313-cp313t-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:f188d580bd870cda1e15183790d1cc2fa78f666e76077d103edf048eed9c356e", size = 8073070, upload-time = "2026-01-02T09:12:04.75Z" },
    { url = "https://files.pythonhosted.org/packages/43/cf/1ea826200de111a9d65724c54f927f3111dc5ae297f294b370a670c17786/pillow-12.1.0-cp313-cp313t-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:0fde7ec5538ab5095cc02df38ee99b0443ff0e1c847a045554cf5f9af1f4aa82", size = 6380176, upload-time = "2026-01-02T09:12:06.626Z" },
    { url = "https://files.pythonhosted.org/packages/03/e0/7938dd2b2013373fd85d96e0f38d62b7a5a262af21ac274250c7ca7847c9/pillow-12.1.0-cp313-cp313t-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:0ed07dca4a8464bada6139ab38f5382f83e5f111698caf3191cb8dbf27d908b4", size = 7067061, upload-time = "2026-01-02T09:12:08.624Z" },
    { url = "https://files.pythonhosted.org/packages/86/ad/a2aa97d37272a929a98437a8c0ac37b3cf012f4f8721e1bd5154699b2518/pillow-12.1.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:f45bd71d1fa5e5749587613037b172e0b3b23159d1c00ef2fc920da6f470e6f0", size = 6491824, upload-time = "2026-01-02T09:12:10.488Z" },
    { url = "https://files.pythonhosted.org/packages/a4/44/80e46611b288d51b115826f136fb3465653c28f491068a72d3da49b54cd4/pillow-12.1.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:277518bf4fe74aa91489e1b20577473b19ee70fb97c374aa50830b279f25841b", size = 7190911, upload-time = "2026-01-02T09:12:12.772Z" },
    { url = "https://files.pythonhosted.org/packages/86/77/eacc62356b4cf81abe99ff9dbc7402750044aed02cfd6a503f7c6fc11f3e/pillow-12.1.0-cp313-cp313t-win32.whl", hash = "sha256:7315f9137087c4e0ee73a761b163fc9aa3b19f5f606a7fc08d83fd3e4379af65", size = 6336445, upload-time = "2026-01-02T09:12:14.775Z" },
    { url = "https://files.pythonhosted.org/packages/e7/3c/57d81d0b74d218706dafccb87a87ea44262c43eef98eb3b164fd000e0491/pillow-12.1.0-cp313-cp313t-win_amd64.whl", hash = "sha256:0ddedfaa8b5f0b4ffbc2fa87b556dc59f6bb4ecb14a53b33f9189713ae8053c0", size = 7045354, upload-time = "2026-01-02T09:12:16.599Z" },
    { url = "https://files.pythonhosted.org/packages/ac/82/8b9b97bba2e3576a340f93b044a3a3a09841170ab4c1eb0d5c93469fd32f/pillow-12.1.0-cp313-cp313t-win_arm64.whl", hash = "sha256:80941e6d573197a0c28f394753de529bb436b1ca990ed6e765cf42426abc39f8", size = 2454547, upload-time = "2026-01-02T09:12:18.704Z" },
    { url = "https://files.pythonhosted.org/packages/8c/87/bdf971d8bbcf80a348cc3bacfcb239f5882100fe80534b0ce67a784181d8/pillow-12.1.0-cp314-cp314-ios_13_0_arm64_iphoneos.whl", hash = "sha256:5cb7bc1966d031aec37ddb9dcf15c2da5b2e9f7cc3ca7c54473a20a927e1eb91", size = 4062533, upload-time = "2026-01-02T09:12:20.791Z" },
    { url = "https://files.pythonhosted.org/packages/ff/4f/5eb37a681c68d605eb7034c004875c81f86ec9ef51f5be4a63eadd58859a/pillow-12.1.0-cp314-cp314-ios_13_0_arm64_iphonesimulator.whl", hash = "sha256:97e9993d5ed946aba26baf9c1e8cf18adbab584b99f452ee72f7ee8acb882796", size = 4138546, upload-time = "2026-01-02T09:12:23.664Z" },
    { url = "https://files.pythonhosted.org/packages/11/6d/19a95acb2edbace40dcd582d077b991646b7083c41b98da4ed7555b59733/pillow-12.1.0-cp314-cp314-ios_13_0_x86_64_iphonesimulator.whl", hash = "sha256:414b9a78e14ffeb98128863314e62c3f24b8a86081066625700b7985b3f529bd", size = 3601163, upload-time = "2026-01-02T09:12:26.338Z" },
    { url = "https://files.pythonhosted.org/packages/fc/36/2b8138e51cb42e4cc39c3297713455548be855a50558c3ac2beebdc251dd/pillow-12.1.0-cp314-cp314-macosx_10_15_x86_64.whl", hash = "sha256:e6bdb408f7c9dd2a5ff2b14a3b0bb6d4deb29fb9961e6eb3ae2031ae9a5cec13", size = 5266086, upload-time = "2026-01-02T09:12:28.782Z" },
    { url = "https://files.pythonhosted.org/packages/53/4b/649056e4d22e1caa90816bf99cef0884aed607ed38075bd75f091a607a38/pillow-12.1.0-cp314-cp314-macosx_11_0_arm64.whl", hash = "sha256:3413c2ae377550f5487991d444428f1a8ae92784aac79caa8b1e3b89b175f77e", size = 4657344, upload-time = "2026-01-02T09:12:31.117Z" },
    { url = "https://files.pythonhosted.org/packages/6c/6b/c5742cea0f1ade0cd61485dc3d81f05261fc2276f537fbdc00802de56779/pillow-12.1.0-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:e5dcbe95016e88437ecf33544ba5db21ef1b8dd6e1b434a2cb2a3d605299e643", size = 6232114, upload-time = "2026-01-02T09:12:32.936Z" },
    { url = "https://files.pythonhosted.org/packages/bf/8f/9f521268ce22d63991601aafd3d48d5ff7280a246a1ef62d626d67b44064/pillow-12.1.0-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:d0a7735df32ccbcc98b98a1ac785cc4b19b580be1bdf0aeb5c03223220ea09d5", size = 8042708, upload-time = "2026-01-02T09:12:34.78Z" },
    { url = "https://files.pythonhosted.org/packages/1a/eb/257f38542893f021502a1bbe0c2e883c90b5cff26cc33b1584a841a06d30/pillow-12.1.0-cp314-cp314-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:0c27407a2d1b96774cbc4a7594129cc027339fd800cd081e44497722ea1179de", size = 6347762, upload-time = "2026-01-02T09:12:36.748Z" },
    { url = "https://files.pythonhosted.org/packages/c4/5a/8ba375025701c09b309e8d5163c5a4ce0102fa86bbf8800eb0d7ac87bc51/pillow-12.1.0-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:15c794d74303828eaa957ff8070846d0efe8c630901a1c753fdc63850e19ecd9", size = 7039265, upload-time = "2026-01-02T09:12:39.082Z" },
    { url = "https://files.pythonhosted.org/packages/cf/dc/cf5e4cdb3db533f539e88a7bbf9f190c64ab8a08a9bc7a4ccf55067872e4/pillow-12.1.0-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:c990547452ee2800d8506c4150280757f88532f3de2a58e3022e9b179107862a", size = 6462341, upload-time = "2026-01-02T09:12:40.946Z" },
    { url = "https://files.pythonhosted.org/packages/d0/47/0291a25ac9550677e22eda48510cfc4fa4b2ef0396448b7fbdc0a6946309/pillow-12.1.0-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:b63e13dd27da389ed9475b3d28510f0f954bca0041e8e551b2a4eb1eab56a39a", size = 7165395, upload-time = "2026-01-02T09:12:42.706Z" },
    { url = "https://files.pythonhosted.org/packages/4f/4c/e005a59393ec4d9416be06e6b45820403bb946a778e39ecec62f5b2b991e/pillow-12.1.0-cp314-cp314-win32.whl", hash = "sha256:1a949604f73eb07a8adab38c4fe50791f9919344398bdc8ac6b307f755fc7030", size = 6431413, upload-time = "2026-01-02T09:12:44.944Z" },
    { url = "https://files.pythonhosted.org/packages/1c/af/f23697f587ac5f9095d67e31b81c95c0249cd461a9798a061ed6709b09b5/pillow-12.1.0-cp314-cp314-win_amd64.whl", hash = "sha256:4f9f6a650743f0ddee5593ac9e954ba1bdbc5e150bc066586d4f26127853ab94", size = 7176779, upload-time = "2026-01-02T09:12:46.727Z" },
    { url = "https://files.pythonhosted.org/packages/b3/36/6a51abf8599232f3e9afbd16d52829376a68909fe14efe29084445db4b73/pillow-12.1.0-cp314-cp314-win_arm64.whl", hash = "sha256:808b99604f7873c800c4840f55ff389936ef1948e4e87645eaf3fccbc8477ac4", size = 2543105, upload-time = "2026-01-02T09:12:49.243Z" },
    { url = "https://files.pythonhosted.org/packages/82/54/2e1dd20c8749ff225080d6ba465a0cab4387f5db0d1c5fb1439e2d99923f/pillow-12.1.0-cp314-cp314t-macosx_10_15_x86_64.whl", hash = "sha256:bc11908616c8a283cf7d664f77411a5ed2a02009b0097ff8abbba5e79128ccf2", size = 5268571, upload-time = "2026-01-02T09:12:51.11Z" },
    { url = "https://files.pythonhosted.org/packages/57/61/571163a5ef86ec0cf30d265ac2a70ae6fc9e28413d1dc94fa37fae6bda89/pillow-12.1.0-cp314-cp314t-macosx_11_0_arm64.whl", hash = "sha256:896866d2d436563fa2a43a9d72f417874f16b5545955c54a64941e87c1376c61", size = 4660426, upload-time = "2026-01-02T09:12:52.865Z" },
    { url = "https://files.pythonhosted.org/packages/5e/e1/53ee5163f794aef1bf84243f755ee6897a92c708505350dd1923f4afec48/pillow-12.1.0-cp314-cp314t-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:8e178e3e99d3c0ea8fc64b88447f7cac8ccf058af422a6cedc690d0eadd98c51", size = 6269908, upload-time = "2026-01-02T09:12:54.884Z" },
    { url = "https://files.pythonhosted.org/packages/bc/0b/b4b4106ff0ee1afa1dc599fde6ab230417f800279745124f6c50bcffed8e/pillow-12.1.0-cp314-cp314t-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:079af2fb0c599c2ec144ba2c02766d1b55498e373b3ac64687e43849fbbef5bc", size = 8074733, upload-time = "2026-01-02T09:12:56.802Z" },
    { url = "https://files.pythonhosted.org/packages/19/9f/80b411cbac4a732439e629a26ad3ef11907a8c7fc5377b7602f04f6fe4e7/pillow-12.1.0-cp314-cp314t-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:bdec5e43377761c5dbca620efb69a77f6855c5a379e32ac5b158f54c84212b14", size = 6381431, upload-time = "2026-01-02T09:12:58.823Z" },
    { url = "https://files.pythonhosted.org/packages/8f/b7/d65c45db463b66ecb6abc17c6ba6917a911202a07662247e1355ce1789e7/pillow-12.1.0-cp314-cp314t-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:565c986f4b45c020f5421a4cea13ef294dde9509a8577f29b2fc5edc7587fff8", size = 7068529, upload-time = "2026-01-02T09:13:00.885Z" },
    { url = "https://files.pythonhosted.org/packages/50/96/dfd4cd726b4a45ae6e3c669fc9e49deb2241312605d33aba50499e9d9bd1/pillow-12.1.0-cp314-cp314t-musllinux_1_2_aarch64.whl", hash = "sha256:43aca0a55ce1eefc0aefa6253661cb54571857b1a7b2964bd8a1e3ef4b729924", size = 6492981, upload-time = "2026-01-02T09:13:03.314Z" },
    { url = "https://files.pythonhosted.org/packages/4d/1c/b5dc52cf713ae46033359c5ca920444f18a6359ce1020dd3e9c553ea5bc6/pillow-12.1.0-cp314-cp314t-musllinux_1_2_x86_64.whl", hash = "sha256:0deedf2ea233722476b3a81e8cdfbad786f7adbed5d848469fa59fe52396e4ef", size = 7191878, upload-time = "2026-01-02T09:13:05.276Z" },
    { url = "https://files.pythonhosted.org/packages/53/26/c4188248bd5edaf543864fe4834aebe9c9cb4968b6f573ce014cc42d0720/pillow-12.1.0-cp314-cp314t-win32.whl", hash = "sha256:b17fbdbe01c196e7e159aacb889e091f28e61020a8abeac07b68079b6e626988", size = 6438703, upload-time = "2026-01-02T09:13:07.491Z" },
    { url = "https://files.pythonhosted.org/packages/b8/0e/69ed296de8ea05cb03ee139cee600f424ca166e632567b2d66727f08c7ed/pillow-12.1.0-cp314-cp314t-win_amd64.whl", hash = "sha256:27b9baecb428899db6c0de572d6d305cfaf38ca1596b5c0542a5182e3e74e8c6", size = 7182927, upload-time = "2026-01-02T09:13:09.841Z" },
    { url = "https://files.pythonhosted.org/packages/fc/f5/68334c015eed9b5cff77814258717dec591ded209ab5b6fb70e2ae873d1d/pillow-12.1.0-cp314-cp314t-win_arm64.whl", hash = "sha256:f61333d817698bdcdd0f9d7793e365ac3d2a21c1f1eb02b32ad6aefb8d8ea831", size = 2545104, upload-time = "2026-01-02T09:13:12.068Z" },
]

[[package]]
name = "pycparser"
version = "3.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/1b/7d/92392ff7815c21062bea51aa7b87d45576f649f16458d78b7cf94b9ab2e6/pycparser-3.0.tar.gz", hash = "sha256:600f49d217304a5902ac3c37e1281c9fe94e4d0489de643a9504c5cdfdfc6b29", size = 103492, upload-time = "2026-01-21T14:26:51.89Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0c/c3/44f3fbbfa403ea2a7c779186dc20772604442dde72947e7d01069cbe98e3/pycparser-3.0-py3-none-any.whl", hash = "sha256:b727414169a36b7d524c1c3e31839a521725078d7b2ff038656844266160a992", size = 48172, upload-time = "2026-01-21T14:26:50.693Z" },
]

[[package]]
name = "pydantic"
version = "2.12.5"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "annotated-types" },
    { name = "pydantic-core" },
    { name = "typing-extensions" },
    { name = "typing-inspection" },
]
sdist = { url = "https://files.pythonhosted.org/packages/69/44/36f1a6e523abc58ae5f928898e4aca2e0ea509b5aa6f6f392a5d882be928/pydantic-2.12.5.tar.gz", hash = "sha256:4d351024c75c0f085a9febbb665ce8c0c6ec5d30e903bdb6394b7ede26aebb49", size = 821591, upload-time = "2025-11-26T15:11:46.471Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/5a/87/b70ad306ebb6f9b585f114d0ac2137d792b48be34d732d60e597c2f8465a/pydantic-2.12.5-py3-none-any.whl", hash = "sha256:e561593fccf61e8a20fc46dfc2dfe075b8be7d0188df33f221ad1f0139180f9d", size = 463580, upload-time = "2025-11-26T15:11:44.605Z" },
]

[[package]]
name = "pydantic-core"
version = "2.41.5"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/71/70/23b021c950c2addd24ec408e9ab05d59b035b39d97cdc1130e1bce647bb6/pydantic_core-2.41.5.tar.gz", hash = "sha256:08daa51ea16ad373ffd5e7606252cc32f07bc72b28284b6bc9c6df804816476e", size = 460952, upload-time = "2025-11-04T13:43:49.098Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/5f/5d/5f6c63eebb5afee93bcaae4ce9a898f3373ca23df3ccaef086d0233a35a7/pydantic_core-2.41.5-cp312-cp312-macosx_10_12_x86_64.whl", hash = "sha256:f41a7489d32336dbf2199c8c0a215390a751c5b014c2c1c5366e817202e9cdf7", size = 2110990, upload-time = "2025-11-04T13:39:58.079Z" },
    { url = "https://files.pythonhosted.org/packages/aa/32/9c2e8ccb57c01111e0fd091f236c7b371c1bccea0fa85247ac55b1e2b6b6/pydantic_core-2.41.5-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:070259a8818988b9a84a449a2a7337c7f430a22acc0859c6b110aa7212a6d9c0", size = 1896003, upload-time = "2025-11-04T13:39:59.956Z" },
    { url = "https://files.pythonhosted.org/packages/68/b8/a01b53cb0e59139fbc9e4fda3e9724ede8de279097179be4ff31f1abb65a/pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:e96cea19e34778f8d59fe40775a7a574d95816eb150850a85a7a4c8f4b94ac69", size = 1919200, upload-time = "2025-11-04T13:40:02.241Z" },
    { url = "https://files.pythonhosted.org/packages/38/de/8c36b5198a29bdaade07b5985e80a233a5ac27137846f3bc2d3b40a47360/pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:ed2e99c456e3fadd05c991f8f437ef902e00eedf34320ba2b0842bd1c3ca3a75", size = 2052578, upload-time = "2025-11-04T13:40:04.401Z" },
    { url = "https://files.pythonhosted.org/packages/00/b5/0e8e4b5b081eac6cb3dbb7e60a65907549a1ce035a724368c330112adfdd/pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:65840751b72fbfd82c3c640cff9284545342a4f1eb1586ad0636955b261b0b05", size = 2208504, upload-time = "2025-11-04T13:40:06.072Z" },
    { url = "https://files.pythonhosted.org/packages/77/56/87a61aad59c7c5b9dc8caad5a41a5545cba3810c3e828708b3d7404f6cef/pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:e536c98a7626a98feb2d3eaf75944ef6f3dbee447e1f841eae16f2f0a72d8ddc", size = 2335816, upload-time = "2025-11-04T13:40:07.835Z" },
    { url = "https://files.pythonhosted.org/packages/0d/76/941cc9f73529988688a665a5c0ecff1112b3d95ab48f81db5f7606f522d3/pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:eceb81a8d74f9267ef4081e246ffd6d129da5d87e37a77c9bde550cb04870c1c", size = 2075366, upload-time = "2025-11-04T13:40:09.804Z" },
    { url = "https://files.pythonhosted.org/packages/d3/43/ebef01f69baa07a482844faaa0a591bad1ef129253ffd0cdaa9d8a7f72d3/pydantic_core-2.41.5-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:d38548150c39b74aeeb0ce8ee1d8e82696f4a4e16ddc6de7b1d8823f7de4b9b5", size = 2171698, upload-time = "2025-11-04T13:40:12.004Z" },
    { url = "https://files.pythonhosted.org/packages/b1/87/41f3202e4193e3bacfc2c065fab7706ebe81af46a83d3e27605029c1f5a6/pydantic_core-2.41.5-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:c23e27686783f60290e36827f9c626e63154b82b116d7fe9adba1fda36da706c", size = 2132603, upload-time = "2025-11-04T13:40:13.868Z" },
    { url = "https://files.pythonhosted.org/packages/49/7d/4c00df99cb12070b6bccdef4a195255e6020a550d572768d92cc54dba91a/pydantic_core-2.41.5-cp312-cp312-musllinux_1_1_armv7l.whl", hash = "sha256:482c982f814460eabe1d3bb0adfdc583387bd4691ef00b90575ca0d2b6fe2294", size = 2329591, upload-time = "2025-11-04T13:40:15.672Z" },
    { url = "https://files.pythonhosted.org/packages/cc/6a/ebf4b1d65d458f3cda6a7335d141305dfa19bdc61140a884d165a8a1bbc7/pydantic_core-2.41.5-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:bfea2a5f0b4d8d43adf9d7b8bf019fb46fdd10a2e5cde477fbcb9d1fa08c68e1", size = 2319068, upload-time = "2025-11-04T13:40:17.532Z" },
    { url = "https://files.pythonhosted.org/packages/49/3b/774f2b5cd4192d5ab75870ce4381fd89cf218af999515baf07e7206753f0/pydantic_core-2.41.5-cp312-cp312-win32.whl", hash = "sha256:b74557b16e390ec12dca509bce9264c3bbd128f8a2c376eaa68003d7f327276d", size = 1985908, upload-time = "2025-11-04T13:40:19.309Z" },
    { url = "https://files.pythonhosted.org/packages/86/45/00173a033c801cacf67c190fef088789394feaf88a98a7035b0e40d53dc9/pydantic_core-2.41.5-cp312-cp312-win_amd64.whl", hash = "sha256:1962293292865bca8e54702b08a4f26da73adc83dd1fcf26fbc875b35d81c815", size = 2020145, upload-time = "2025-11-04T13:40:21.548Z" },
    { url = "https://files.pythonhosted.org/packages/f9/22/91fbc821fa6d261b376a3f73809f907cec5ca6025642c463d3488aad22fb/pydantic_core-2.41.5-cp312-cp312-win_arm64.whl", hash = "sha256:1746d4a3d9a794cacae06a5eaaccb4b8643a131d45fbc9af23e353dc0a5ba5c3", size = 1976179, upload-time = "2025-11-04T13:40:23.393Z" },
    { url = "https://files.pythonhosted.org/packages/87/06/8806241ff1f70d9939f9af039c6c35f2360cf16e93c2ca76f184e76b1564/pydantic_core-2.41.5-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:941103c9be18ac8daf7b7adca8228f8ed6bb7a1849020f643b3a14d15b1924d9", size = 2120403, upload-time = "2025-11-04T13:40:25.248Z" },
    { url = "https://files.pythonhosted.org/packages/94/02/abfa0e0bda67faa65fef1c84971c7e45928e108fe24333c81f3bfe35d5f5/pydantic_core-2.41.5-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:112e305c3314f40c93998e567879e887a3160bb8689ef3d2c04b6cc62c33ac34", size = 1896206, upload-time = "2025-11-04T13:40:27.099Z" },
    { url = "https://files.pythonhosted.org/packages/15/df/a4c740c0943e93e6500f9eb23f4ca7ec9bf71b19e608ae5b579678c8d02f/pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0cbaad15cb0c90aa221d43c00e77bb33c93e8d36e0bf74760cd00e732d10a6a0", size = 1919307, upload-time = "2025-11-04T13:40:29.806Z" },
    { url = "https://files.pythonhosted.org/packages/9a/e3/6324802931ae1d123528988e0e86587c2072ac2e5394b4bc2bc34b61ff6e/pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:03ca43e12fab6023fc79d28ca6b39b05f794ad08ec2feccc59a339b02f2b3d33", size = 2063258, upload-time = "2025-11-04T13:40:33.544Z" },
    { url = "https://files.pythonhosted.org/packages/c9/d4/2230d7151d4957dd79c3044ea26346c148c98fbf0ee6ebd41056f2d62ab5/pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:dc799088c08fa04e43144b164feb0c13f9a0bc40503f8df3e9fde58a3c0c101e", size = 2214917, upload-time = "2025-11-04T13:40:35.479Z" },
    { url = "https://files.pythonhosted.org/packages/e6/9f/eaac5df17a3672fef0081b6c1bb0b82b33ee89aa5cec0d7b05f52fd4a1fa/pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:97aeba56665b4c3235a0e52b2c2f5ae9cd071b8a8310ad27bddb3f7fb30e9aa2", size = 2332186, upload-time = "2025-11-04T13:40:37.436Z" },
    { url = "https://files.pythonhosted.org/packages/cf/4e/35a80cae583a37cf15604b44240e45c05e04e86f9cfd766623149297e971/pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:406bf18d345822d6c21366031003612b9c77b3e29ffdb0f612367352aab7d586", size = 2073164, upload-time = "2025-11-04T13:40:40.289Z" },
    { url = "https://files.pythonhosted.org/packages/bf/e3/f6e262673c6140dd3305d144d032f7bd5f7497d3871c1428521f19f9efa2/pydantic_core-2.41.5-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:b93590ae81f7010dbe380cdeab6f515902ebcbefe0b9327cc4804d74e93ae69d", size = 2179146, upload-time = "2025-11-04T13:40:42.809Z" },
    { url = "https://files.pythonhosted.org/packages/75/c7/20bd7fc05f0c6ea2056a4565c6f36f8968c0924f19b7d97bbfea55780e73/pydantic_core-2.41.5-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:01a3d0ab748ee531f4ea6c3e48ad9dac84ddba4b0d82291f87248f2f9de8d740", size = 2137788, upload-time = "2025-11-04T13:40:44.752Z" },
    { url = "https://files.pythonhosted.org/packages/3a/8d/34318ef985c45196e004bc46c6eab2eda437e744c124ef0dbe1ff2c9d06b/pydantic_core-2.41.5-cp313-cp313-musllinux_1_1_armv7l.whl", hash = "sha256:6561e94ba9dacc9c61bce40e2d6bdc3bfaa0259d3ff36ace3b1e6901936d2e3e", size = 2340133, upload-time = "2025-11-04T13:40:46.66Z" },
    { url = "https://files.pythonhosted.org/packages/9c/59/013626bf8c78a5a5d9350d12e7697d3d4de951a75565496abd40ccd46bee/pydantic_core-2.41.5-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:915c3d10f81bec3a74fbd4faebe8391013ba61e5a1a8d48c4455b923bdda7858", size = 2324852, upload-time = "2025-11-04T13:40:48.575Z" },
    { url = "https://files.pythonhosted.org/packages/1a/d9/c248c103856f807ef70c18a4f986693a46a8ffe1602e5d361485da502d20/pydantic_core-2.41.5-cp313-cp313-win32.whl", hash = "sha256:650ae77860b45cfa6e2cdafc42618ceafab3a2d9a3811fcfbd3bbf8ac3c40d36", size = 1994679, upload-time = "2025-11-04T13:40:50.619Z" },
    { url = "https://files.pythonhosted.org/packages/9e/8b/341991b158ddab181cff136acd2552c9f35bd30380422a639c0671e99a91/pydantic_core-2.41.5-cp313-cp313-win_amd64.whl", hash = "sha256:79ec52ec461e99e13791ec6508c722742ad745571f234ea6255bed38c6480f11", size = 2019766, upload-time = "2025-11-04T13:40:52.631Z" },
    { url = "https://files.pythonhosted.org/packages/73/7d/f2f9db34af103bea3e09735bb40b021788a5e834c81eedb541991badf8f5/pydantic_core-2.41.5-cp313-cp313-win_arm64.whl", hash = "sha256:3f84d5c1b4ab906093bdc1ff10484838aca54ef08de4afa9de0f5f14d69639cd", size = 1981005, upload-time = "2025-11-04T13:40:54.734Z" },
    { url = "https://files.pythonhosted.org/packages/ea/28/46b7c5c9635ae96ea0fbb779e271a38129df2550f763937659ee6c5dbc65/pydantic_core-2.41.5-cp314-cp314-macosx_10_12_x86_64.whl", hash = "sha256:3f37a19d7ebcdd20b96485056ba9e8b304e27d9904d233d7b1015db320e51f0a", size = 2119622, upload-time = "2025-11-04T13:40:56.68Z" },
    { url = "https://files.pythonhosted.org/packages/74/1a/145646e5687e8d9a1e8d09acb278c8535ebe9e972e1f162ed338a622f193/pydantic_core-2.41.5-cp314-cp314-macosx_11_0_arm64.whl", hash = "sha256:1d1d9764366c73f996edd17abb6d9d7649a7eb690006ab6adbda117717099b14", size = 1891725, upload-time = "2025-11-04T13:40:58.807Z" },
    { url = "https://files.pythonhosted.org/packages/23/04/e89c29e267b8060b40dca97bfc64a19b2a3cf99018167ea1677d96368273/pydantic_core-2.41.5-cp314-cp314-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:25e1c2af0fce638d5f1988b686f3b3ea8cd7de5f244ca147c777769e798a9cd1", size = 1915040, upload-time = "2025-11-04T13:41:00.853Z" },
    { url = "https://files.pythonhosted.org/packages/84/a3/15a82ac7bd97992a82257f777b3583d3e84bdb06ba6858f745daa2ec8a85/pydantic_core-2.41.5-cp314-cp314-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:506d766a8727beef16b7adaeb8ee6217c64fc813646b424d0804d67c16eddb66", size = 2063691, upload-time = "2025-11-04T13:41:03.504Z" },
    { url = "https://files.pythonhosted.org/packages/74/9b/0046701313c6ef08c0c1cf0e028c67c770a4e1275ca73131563c5f2a310a/pydantic_core-2.41.5-cp314-cp314-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:4819fa52133c9aa3c387b3328f25c1facc356491e6135b459f1de698ff64d869", size = 2213897, upload-time = "2025-11-04T13:41:05.804Z" },
    { url = "https://files.pythonhosted.org/packages/8a/cd/6bac76ecd1b27e75a95ca3a9a559c643b3afcd2dd62086d4b7a32a18b169/pydantic_core-2.41.5-cp314-cp314-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2b761d210c9ea91feda40d25b4efe82a1707da2ef62901466a42492c028553a2", size = 2333302, upload-time = "2025-11-04T13:41:07.809Z" },
    { url = "https://files.pythonhosted.org/packages/4c/d2/ef2074dc020dd6e109611a8be4449b98cd25e1b9b8a303c2f0fca2f2bcf7/pydantic_core-2.41.5-cp314-cp314-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:22f0fb8c1c583a3b6f24df2470833b40207e907b90c928cc8d3594b76f874375", size = 2064877, upload-time = "2025-11-04T13:41:09.827Z" },
    { url = "https://files.pythonhosted.org/packages/18/66/e9db17a9a763d72f03de903883c057b2592c09509ccfe468187f2a2eef29/pydantic_core-2.41.5-cp314-cp314-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:2782c870e99878c634505236d81e5443092fba820f0373997ff75f90f68cd553", size = 2180680, upload-time = "2025-11-04T13:41:12.379Z" },
    { url = "https://files.pythonhosted.org/packages/d3/9e/3ce66cebb929f3ced22be85d4c2399b8e85b622db77dad36b73c5387f8f8/pydantic_core-2.41.5-cp314-cp314-musllinux_1_1_aarch64.whl", hash = "sha256:0177272f88ab8312479336e1d777f6b124537d47f2123f89cb37e0accea97f90", size = 2138960, upload-time = "2025-11-04T13:41:14.627Z" },
    { url = "https://files.pythonhosted.org/packages/a6/62/205a998f4327d2079326b01abee48e502ea739d174f0a89295c481a2272e/pydantic_core-2.41.5-cp314-cp314-musllinux_1_1_armv7l.whl", hash = "sha256:63510af5e38f8955b8ee5687740d6ebf7c2a0886d15a6d65c32814613681bc07", size = 2339102, upload-time = "2025-11-04T13:41:16.868Z" },
    { url = "https://files.pythonhosted.org/packages/3c/0d/f05e79471e889d74d3d88f5bd20d0ed189ad94c2423d81ff8d0000aab4ff/pydantic_core-2.41.5-cp314-cp314-musllinux_1_1_x86_64.whl", hash = "sha256:e56ba91f47764cc14f1daacd723e3e82d1a89d783f0f5afe9c364b8bb491ccdb", size = 2326039, upload-time = "2025-11-04T13:41:18.934Z" },
    { url = "https://files.pythonhosted.org/packages/ec/e1/e08a6208bb100da7e0c4b288eed624a703f4d129bde2da475721a80cab32/pydantic_core-2.41.5-cp314-cp314-win32.whl", hash = "sha256:aec5cf2fd867b4ff45b9959f8b20ea3993fc93e63c7363fe6851424c8a7e7c23", size = 1995126, upload-time = "2025-11-04T13:41:21.418Z" },
    { url = "https://files.pythonhosted.org/packages/48/5d/56ba7b24e9557f99c9237e29f5c09913c81eeb2f3217e40e922353668092/pydantic_core-2.41.5-cp314-cp314-win_amd64.whl", hash = "sha256:8e7c86f27c585ef37c35e56a96363ab8de4e549a95512445b85c96d3e2f7c1bf", size = 2015489, upload-time = "2025-11-04T13:41:24.076Z" },
    { url = "https://files.pythonhosted.org/packages/4e/bb/f7a190991ec9e3e0ba22e4993d8755bbc4a32925c0b5b42775c03e8148f9/pydantic_core-2.41.5-cp314-cp314-win_arm64.whl", hash = "sha256:e672ba74fbc2dc8eea59fb6d4aed6845e6905fc2a8afe93175d94a83ba2a01a0", size = 1977288, upload-time = "2025-11-04T13:41:26.33Z" },
    { url = "https://files.pythonhosted.org/packages/92/ed/77542d0c51538e32e15afe7899d79efce4b81eee631d99850edc2f5e9349/pydantic_core-2.41.5-cp314-cp314t-macosx_10_12_x86_64.whl", hash = "sha256:8566def80554c3faa0e65ac30ab0932b9e3a5cd7f8323764303d468e5c37595a", size = 2120255, upload-time = "2025-11-04T13:41:28.569Z" },
    { url = "https://files.pythonhosted.org/packages/bb/3d/6913dde84d5be21e284439676168b28d8bbba5600d838b9dca99de0fad71/pydantic_core-2.41.5-cp314-cp314t-macosx_11_0_arm64.whl", hash = "sha256:b80aa5095cd3109962a298ce14110ae16b8c1aece8b72f9dafe81cf597ad80b3", size = 1863760, upload-time = "2025-11-04T13:41:31.055Z" },
    { url = "https://files.pythonhosted.org/packages/5a/f0/e5e6b99d4191da102f2b0eb9687aaa7f5bea5d9964071a84effc3e40f997/pydantic_core-2.41.5-cp314-cp314t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3006c3dd9ba34b0c094c544c6006cc79e87d8612999f1a5d43b769b89181f23c", size = 1878092, upload-time = "2025-11-04T13:41:33.21Z" },
    { url = "https://files.pythonhosted.org/packages/71/48/36fb760642d568925953bcc8116455513d6e34c4beaa37544118c36aba6d/pydantic_core-2.41.5-cp314-cp314t-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:72f6c8b11857a856bcfa48c86f5368439f74453563f951e473514579d44aa612", size = 2053385, upload-time = "2025-11-04T13:41:35.508Z" },
    { url = "https://files.pythonhosted.org/packages/20/25/92dc684dd8eb75a234bc1c764b4210cf2646479d54b47bf46061657292a8/pydantic_core-2.41.5-cp314-cp314t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:5cb1b2f9742240e4bb26b652a5aeb840aa4b417c7748b6f8387927bc6e45e40d", size = 2218832, upload-time = "2025-11-04T13:41:37.732Z" },
    { url = "https://files.pythonhosted.org/packages/e2/09/f53e0b05023d3e30357d82eb35835d0f6340ca344720a4599cd663dca599/pydantic_core-2.41.5-cp314-cp314t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:bd3d54f38609ff308209bd43acea66061494157703364ae40c951f83ba99a1a9", size = 2327585, upload-time = "2025-11-04T13:41:40Z" },
    { url = "https://files.pythonhosted.org/packages/aa/4e/2ae1aa85d6af35a39b236b1b1641de73f5a6ac4d5a7509f77b814885760c/pydantic_core-2.41.5-cp314-cp314t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2ff4321e56e879ee8d2a879501c8e469414d948f4aba74a2d4593184eb326660", size = 2041078, upload-time = "2025-11-04T13:41:42.323Z" },
    { url = "https://files.pythonhosted.org/packages/cd/13/2e215f17f0ef326fc72afe94776edb77525142c693767fc347ed6288728d/pydantic_core-2.41.5-cp314-cp314t-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:d0d2568a8c11bf8225044aa94409e21da0cb09dcdafe9ecd10250b2baad531a9", size = 2173914, upload-time = "2025-11-04T13:41:45.221Z" },
    { url = "https://files.pythonhosted.org/packages/02/7a/f999a6dcbcd0e5660bc348a3991c8915ce6599f4f2c6ac22f01d7a10816c/pydantic_core-2.41.5-cp314-cp314t-musllinux_1_1_aarch64.whl", hash = "sha256:a39455728aabd58ceabb03c90e12f71fd30fa69615760a075b9fec596456ccc3", size = 2129560, upload-time = "2025-11-04T13:41:47.474Z" },
    { url = "https://files.pythonhosted.org/packages/3a/b1/6c990ac65e3b4c079a4fb9f5b05f5b013afa0f4ed6780a3dd236d2cbdc64/pydantic_core-2.41.5-cp314-cp314t-musllinux_1_1_armv7l.whl", hash = "sha256:239edca560d05757817c13dc17c50766136d21f7cd0fac50295499ae24f90fdf", size = 2329244, upload-time = "2025-11-04T13:41:49.992Z" },
    { url = "https://files.pythonhosted.org/packages/d9/02/3c562f3a51afd4d88fff8dffb1771b30cfdfd79befd9883ee094f5b6c0d8/pydantic_core-2.41.5-cp314-cp314t-musllinux_1_1_x86_64.whl", hash = "sha256:2a5e06546e19f24c6a96a129142a75cee553cc018ffee48a460059b1185f4470", size = 2331955, upload-time = "2025-11-04T13:41:54.079Z" },
    { url = "https://files.pythonhosted.org/packages/5c/96/5fb7d8c3c17bc8c62fdb031c47d77a1af698f1d7a406b0f79aaa1338f9ad/pydantic_core-2.41.5-cp314-cp314t-win32.whl", hash = "sha256:b4ececa40ac28afa90871c2cc2b9ffd2ff0bf749380fbdf57d165fd23da353aa", size = 1988906, upload-time = "2025-11-04T13:41:56.606Z" },
    { url = "https://files.pythonhosted.org/packages/22/ed/182129d83032702912c2e2d8bbe33c036f342cc735737064668585dac28f/pydantic_core-2.41.5-cp314-cp314t-win_amd64.whl", hash = "sha256:80aa89cad80b32a912a65332f64a4450ed00966111b6615ca6816153d3585a8c", size = 1981607, upload-time = "2025-11-04T13:41:58.889Z" },
    { url = "https://files.pythonhosted.org/packages/9f/ed/068e41660b832bb0b1aa5b58011dea2a3fe0ba7861ff38c4d4904c1c1a99/pydantic_core-2.41.5-cp314-cp314t-win_arm64.whl", hash = "sha256:35b44f37a3199f771c3eaa53051bc8a70cd7b54f333531c59e29fd4db5d15008", size = 1974769, upload-time = "2025-11-04T13:42:01.186Z" },
    { url = "https://files.pythonhosted.org/packages/09/32/59b0c7e63e277fa7911c2fc70ccfb45ce4b98991e7ef37110663437005af/pydantic_core-2.41.5-graalpy312-graalpy250_312_native-macosx_10_12_x86_64.whl", hash = "sha256:7da7087d756b19037bc2c06edc6c170eeef3c3bafcb8f532ff17d64dc427adfd", size = 2110495, upload-time = "2025-11-04T13:42:49.689Z" },
    { url = "https://files.pythonhosted.org/packages/aa/81/05e400037eaf55ad400bcd318c05bb345b57e708887f07ddb2d20e3f0e98/pydantic_core-2.41.5-graalpy312-graalpy250_312_native-macosx_11_0_arm64.whl", hash = "sha256:aabf5777b5c8ca26f7824cb4a120a740c9588ed58df9b2d196ce92fba42ff8dc", size = 1915388, upload-time = "2025-11-04T13:42:52.215Z" },
    { url = "https://files.pythonhosted.org/packages/6e/0d/e3549b2399f71d56476b77dbf3cf8937cec5cd70536bdc0e374a421d0599/pydantic_core-2.41.5-graalpy312-graalpy250_312_native-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c007fe8a43d43b3969e8469004e9845944f1a80e6acd47c150856bb87f230c56", size = 1942879, upload-time = "2025-11-04T13:42:56.483Z" },
    { url = "https://files.pythonhosted.org/packages/f7/07/34573da085946b6a313d7c42f82f16e8920bfd730665de2d11c0c37a74b5/pydantic_core-2.41.5-graalpy312-graalpy250_312_native-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:76d0819de158cd855d1cbb8fcafdf6f5cf1eb8e470abe056d5d161106e38062b", size = 2139017, upload-time = "2025-11-04T13:42:59.471Z" },
]

[[package]]
name = "pypdf"
version = "6.6.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/90/da/50a15233ca512dcac68fd6edeabbf1cad5260e7f032f5a24934a7d5662c5/pypdf-6.6.1.tar.gz", hash = "sha256:7ea09ab3748644dacef1a30c475d1edc031656f69c03332c291ebf5b9291a909", size = 5281346, upload-time = "2026-01-25T14:13:36.275Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b9/cc/d9fd9f87bec8ebbfde76aaa9e30703e37810118e913538ef6526e94ebe51/pypdf-6.6.1-py3-none-any.whl", hash = "sha256:453354ddb4398319197f4006fbd1b93c4fbc995f15b15c0af88cba99494ce65a", size = 328987, upload-time = "2026-01-25T14:13:34.901Z" },
]

[[package]]
name = "pypdfium2"
version = "5.3.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/18/83/173dab58beb6c7e772b838199014c173a2436018dd7cfde9bbf4a3be15da/pypdfium2-5.3.0.tar.gz", hash = "sha256:2873ffc95fcb01f329257ebc64a5fdce44b36447b6b171fe62f7db5dc3269885", size = 268742, upload-time = "2026-01-05T16:29:03.02Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e3/a4/6bb5b5918c7fc236ec426be8a0205a984fe0a26ae23d5e4dd497398a6571/pypdfium2-5.3.0-py3-none-android_23_arm64_v8a.whl", hash = "sha256:885df6c78d41600cb086dc0c76b912d165b5bd6931ca08138329ea5a991b3540", size = 2763287, upload-time = "2026-01-05T16:28:24.21Z" },
    { url = "https://files.pythonhosted.org/packages/3e/64/24b41b906006bf07099b095f0420ee1f01a3a83a899f3e3731e4da99c06a/pypdfium2-5.3.0-py3-none-android_23_armeabi_v7a.whl", hash = "sha256:6e53dee6b333ee77582499eff800300fb5aa0c7eb8f52f95ccb5ca35ebc86d48", size = 2303285, upload-time = "2026-01-05T16:28:26.274Z" },
    { url = "https://files.pythonhosted.org/packages/c2/c0/3ec73f4ded83ba6c02acf6e9d228501759d5d74fe57f1b93849ab92dcc20/pypdfium2-5.3.0-py3-none-macosx_11_0_arm64.whl", hash = "sha256:ce4466bdd62119fe25a5f74d107acc9db8652062bf217057630c6ff0bb419523", size = 2816066, upload-time = "2026-01-05T16:28:28.099Z" },
    { url = "https://files.pythonhosted.org/packages/62/ca/e553b3b8b5c2cdc3d955cc313493ac27bbe63fc22624769d56ded585dd5e/pypdfium2-5.3.0-py3-none-macosx_11_0_x86_64.whl", hash = "sha256:cc2647fd03db42b8a56a8835e8bc7899e604e2042cd6fedeea53483185612907", size = 2945545, upload-time = "2026-01-05T16:28:29.489Z" },
    { url = "https://files.pythonhosted.org/packages/a1/56/615b776071e95c8570d579038256d0c77969ff2ff381e427be4ab8967f44/pypdfium2-5.3.0-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:35e205f537ddb4069e4b4e22af7ffe84fcf2d686c3fee5e5349f73268a0ef1ca", size = 2979892, upload-time = "2026-01-05T16:28:31.088Z" },
    { url = "https://files.pythonhosted.org/packages/df/10/27114199b765bdb7d19a9514c07036ad2fc3a579b910e7823ba167ead6de/pypdfium2-5.3.0-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:b5795298f44050797ac030994fc2525ea35d2d714efe70058e0ee22e5f613f27", size = 2765738, upload-time = "2026-01-05T16:28:33.18Z" },
    { url = "https://files.pythonhosted.org/packages/b4/d7/2a3afa35e6c205a4f6264c33b8d2f659707989f93c30b336aa58575f66fa/pypdfium2-5.3.0-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:b7cd43dfceb77137e69e74c933d41506da1dddaff70f3a794fb0ad0d73e90d75", size = 3064338, upload-time = "2026-01-05T16:28:34.731Z" },
    { url = "https://files.pythonhosted.org/packages/a2/f1/6658755cf6e369bb51d0bccb81c51c300404fbe67c2f894c90000b6442dd/pypdfium2-5.3.0-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d5956867558fd3a793e58691cf169718864610becb765bfe74dd83f05cbf1ae3", size = 3415059, upload-time = "2026-01-05T16:28:37.313Z" },
    { url = "https://files.pythonhosted.org/packages/f5/34/f86482134fa641deb1f524c45ec7ebd6fc8d404df40c5657ddfce528593e/pypdfium2-5.3.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b3ff1071e9a782625822658dfe6e29e3a644a66960f8713bb17819f5a0ac5987", size = 2998517, upload-time = "2026-01-05T16:28:38.873Z" },
    { url = "https://files.pythonhosted.org/packages/09/34/40ab99425dcf503c172885904c5dc356c052bfdbd085f9f3cc920e0b8b25/pypdfium2-5.3.0-py3-none-manylinux_2_27_s390x.manylinux_2_28_s390x.whl", hash = "sha256:f319c46ead49d289ab8c1ed2ea63c91e684f35bdc4cf4dc52191c441182ac481", size = 3673154, upload-time = "2026-01-05T16:28:40.347Z" },
    { url = "https://files.pythonhosted.org/packages/a5/67/0f7532f80825a7728a5cbff3f1104857f8f9fe49ebfd6cb25582a89ae8e1/pypdfium2-5.3.0-py3-none-manylinux_2_38_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:6dc67a186da0962294321cace6ccc0a4d212dbc5e9522c640d35725a812324b8", size = 2965002, upload-time = "2026-01-05T16:28:42.143Z" },
    { url = "https://files.pythonhosted.org/packages/ce/6c/c03d2a3d6621b77aac9604bce1c060de2af94950448787298501eac6c6a2/pypdfium2-5.3.0-py3-none-musllinux_1_2_aarch64.whl", hash = "sha256:0ad0afd3d2b5b54d86287266fd6ae3fef0e0a1a3df9d2c4984b3e3f8f70e6330", size = 4130530, upload-time = "2026-01-05T16:28:44.264Z" },
    { url = "https://files.pythonhosted.org/packages/af/39/9ad1f958cbe35d4693ae87c09ebafda4bb3e4709c7ccaec86c1a829163a3/pypdfium2-5.3.0-py3-none-musllinux_1_2_armv7l.whl", hash = "sha256:1afe35230dc3951b3e79b934c0c35a2e79e2372d06503fce6cf1926d2a816f47", size = 3746568, upload-time = "2026-01-05T16:28:45.897Z" },
    { url = "https://files.pythonhosted.org/packages/2a/e2/4d32310166c2d6955d924737df8b0a3e3efc8d133344a98b10f96320157d/pypdfium2-5.3.0-py3-none-musllinux_1_2_i686.whl", hash = "sha256:00385793030cadce08469085cd21b168fd8ff981b009685fef3103bdc5fc4686", size = 4336683, upload-time = "2026-01-05T16:28:47.584Z" },
    { url = "https://files.pythonhosted.org/packages/14/ea/38c337ff12a8cec4b00fd4fdb0a63a70597a344581e20b02addbd301ab56/pypdfium2-5.3.0-py3-none-musllinux_1_2_ppc64le.whl", hash = "sha256:d911e82676398949697fef80b7f412078df14d725a91c10e383b727051530285", size = 4375030, upload-time = "2026-01-05T16:28:49.5Z" },
    { url = "https://files.pythonhosted.org/packages/a1/77/9d8de90c35d2fc383be8819bcde52f5821dacbd7404a0225e4010b99d080/pypdfium2-5.3.0-py3-none-musllinux_1_2_riscv64.whl", hash = "sha256:ca1dc625ed347fac3d9002a3ed33d521d5803409bd572e7b3f823c12ab2ef58f", size = 3928914, upload-time = "2026-01-05T16:28:51.433Z" },
    { url = "https://files.pythonhosted.org/packages/a5/39/9d4a6fbd78fcb6803b0ea5e4952a31d6182a0aaa2609cfcd0eb88446fdb8/pypdfium2-5.3.0-py3-none-musllinux_1_2_s390x.whl", hash = "sha256:ea4f9db2d3575f22cd41f4c7a855240ded842f135e59a961b5b1351a65ce2b6e", size = 4997777, upload-time = "2026-01-05T16:28:53.589Z" },
    { url = "https://files.pythonhosted.org/packages/9d/38/cdd4ed085c264234a59ad32df1dfe432c77a7403da2381e0fcc1ba60b74e/pypdfium2-5.3.0-py3-none-musllinux_1_2_x86_64.whl", hash = "sha256:0ea24409613df350223c6afc50911c99dca0d43ddaf2616c5a1ebdffa3e1bcb5", size = 4179895, upload-time = "2026-01-05T16:28:55.322Z" },
    { url = "https://files.pythonhosted.org/packages/93/4c/d2f40145c9012482699664f615d7ae540a346c84f68a8179449e69dcc4d8/pypdfium2-5.3.0-py3-none-win32.whl", hash = "sha256:5bf695d603f9eb8fdd7c1786add5cf420d57fbc81df142ed63c029ce29614df9", size = 2993570, upload-time = "2026-01-05T16:28:58.37Z" },
    { url = "https://files.pythonhosted.org/packages/2c/dc/1388ea650020c26ef3f68856b9227e7f153dcaf445e7e4674a0b8f26891e/pypdfium2-5.3.0-py3-none-win_amd64.whl", hash = "sha256:8365af22a39d4373c265f8e90e561cd64d4ddeaf5e6a66546a8caed216ab9574", size = 3102340, upload-time = "2026-01-05T16:28:59.933Z" },
    { url = "https://files.pythonhosted.org/packages/c8/71/a433668d33999b3aeb2c2dda18aaf24948e862ea2ee148078a35daac6c1c/pypdfium2-5.3.0-py3-none-win_arm64.whl", hash = "sha256:0b2c6bf825e084d91d34456be54921da31e9199d9530b05435d69d1a80501a12", size = 2940987, upload-time = "2026-01-05T16:29:01.511Z" },
]

[[package]]
name = "python-docx"
version = "1.2.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "lxml" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a9/f7/eddfe33871520adab45aaa1a71f0402a2252050c14c7e3009446c8f4701c/python_docx-1.2.0.tar.gz", hash = "sha256:7bc9d7b7d8a69c9c02ca09216118c86552704edc23bac179283f2e38f86220ce", size = 5723256, upload-time = "2025-06-16T20:46:27.921Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d0/00/1e03a4989fa5795da308cd774f05b704ace555a70f9bf9d3be057b680bcf/python_docx-1.2.0-py3-none-any.whl", hash = "sha256:3fd478f3250fbbbfd3b94fe1e985955737c145627498896a8a6bf81f4baf66c7", size = 252987, upload-time = "2025-06-16T20:46:22.506Z" },
]

[[package]]
name = "python-dotenv"
version = "1.2.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f0/26/19cadc79a718c5edbec86fd4919a6b6d3f681039a2f6d66d14be94e75fb9/python_dotenv-1.2.1.tar.gz", hash = "sha256:42667e897e16ab0d66954af0e60a9caa94f0fd4ecf3aaf6d2d260eec1aa36ad6", size = 44221, upload-time = "2025-10-26T15:12:10.434Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/14/1b/a298b06749107c305e1fe0f814c6c74aea7b2f1e10989cb30f544a1b3253/python_dotenv-1.2.1-py3-none-any.whl", hash = "sha256:b81ee9561e9ca4004139c6cbba3a238c32b03e4894671e181b671e8cb8425d61", size = 21230, upload-time = "2025-10-26T15:12:09.109Z" },
]

[[package]]
name = "python-pptx"
version = "1.0.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "lxml" },
    { name = "pillow" },
    { name = "typing-extensions" },
    { name = "xlsxwriter" },
]
sdist = { url = "https://files.pythonhosted.org/packages/52/a9/0c0db8d37b2b8a645666f7fd8accea4c6224e013c42b1d5c17c93590cd06/python_pptx-1.0.2.tar.gz", hash = "sha256:479a8af0eaf0f0d76b6f00b0887732874ad2e3188230315290cd1f9dd9cc7095", size = 10109297, upload-time = "2024-08-07T17:33:37.772Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d9/4f/00be2196329ebbff56ce564aa94efb0fbc828d00de250b1980de1a34ab49/python_pptx-1.0.2-py3-none-any.whl", hash = "sha256:160838e0b8565a8b1f67947675886e9fea18aa5e795db7ae531606d68e785cba", size = 472788, upload-time = "2024-08-07T17:33:28.192Z" },
]

[[package]]
name = "sniffio"
version = "1.3.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a2/87/a6771e1546d97e7e041b6ae58d80074f81b7d5121207425c964ddf5cfdbd/sniffio-1.3.1.tar.gz", hash = "sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc", size = 20372, upload-time = "2024-02-25T23:20:04.057Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl", hash = "sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2", size = 10235, upload-time = "2024-02-25T23:20:01.196Z" },
]

[[package]]
name = "tqdm"
version = "4.67.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a8/4b/29b4ef32e036bb34e4ab51796dd745cdba7ed47ad142a9f4a1eb8e0c744d/tqdm-4.67.1.tar.gz", hash = "sha256:f8aef9c52c08c13a65f30ea34f4e5aac3fd1a34959879d7e59e63027286627f2", size = 169737, upload-time = "2024-11-24T20:12:22.481Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl", hash = "sha256:26445eca388f82e72884e0d580d5464cd801a3ea01e63e5601bdff9ba6a48de2", size = 78540, upload-time = "2024-11-24T20:12:19.698Z" },
]

[[package]]
name = "typing-extensions"
version = "4.15.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/72/94/1a15dd82efb362ac84269196e94cf00f187f7ed21c242792a923cdb1c61f/typing_extensions-4.15.0.tar.gz", hash = "sha256:0cea48d173cc12fa28ecabc3b837ea3cf6f38c6d1136f85cbaaf598984861466", size = 109391, upload-time = "2025-08-25T13:49:26.313Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/18/67/36e9267722cc04a6b9f15c7f3441c2363321a3ea07da7ae0c0707beb2a9c/typing_extensions-4.15.0-py3-none-any.whl", hash = "sha256:f0fa19c6845758ab08074a0cfa8b7aecb71c999ca73d62883bc25cc018c4e548", size = 44614, upload-time = "2025-08-25T13:49:24.86Z" },
]

[[package]]
name = "typing-inspection"
version = "0.4.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/55/e3/70399cb7dd41c10ac53367ae42139cf4b1ca5f36bb3dc6c9d33acdb43655/typing_inspection-0.4.2.tar.gz", hash = "sha256:ba561c48a67c5958007083d386c3295464928b01faa735ab8547c5692e87f464", size = 75949, upload-time = "2025-10-01T02:14:41.687Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/dc/9b/47798a6c91d8bdb567fe2698fe81e0c6b7cb7ef4d13da4114b41d239f65d/typing_inspection-0.4.2-py3-none-any.whl", hash = "sha256:4ed1cacbdc298c220f1bd249ed5287caa16f34d44ef4e9c3d0cbad5b521545e7", size = 14611, upload-time = "2025-10-01T02:14:40.154Z" },
]

[[package]]
name = "xlsxwriter"
version = "3.2.9"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/46/2c/c06ef49dc36e7954e55b802a8b231770d286a9758b3d936bd1e04ce5ba88/xlsxwriter-3.2.9.tar.gz", hash = "sha256:254b1c37a368c444eac6e2f867405cc9e461b0ed97a3233b2ac1e574efb4140c", size = 215940, upload-time = "2025-09-16T00:16:21.63Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/3a/0c/3662f4a66880196a590b202f0db82d919dd2f89e99a27fadef91c4a33d41/xlsxwriter-3.2.9-py3-none-any.whl", hash = "sha256:9a5db42bc5dff014806c58a20b9eae7322a134abb6fce3c92c181bfb275ec5b3", size = 175315, upload-time = "2025-09-16T00:16:20.108Z" },
]



================================================
FILE: .env.sample
================================================
# Doubleword API Configuration
# Copy this file to .env and fill in your actual values

# Your Doubleword API authentication token
DOUBLEWORD_AUTH_TOKEN=

# Doubleword API base URL
DOUBLEWORD_BASE_URL=https://api.doubleword.ai/v1

# API endpoint for chat completions (relative to base URL)
CHAT_COMPLETIONS_ENDPOINT=/v1/chat/completions

# Model to use for batch processing
DOUBLEWORD_MODEL=Qwen/Qwen3-VL-235B-A22B-Instruct-FP8
#DOUBLEWORD_MODEL=Qwen/Qwen3-VL-30B-A3B-Instruct-FP8

# Polling interval in seconds (how often to check batch status)
POLLING_INTERVAL=60

# Batch completion window or SLA (how long the API has to complete the job)
# Options: "1h" or "24h"
COMPLETION_WINDOW=1h

# Summary word count (target length for generated summaries)
SUMMARY_WORD_COUNT=1500

# Maximum tokens for model response (includes reasoning + summary)
# Adjust based on your SUMMARY_WORD_COUNT: roughly word_count * 1.5 + buffer for reasoning
MAX_TOKENS=5000



================================================
FILE: data/summaries_235B_samples/0001__IFoA_DSSCC_Part_1_Green_Bonds__5BNov_2023_5D_summary_20260125_143906.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** Time series analysis of GSS bonds: Part 1 – Introductory analysis of S&P Green Bond Index  
**Authors:** D Dey (Chair), Cem Öztürk, Shubham Mehta (Working Party members)  
**Date of publication:** 2023-11-01  
**Topic:** Time series forecasting  
**Sub-topic:** Application of neural networks and XGBoost to green bond index prediction  
**URL:** Not available  

---

## Summary

### 1. Modeling Techniques

The paper evaluates six distinct modeling approaches for univariate time series forecasting of the S&P Green Bond Index:

- **Baseline Model**: A naive model where today’s index value is assumed equal to yesterday’s value. This serves as a benchmark for comparison.
- **Deep Neural Network (DNN)**: Feedforward neural networks with 1, 2, or 3 hidden dense layers. Variants include DNN0 (1 layer), DNN1 (2 layers), and DNN2 (3 layers).
- **Convolutional Neural Network (CNN)**: Specifically, 1D CNN architectures (Conv1D) to extract temporal features from the time series. Models tested include CNN0 (single Conv1D layer) and CNN1 (Conv1D + one dense layer).
- **Long Short-Term Memory (LSTM)**: Recurrent neural network designed to handle long-term dependencies and mitigate vanishing gradients. Variants tested include LSTM_0HL_F, LSTM_0HL_T, LSTM_1HL_F, and LSTM_1HL_T, differing in return sequence setting and presence of additional dense layers.
- **Gated Recurrent Unit (GRU)**: Simplified variant of LSTM with fewer gates. Models include GRU_0HL_F, GRU_0HL_T, GRU_1HL_F, and GRU_1HL_T, varying in return sequence and hidden layer configuration.
- **XGBoost**: Gradient boosting decision tree ensemble algorithm. Used for regression with hyperparameters tuned via Optuna.

All models are trained to predict the next day’s index value based on the prior day’s value (1-day rolling window). The paper explicitly excludes stationarity tests and multivariate analysis, which are reserved for future work.

### 2. Code Availability

The paper states that code was implemented in **Google Colab** using **TensorFlow/Keras** for neural networks and **XGBoost** and **Optuna** libraries for hyperparameter tuning. However, **no public GitHub repository or code release is mentioned**. The authors acknowledge code review by Cem Öztürk, but no public access point is provided. Therefore:

**Code Availability: Not available**

### 3. Learning Type

All models are trained using **supervised learning**. The task is regression: predicting a continuous value (next day’s index) based on a single input feature (previous day’s index). The dataset is split chronologically into train (70%), validation (20%), and test (10%) sets, and no self-supervised or unsupervised techniques are employed.

### 4. Dataset

The dataset used is the **S&P Green Bond Index (Total Performance, USD)**, covering the period from **31 January 2013 to 17 February 2023**, inclusive. The data is sourced directly from the S&P website and consists of **2,615 daily observations** (workdays only). No adjustments, normalization, or transformations (e.g., log scaling) were applied.

The dataset is split chronologically:
- **Train set**: 31 Jan 2013 to 16 Feb 2020 (1,830 entries)
- **Validation set**: 17 Feb 2020 to 16 Feb 2022 (523 entries)
- **Test set**: 17 Feb 2022 to 17 Feb 2023 (262 entries)

Key statistics:
- Index range: 109.80 to 158.99
- Train set mean: 133.60, std: 5.32
- Validation set mean: 150.46, std: 5.70
- Test set mean: 123.96, std: 8.04 (more volatile)

The test set exhibits higher volatility than training/validation sets, posing a challenge for model generalization.

### 5. Implementation Details

- **Programming language(s):** Python
- **Key libraries/frameworks:**
  - **TensorFlow/Keras** for building and training DNN, CNN, LSTM, and GRU models
  - **XGBoost** for gradient boosting model
  - **Optuna** for hyperparameter optimization
  - **Pandas** for data handling
  - **NumPy** for numerical operations
  - **Matplotlib** for visualization
  - **Scikit-learn** (implied for data splitting, though not explicitly named)

All models were trained on **Google Colab**.

### 6. Model Architecture

#### Baseline Model
Trivial: `y_t = y_{t-1}`. No trainable parameters. Used as benchmark.

#### Deep Neural Network (DNN)
- Feedforward architecture with fully connected (dense) layers.
- Input: 1 feature (prior day’s index)
- Output: 1 value (predicted index)
- Hidden layers: 1, 2, or 3 dense layers with varying neuron counts (optimized via Optuna).
- Activation functions: relu, elu, gelu, swish, linear, sigmoid, tanh (tested via hyperparameter tuning).
- No dropout or batch normalization applied.

#### Convolutional Neural Network (CNN)
- **Conv1D layer** applied to 1D time series data.
- Kernel size: hyperparameter tuned (typically 1–3 for 1-day input, but potentially larger in future work).
- Filter size: number of filters (features) learned, tuned via Optuna.
- No pooling layer used in this initial analysis.
- Optional dense layer added after convolution (CNN1 variant).
- Input shape: (1, 1) — single time step, single feature.

#### Long Short-Term Memory (LSTM)
- Single LSTM layer with return_sequences set to True or False.
- Optional dense layer after LSTM (LSTM_1HL variants).
- Cell state, forget gate, input gate, and output gate implemented as per standard LSTM equations.
- Hyperparameters tuned: number of units, activation function, learning rate, L2 regularization, return_sequences.
- Input shape: (1, 1) — batch size, timesteps, features.

#### Gated Recurrent Unit (GRU)
- Single GRU layer with return_sequences True/False.
- Optional dense layer (GRU_1HL variants).
- Simpler than LSTM: reset gate and update gate only.
- Hyperparameters tuned similarly to LSTM.
- Input shape: (1, 1).

#### XGBoost
- Ensemble of regression trees.
- Hyperparameters tuned: eta (learning rate), gamma (pruning), max_depth, reg_lambda (L2 regularization).
- No feature engineering; input is single lagged value.
- Tree pruning and handling of missing values enabled.

### 7. Technical Content

This paper presents the foundational stage of a time series analysis project focused on predicting the S&P Green Bond Index using machine learning models. The goal is to establish whether non-traditional methods like neural networks can outperform a simple baseline (yesterday’s value = today’s value) in forecasting green bond index movements. The analysis is intentionally simplified: univariate (only prior day’s value used), no stationarity adjustments, and no multivariate inputs (e.g., stock market, oil prices) are considered — these are deferred to future work.

#### Methodology

The core methodology involves a **rolling 1-day window**: for each day `t`, the model uses the value at `t-1` to predict the value at `t`. This is repeated across the entire time series. The dataset is split chronologically into train (70%), validation (20%), and test (10%) sets to preserve temporal order and avoid data leakage. This is critical for time series, as random splitting would invalidate the temporal dependency assumption.

Models are trained to minimize **Mean Absolute Error (MAE)** using the **Adam optimizer**. **L2 regularization** is applied to all neural network models to mitigate overfitting. **Optuna** with **Tree-structure Parzen Estimator (TPE)** Bayesian optimization is used to tune hyperparameters across all models. For XGBoost, hyperparameters include learning rate (eta), max depth, gamma (pruning), and L2 regularization (reg_lambda).

#### Model Training and Optimization

The Adam optimizer is chosen for its efficiency in adapting learning rates per parameter and combining momentum with RMSProp. The update rule involves bias-corrected first and second moment estimates of gradients. L2 regularization adds a penalty term proportional to the square of weights to the loss function, encouraging smaller weights and reducing model complexity.

Optuna’s TPE algorithm constructs probabilistic models of the objective function (MAE) based on past evaluations. It uses two Gaussian mixtures — one for good trials (l(x)) and one for poor trials (g(x)) — to guide the search toward promising regions of the hyperparameter space. The Expected Improvement (EI) acquisition function balances exploration and exploitation.

Activation functions tested include ReLU, ELU, GELU, Swish, Linear, Sigmoid, and Tanh. ReLU is likely the default, but Optuna explores which performs best for each model type.

#### Results

Results are evaluated on the test set (Feb 2022–Feb 2023) using **MAE** and **MAPE**. The Baseline model achieves MAE = 0.610 and MAPE = 0.4969%. The best-performing models per category are:

- **DNN_best**: MAE = 0.607 (−0.003 vs Baseline), MAPE = 0.4947% (−0.0022%)
- **LSTM_best**: MAE = 0.609 (−0.001), MAPE = 0.4966% (−0.0003%)
- **CNN_best**: MAE = 0.611 (+0.001), MAPE = 0.4977% (+0.0008%)
- **GRU_best**: MAE = 0.615 (+0.005), MAPE = 0.5011% (+0.0042%)

**XGBoost** performs poorly: MAE = 2.840 (+2.23), MAPE = 2.4445% (+1.9476%). This is attributed to decision trees’ inability to extrapolate beyond the range of training data — the test set includes values below the training minimum (~122), which XGBoost cannot predict.

Graphically, the Baseline model fits the test data well, reflecting low daily volatility in the index. The DNN and LSTM models show marginal improvements, but differences are within ±0.1% MAPE — statistically and practically insignificant. When excluding 2022 data, the ranking changes (DNN, CNN, GRU beat Baseline; LSTM does not), further indicating no conclusive outperformance.

#### Conclusions

The key conclusion is that **no model materially outperforms the Baseline**. The small improvements by DNN and LSTM are not statistically significant and likely due to noise or hyperparameter tuning luck. The authors attribute this to the simplicity of the task: predicting one day ahead from one prior day’s value provides insufficient information for complex models to learn meaningful patterns. The test set’s higher volatility also challenges generalization.

XGBoost’s failure highlights a key limitation of tree-based models for time series: they cannot extrapolate beyond training data ranges. This is a known issue and not a flaw of the implementation.

#### Limitations and Future Work

The authors explicitly state this is a foundational study. Key limitations include:
- No stationarity treatment (e.g., differencing, ARIMA)
- No multivariate inputs (e.g., market indices, oil prices)
- No multi-step forecasting (predicting next week/month)
- No data preprocessing (normalization, log transforms)
- Small input window (only 1 day)

Future work will address these:
1. **Stationarity**: Introduce ARIMA or SARIMAX models.
2. **Multivariate analysis**: Incorporate stock market, oil prices, etc.
3. **Wider GSS universe**: Extend to Bloomberg MSCI, FTSE indices.
4. **Advanced models**: Test CEEMDAN-LSTM (hybrid decomposition + LSTM) and N-BEATS (interpretable time series architecture).
5. **Larger windows/horizons**: Use 5, 10, or 30 prior days to predict 1, 5, or 10 days ahead.
6. **Preprocessing**: Apply normalization or log transforms.

The authors emphasize that while results are inconclusive here, neural networks show potential for more complex time series problems, and this paper lays the groundwork for future, more sophisticated analyses.

#### Broader Implications

This work contributes to the growing literature on applying ML to sustainable finance. Green bonds are a rapidly growing asset class (>$2T global issuance in 2022), and accurate forecasting models could aid portfolio management, risk assessment, and actuarial modeling. The paper’s transparent methodology and clear limitations provide a replicable framework for future studies. The focus on open-source tools (TensorFlow, Optuna, XGBoost) also promotes accessibility.

In summary, this paper is a methodological foundation. It demonstrates that simple models can be competitive with complex ones when data is limited, and it sets the stage for more ambitious analyses that incorporate stationarity, multivariate inputs, and advanced architectures — where neural networks may truly shine.

---

**Word Count:** ~1520 words


================================================
FILE: data/summaries_235B_samples/actuaries-using-data-science-and-artificial-intelligenc_summary_20260125_143906.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** Actuaries using data science and artificial intelligence techniques  
**Authors:** Alan Marshall  
**Date of publication:** 2024-02  
**Topic:** Application of data science and AI in actuarial practice  
**Sub-topic:** Ethical, regulatory, and technical adoption of machine learning and AI in actuarial domains  
**URL:** Not available  

---

## Summary

### 1. Modeling Techniques

The report does not focus on a single modeling technique but instead surveys a wide range of AI and data science methodologies currently in use or under exploration by actuaries. Techniques mentioned include:

- **Generalized Linear Models (GLM)**: Still widely used in General Insurance (GI) pricing, though increasingly supplemented or replaced by more advanced methods.
- **Gradient Boosting Machines (GBM)**: Gaining popularity in GI pricing for their predictive power and ability to handle non-linear relationships.
- **Random Forests**: Used in multiple case studies, including modeling borrower earnings progression and predicting underwriting decisions. Ensemble learning approaches are emphasized for robustness.
- **Markov Models**: Applied in loan portfolio analysis to predict earnings transitions.
- **Large Language Models (LLM)**: Used to automatically build actuarial models from unstructured product specification documents (PDFs, Word, web text) and to assist in building capital models from scratch.
- **Deep Learning-Based Semantic Search**: Applied to process large volumes of unstructured text (e.g., reports) by understanding context and meaning rather than relying on keyword matching.
- **Explainable AI (XAI)**: Used in lapse modeling to interpret model outputs and gain insights into drivers of policyholder behavior.
- **Reinforcement Learning**: Mentioned in the context of dynamic hedging for variable annuities (in conference topics).
- **Neural Networks with Constraints**: Smoothness and monotonicity constraints applied to neural networks in pricing contexts.

The report highlights that while traditional actuarial models (e.g., GLMs) remain relevant, there is a clear trend toward adopting more complex, data-driven techniques—particularly ensemble methods and machine learning—to improve predictive accuracy and handle larger, more heterogeneous datasets.

### 2. Code Availability

**Not available.** The report does not mention any publicly available code repositories, GitHub links, or open-source implementations associated with the case studies or methodologies described. The focus is on high-level applications, ethical considerations, and regulatory implications rather than technical reproducibility.

### 3. Learning Type

The report does not specify a single learning paradigm across all case studies. However, based on the techniques described:

- **Supervised Learning** is predominant: Most applications (e.g., underwriting prediction, earnings modeling, lapse rate prediction) involve labeled data and target variable prediction.
- **Unsupervised Learning** is implied in semantic search and clustering applications for text analysis.
- **Self-supervised Learning** is not explicitly mentioned, though LLMs used for document parsing and model generation likely rely on pre-trained self-supervised architectures.
- **Reinforcement Learning** is mentioned in conference topics (dynamic hedging), indicating exploratory use in actuarial contexts.

Overall, the learning type is **predominantly supervised**, with emerging interest in unsupervised and reinforcement methods.

### 4. Dataset

The report references multiple datasets but does not provide detailed metadata (e.g., size, structure, public availability). Key datasets include:

- **General Insurance Claims Triangles**: Paid claims, provisions, and reported-but-not-settled reserves across multiple firms and lines of business.
- **Pension Scheme Experience Data**: Includes active, pensioner, and dependent member data (date of birth, gender, movement dates, pension/salary amounts) for experience analysis.
- **Loan Portfolio Data**: Borrower demographics (date of birth, gender, loan inception year, field of work) and historical earnings.
- **Life Insurance Application Data**: Customer, policy, and high-level medical/family history disclosures for underwriting prediction.
- **Unstructured Text Data**: Reports, product specifications (PDF, Word, web text) processed via LLMs.
- **Weather and Agricultural Data**: Satellite and farmer data for weather-indexed crop insurance in Africa.

All datasets appear to be **real-world, proprietary data** collected by institutions (insurers, pension funds, lenders). No synthetic datasets are mentioned. Data sources are not publicly available.

### 5. Implementation Details

- **Programming language(s):**  
  - **Python**: Used for dashboarding and model deployment (e.g., in pension experience analysis).
  - **R**: Used for statistical modeling and data mining (e.g., random forests for earnings modeling).
  - **Not specified**: For LLM-based model generation and semantic search systems, though likely Python-based given industry norms.

- **Key libraries/frameworks:**  
  - **Scikit-learn**: Implied for random forests and other ML models.
  - **XGBoost/LightGBM**: Likely used for gradient boosting in GI pricing.
  - **Hugging Face Transformers / LangChain**: Implied for LLM applications (though not named).
  - **Pandas/NumPy**: For data preprocessing (implied in Python dashboarding).
  - **Shiny (R)** or **Plotly Dash (Python)**: For dashboarding (implied in pension analysis).
  - **Not specified**: For Markov models or deep learning semantic search.

The report does not provide explicit library names for most case studies, but the tools used are consistent with industry-standard data science stacks.

### 6. Model Architecture

The report describes several composite and non-vanilla architectures:

- **Ensemble Learning (Random Forest)**: Used in underwriting prediction. Multiple decision trees are trained on bootstrapped samples and combined via voting or averaging. Feature importance is extracted to understand drivers.
- **LLM-Powered Model Builder**: Proprietary software uses third-party AI libraries (e.g., NLP models) to parse unstructured documents and generate actuarial models. Architecture likely involves:
  - Document ingestion → Text embedding → Intent/structure extraction → Model template generation → Customization interface.
- **Semantic Search System**: Deep learning-based, likely using transformer architectures (e.g., BERT variants) for contextual understanding. Architecture:
  - Query embedding → Document embedding → Vector similarity search → Relevance ranking.
- **Markov Transition Model**: Discrete-state, discrete-time model for predicting borrower earnings progression. States defined by earnings bands; transitions estimated from historical data.
- **Explainable AI (XAI) for Lapse Modeling**: Likely uses SHAP or LIME to explain predictions from black-box models (e.g., GBM or neural networks).

No deep neural network architectures (e.g., CNNs, RNNs) are described in detail, though “neural networks with constraints” are mentioned in conference topics.

### 7. Technical Content

The report provides a comprehensive overview of how actuaries are integrating data science and AI into traditional and emerging domains. It is structured around findings derived from case studies, surveys, and regulatory analysis, with a strong emphasis on ethical, governance, and educational implications.

#### Context and Motivation

Actuaries have historically been “data scientists” by nature—analyzing large datasets to project future risks (e.g., mortality, asset returns). However, the rapid evolution of AI tools (especially LLMs since late 2022) has intensified the need for actuaries to adapt. The IFoA launched this thematic review to assess current adoption, identify risks, and inform future guidance. The review collected submissions from organizations (e.g., Royal London, Prudential Regulation Authority) and individuals, supplemented by interviews and regulatory research.

#### Key Technical Applications

**1. General Insurance Pricing**  
Traditionally reliant on GLMs, GI pricing now incorporates Gradient Boosting Machines (GBMs) and other ensemble methods. These models handle non-linearities and interactions better than GLMs, improving predictive accuracy. The shift reflects a broader industry trend toward “black-box” models, raising concerns about explainability and governance.

**2. Claims Analysis**  
Machine learning is used to analyze claims triangles across firms. Off-the-shelf software applies pattern recognition to identify trends in paid claims, provisions, and reserves. This enables faster, consistent diagnostics across lines of business. The approach is still experimental but shows promise for automating actuarial reserving.

**3. Pension Experience Analysis**  
A Python dashboard processes cleaned pension data (member movements, retirements, mortality) to generate visualizations and summaries. The system is being migrated to R for speed and flexibility. Future enhancements include modeling workforce dynamics (e.g., propensity to leave service, lump sum choices).

**4. Actuarial Model Generation from Documents**  
Generative AI is used to build actuarial models from unstructured inputs (PDFs, Word docs, websites). Third-party NLP libraries extract key terms and structure, which proprietary software converts into executable models. This reduces manual model-building time and allows rapid customization.

**5. Earnings Modeling for Loan Portfolios**  
A Markov model predicts borrower earnings progression. A sub-project uses random forests to validate and enhance this model. Feature importance analysis identifies key predictors (e.g., field of work, historical earnings). The ML model confirms the Markov model’s robustness and suggests improvements.

**6. Life Underwriting**  
75% of applications are auto-decided via rules; the remaining 25% are scored by ensemble ML models (random forests). Multiple models are combined to optimize risk loading and decision profiles. The system has been in production for five years, demonstrating stability and business value.

**7. Semantic Search for Unstructured Text**  
A deep learning system retrieves information from reports by understanding natural language queries. Unlike keyword-based search, it captures context and meaning, handling ambiguity and variability. The system is in user acceptance testing, indicating early-stage deployment.

**8. Lapse Rate Modeling**  
Machine learning (e.g., GBM) is applied to model policyholder lapses, a critical assumption for pricing and reserving. XAI techniques (e.g., SHAP) explain model outputs, revealing drivers like policy age or premium level. Comparisons to traditional models show trade-offs between accuracy and interpretability.

**9. Capital Modeling in Open-Source Frameworks**  
An actuary with no coding experience built a capital model using an LLM. The LLM generated code and explanations, which were then packaged into a user-friendly dashboard. This experiment highlights LLMs’ potential to democratize complex modeling—but also raises questions about competence and oversight.

**10. AI for Insurance in Africa**  
Weather-indexed crop insurance for small-scale farmers uses AI/ML on satellite and farmer data. The goal is to reduce administrative costs and increase uptake, enhancing food security. This application extends actuarial work into development economics and social impact.

#### Ethical and Governance Challenges

Ethics and fairness are central themes. Key concerns include:

- **Bias and Discrimination**: Models may perpetuate or amplify biases in training data (e.g., in underwriting or pricing). Fairness checks are conducted at development and pre-implementation stages.
- **Explainability**: AI models (especially LLMs and deep learning) are often “black boxes.” Actuaries must balance predictive power with transparency to meet regulatory and ethical standards.
- **Data Privacy**: GDPR compliance is critical. Solutions include using reputable AI providers or private LLM instances.
- **Professional Competence**: Actuaries must ensure they have the skills to build, validate, or review AI models. The Actuaries’ Code (Principle 2) mandates competence; Principle 6 requires accurate, non-misleading communication.
- **Governance**: Firms are adapting existing model risk management frameworks for AI/ML. The FRC’s TAS 100 and Model Guidance explicitly cover emerging AI models.

#### Regulatory Landscape

Global regulators are actively developing AI frameworks. Common themes include:

- **Safety and Robustness**: Ensuring models operate reliably and mitigate harm.
- **Fairness**: Avoiding discrimination and benefiting society.
- **Transparency**: Explaining model decisions.
- **Accountability**: Clear oversight and challenge processes.

Key jurisdictions:

- **UK**: Pro-innovation approach; discussion papers and white papers underway.
- **EU**: AI Act (adopted Dec 2023) bans social scoring and mandates explanations.
- **China**: Has specific regulations (e.g., for generative AI).
- **Singapore**: MAS toolkit for responsible AI; planned generative AI risk framework.
- **US**: Executive order on AI; bipartisan framework under development.
- **Global**: OECD principles, Bletchley Declaration, NCSC/CISA guidelines.

The IFoA’s position is to adapt existing standards (e.g., Actuaries’ Code, TASs) rather than create AI-specific rules, given the fast pace of change. A 2023 Risk Alert reminds members of key risks (bias, lack of explainability, data security).

#### Educational and Professional Development

The IFoA offers:

- **Curriculum**: Data science covered in Actuarial Statistics (large datasets, ML), Specialist Principles (domain applications), and Specialist Advanced (ethics/regulation).
- **Lifelong Learning**: Certificate in Data Science (with University of Southampton); webinars, conferences, and community-led workstreams.
- **Future Plans**: Enhance curriculum and CPD to keep pace with AI developments. Risk of obsolescence if education lags.

#### Collaboration and Future Directions

Actuaries increasingly work alongside data scientists, with employers prioritizing skills over qualifications. This poses a challenge to the profession’s relevance. Opportunities exist to collaborate with:

- Other actuarial associations (e.g., SOA, Actuaries Institute Australia).
- Institutes (Alan Turing Institute, RSS).
- Regulators (FRC, PRA, MAS).

The IFoA encourages this collaboration to shape responsible AI use globally.

#### Conclusions

The report concludes that:

1. AI/ML adoption is accelerating across actuarial domains (beyond GI pricing), creating opportunities but also risks (bias, lack of transparency, skill gaps).
2. Global regulation is evolving rapidly, with a focus on responsible, ethical AI. Actuaries must navigate this landscape using existing standards while advocating for proportionate, relevant guidance.
3. Education and collaboration are critical to maintaining the profession’s relevance. The IFoA must continue developing resources and fostering partnerships.

In essence, actuaries are well-positioned to lead in responsible AI adoption—but only if they embrace continuous learning, ethical rigor, and cross-disciplinary collaboration.

--- 

**Word count**: ~1520 words


================================================
FILE: data/summaries_235B_samples/actuary-gpt-applications-of-large-language-models-to-in_summary_20260125_143906.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** ACTUARYGPT: APPLICATIONS OF LARGE LANGUAGE MODELS TO INSURANCE AND ACTUARIAL WORK  
**Authors:** Caesar Balona  
**Date of publication:** 2023-08-17  
**Topic:** Artificial Intelligence in Actuarial Science  
**Sub-topic:** Large Language Models for Insurance Claims, Underwriting, Compliance, and Workflow Automation  
**URL:** https://ssrn.com/abstract=4543652

---

## Summary

### 1. Modeling Techniques

The document explores the application of **Large Language Models (LLMs)** — specifically **GPT-3.5 and GPT-4** from OpenAI — across multiple domains in actuarial and insurance work. These models are not trained from scratch for specific actuarial tasks but are leveraged via **prompt engineering**, **few-shot learning**, and **API-based integration** to perform tasks such as:

- **Text summarization** (e.g., summarizing claims descriptions, medical reports, police reports)
- **Sentiment and emotion analysis** (e.g., detecting policyholder anger or distress)
- **Named Entity Recognition (NER)** (e.g., extracting dates, locations, parties from free-text claims)
- **Information extraction and structuring** (e.g., parsing reinsurance treaties into JSON, extracting inconsistencies in claims)
- **Regulatory compliance assistance** (via vector databases for context-aware querying)
- **Emerging risk identification** (e.g., summarizing news articles on cyber risks)
- **Code generation and debugging** (e.g., assisting actuaries with IBNR reserving code in Python)
- **Scenario analysis and stress testing** (e.g., generating economic impact assessments for geopolitical events)

The paper does not describe training custom neural architectures but focuses on **zero-shot and few-shot prompting** of pre-trained LLMs to perform domain-specific tasks. The LLMs are treated as **black-box assistants** or **automation engines** rather than models trained on actuarial datasets.

### 2. Code Availability

**Yes**, implementation code is available. The paper references a **GitHub repository** containing Python scripts for all case studies:

- **Repository URL:** https://github.com/cbalona/actuarygpt-code
- The repository includes code for:
  - Parsing claims descriptions (Case Study 1)
  - Automating reinsurance treaty extraction (Case Study 4)
  - Regulatory knowledgebase construction (Case Study 3 — partial code referenced)
  - Example scripts for news scraping and summarization (Case Study 2)

The code uses the **OpenAI API** to interact with GPT-3.5 and GPT-4, and includes utilities for PDF text extraction (PyPDF2), JSON schema enforcement, and vector database integration (Pinecone).

### 3. Learning Type

The approach primarily uses **zero-shot and few-shot learning** — meaning the LLMs are **not fine-tuned** on actuarial datasets. Instead, they are prompted with instructions, examples, or schemas to perform tasks on-demand.

- **Zero-shot**: The model is given a task without prior examples (e.g., “Summarize this claim”).
- **Few-shot**: The model is shown 2–3 examples of desired input-output pairs before being asked to generalize (e.g., demonstrating how to format economic impact scenarios before asking for a new one).

This is **not supervised learning** in the traditional sense (no labeled training data is used to update model weights). It is **prompt-based inference** using pre-trained models.

### 4. Dataset

The datasets used are **synthetic or illustrative**, generated for demonstration purposes:

- **Claims data**: Fictional JSON structures containing claim IDs, policyholder IDs, incident descriptions, communication logs (phone/email transcripts), medical reports, and police reports.
- **Reinsurance treaties**: Simplified, fabricated PDF documents used to demonstrate structured data extraction.
- **Regulatory documents**: Real documents from the **South African Solvency Assessment and Management (SAM)** framework (FSI and GOI standards), embedded into a vector database.
- **News articles**: Scraped results from Google searches on cyber risks (simulated for the case study).

No real-world, proprietary insurance datasets are used. The paper emphasizes that LLMs can handle **unstructured data** (free text, PDFs, emails) and convert them into **structured formats** (JSON, databases) — a key value proposition.

### 5. Implementation Details

- **Programming language(s):** Python
- **Key libraries/frameworks:**
  - `openai` (for API access to GPT-3.5/GPT-4)
  - `PyPDF2` (for extracting text from PDFs in Case Study 4)
  - `pandas` (for data manipulation)
  - `json` (for schema validation and output formatting)
  - `requests` (for web scraping in Case Study 2)
  - `pinecone` (for vector database in Case Study 3 — referenced but not fully implemented in repo)
  - `chainladder` (used in Appendix C for IBNR reserving example)

The paper also references **GitHub Copilot** (a code-specific LLM) and **ChatGPT plugins** (like Code Interpreter) as auxiliary tools for data cleaning and code generation.

### 6. Model Architecture

The paper does not describe custom model architectures. Instead, it leverages **off-the-shelf LLMs**:

- **GPT-3.5 and GPT-4** (OpenAI): These are **decoder-only transformer models** with billions of parameters, trained on massive web corpora. They are accessed via API and used as black boxes.
- **No fine-tuning** is performed. The models are used in their **base, pre-trained form**.
- **Composite systems** are built around the LLMs:
  - **Vector database + LLM** (Case Study 3): Pinecone stores embeddings of regulatory documents; queries are matched to relevant context before being passed to GPT-4.
  - **Prompt + Schema + LLM** (Case Study 4): A JSON schema is provided to GPT-3.5 to enforce structured output.
  - **Few-shot prompt + LLM** (Appendix C.3): Examples are provided to guide output format.

The architecture is **modular**: LLMs are embedded in workflows (e.g., claims processing pipeline, regulatory compliance tool) rather than being end-to-end models.

### 7. Technical Content

The paper presents a comprehensive exploration of how Large Language Models (LLMs) — particularly OpenAI’s GPT-3.5 and GPT-4 — can be integrated into actuarial and insurance workflows. The core thesis is that LLMs are not replacements for actuaries but powerful **assistants and automation tools** that can handle unstructured data, reduce manual labor, and enhance decision-making — provided they are used responsibly and with appropriate safeguards.

#### Direct Applications in Claims Processing

The paper details a **7-step claims management process** (Reporting → Adjusting → Investigating → Negotiating → Agreements → Payments → Compliance) and maps LLM applications to each stage:

- **Reporting**: LLMs extract structured data (date, location, incident type) from free-text inputs (emails, call transcripts). They also perform **sentiment analysis** to detect anger or distress, which may indicate fraud or need for customer support. **Entity recognition** identifies key people, places, and events. The output is stored in a **NoSQL database** (e.g., MongoDB) for flexibility in handling unstructured data.
  
- **Adjusting**: LLMs summarize information from multiple sources (call logs, emails, reports) into a structured report for adjusters. They can also **classify claims by severity** (low/medium/high) based on descriptions (e.g., “bumper bashing” vs. “high-impact collision”) to prioritize investigations.

- **Investigating**: LLMs identify **inconsistencies** across documents (e.g., police report vs. claimant statement) to flag potential fraud. They can also be given policy terms to assess whether claims violate conditions (e.g., speeding in motor claims).

- **Negotiating, Agreements, Payments**: LLMs are less directly involved here, but insights from earlier stages (e.g., sentiment, severity, fraud risk) inform negotiations and automate payment triggers.

- **Compliance**: LLMs ingest regulatory documents (via vector databases) and answer queries about compliance (e.g., “Does this claim comply with GOI Section 5.2?”). This automates a traditionally manual, time-consuming task.

The paper emphasizes that LLMs **add value by automating labor-intensive tasks** (e.g., data entry, document summarization) and **providing new insights** (e.g., detecting fraud patterns, identifying emotional cues) that would be too costly to gather manually.

#### High-Level Applications in Other Insurance Functions

Beyond claims, the paper explores LLM applications in:

- **Emerging Risk Identification**: LLMs analyze scraped news articles to identify trends (e.g., rising cyber risks, climate-related litigation) and generate **actionable summaries and project plans**. For example, after summarizing cyber risks, the LLM generates “Action Points” and then creates detailed “Project Plans” for each point — demonstrating **recursive prompting**.

- **Commercial Underwriting**: LLMs extract key information from technical reports (e.g., engineering assessments) and output it in structured formats (e.g., JSON) for underwriting systems. This streamlines the review of complex documents.

- **Compliance**: LLMs can answer queries against internal policy documents or regulatory texts. The paper demonstrates this using a **vector database** (Pinecone) to overcome the LLM’s context length limitation. For example, when asked “How should a real estate investment be stressed on the balance sheet?”, the LLM returns a generic answer without context, but with the SAM regulatory knowledgebase, it returns a precise, regulation-specific answer.

#### Decision Framework for LLM Suitability

The paper introduces a **two-part decision framework** to evaluate whether an LLM is suitable for a given task:

1. **Technical Assessment Tree**:
   - Is the data structured/numeric? → If yes, traditional methods may suffice.
   - Does the task require generating new content or complex pattern recognition? → If yes, LLMs may be suitable.
   - Do the benefits (accuracy, efficiency) outweigh costs (API fees, maintenance)?
   - Do you have the resources (data, expertise, compute)?

2. **Risk Assessment Tree**:
   - Could bias negatively impact the task?
   - Are there ethical concerns (e.g., using social media data)?
   - Is interpretability required? (Critical for actuaries who must attest to results.)
   - Is there a risk of data leakage or memorization?
   - Is output variability acceptable?
   - Are adversarial conditions likely?
   - Are the consequences of errors acceptable?

The framework is applied to a real-world example: **extracting structured data from reinsurance treaties**. The technical assessment concludes LLMs are suitable (complex pattern recognition needed, benefits outweigh costs). The risk assessment identifies minimal risk (no content generation, low variability, data leakage mitigated via ring-fenced models).

#### Assistance Applications: LLMs as Actuarial Assistants

The paper dedicates a section to **indirect, assistant-style applications** of LLMs:

- **Coding Assistant**: LLMs help write, debug, and optimize code (e.g., Python for IBNR reserving). The paper includes a transcript where ChatGPT explains Python concepts (e.g., “dot notation”, “random seeds”) and generates working code using the `chainladder` library.

- **Problem Solving**: LLMs help actuaries articulate problems, explore solutions, and refine ideas. For example, when asked to price a policy for LLM-generated misinformation, ChatGPT advises on risk quantification, scenario analysis, and expert judgment — demonstrating **collaborative problem-solving**.

- **Drafting Reports and Summarization**: LLMs can generate executive summaries, translate technical findings into layman’s terms, and even create visualizations (charts, infographics) to accompany reports. The key is providing **detailed prompts** (e.g., “Summarize this report in 3 bullet points, tone: formal, focus on financial impact”).

- **Education**: LLMs serve as tutors, explaining concepts (e.g., hyperparameter tuning), providing analogies, and simulating business scenarios for training.

- **Data Cleaning and Preparation**: Using ChatGPT’s Code Interpreter plugin, users can upload datasets and ask the LLM to clean, analyze, or visualize data — accelerating the EDA phase.

- **Model Development and Interpretation**: LLMs help explain model assumptions, suggest hyperparameter search spaces, and translate complex model outputs for non-technical stakeholders.

#### Case Studies

The paper includes four detailed case studies with code:

1. **Parsing Claims Descriptions (Case Study 1)**: GPT-4 analyzes a claim (including call logs, medical, and police reports) to extract claim ID, policy ID, inconsistencies, claimant emotion, and summary. Output is in JSON for system integration. Example: Detects that a claimant’s description of “horrifying accident” contradicts a police report stating “no serious injuries”.

2. **Identifying Emerging Risks (Case Study 2)**: GPT-4 summarizes scraped news on cyber risks, generates “Action Points”, and then creates “Project Plans” for each action — demonstrating **multi-step prompting**.

3. **Regulatory Knowledgebase (Case Study 3)**: Uses Pinecone to store embeddings of SAM regulatory documents. When asked “How should a real estate investment be stressed?”, the LLM returns a regulation-specific answer, unlike the generic response without context.

4. **Parsing Reinsurance Documents (Case Study 4)**: GPT-3.5 extracts structured data (treaty type, reinsurer, loss layers, exclusions) from a PDF reinsurance treaty and outputs it in JSON. The schema is provided to ensure consistent formatting.

#### Impact on the Actuarial Profession

The paper argues that LLMs will **augment, not replace**, actuaries. They free actuaries from manual, repetitive tasks (e.g., data extraction) so they can focus on **high-value activities** requiring judgment, creativity, and strategic thinking. However, actuaries must:

- Understand LLM limitations (bias, hallucination, context limits).
- Manage risks (data privacy, ethical use, output validation).
- Collaborate with data scientists, ethicists, and IT professionals.
- Adapt to new workflows and continuously learn.

The paper also highlights **risks**: LLMs can produce incorrect or biased outputs, hallucinate facts, or leak sensitive data. Actuaries must **review and validate** all LLM outputs, especially for high-stakes decisions.

#### Conclusion

LLMs offer significant potential to **enhance efficiency, reduce errors, and unlock insights** in actuarial work — particularly in handling unstructured data and automating routine tasks. The paper provides a practical framework for evaluating LLM suitability and demonstrates real-world applications via case studies. However, it stresses that LLMs are tools, not oracles — their outputs require **human oversight, ethical consideration, and professional accountability**. The future of actuarial science lies in **human-AI collaboration**, where actuaries leverage LLMs to amplify their expertise, not outsource it.

---

**Word Count**: ~1500 words (excluding metadata and headers)


================================================
FILE: data/summaries_235B_samples/AI,_data_science_and_emerging_technologies_summary_20260125_143906.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** AI, Data Science and Emerging Technologies Practice Board  
**Authors:** Not specified  
**Date of publication:** Not specified (document references formation in November 2025)  
**Topic:** Actuarial Science and Emerging Technologies  
**Sub-topic:** Integration of AI and Data Science into Actuarial Practice  
**URL:** Not available  

---

## Summary

### 1. Modeling Techniques

The document does not describe any specific machine learning or artificial intelligence modeling techniques. It is an organizational and strategic overview of a practice board formed by the Institute and Faculty of Actuaries (IFoA) to explore the impact of AI, data science, and emerging technologies on actuarial practice. No technical models (e.g., neural networks, decision trees, clustering algorithms, etc.) are implemented, evaluated, or referenced. The focus is on governance, research direction, and professional development rather than algorithmic or computational modeling.

### 2. Code Availability

Implementation code is not available. The document does not reference any software, code repositories, or computational experiments. It is a descriptive and organizational document, not a technical research paper or software development project. No GitHub or other code-sharing platforms are mentioned.

### 3. Learning Type

The document does not involve any machine learning paradigm (supervised, unsupervised, or self-supervised). It is not a machine learning study. Instead, it outlines a professional initiative focused on education, research, and ethical engagement with emerging technologies. The term “learning” here refers to lifelong professional development, not algorithmic learning.

### 4. Dataset

No datasets are referenced. The document does not describe any data collection, preprocessing, or analysis activities. It does not mention real-world or synthetic datasets, nor does it reference any data sources used for research or modeling. The emphasis is on organizational structure and strategic goals rather than empirical data analysis.

### 5. Implementation Details

- **Programming language(s):** Not applicable. No code implementation is described.
- **Key libraries/frameworks:** Not applicable. No software libraries, frameworks (e.g., TensorFlow, scikit-learn), or computational tools are mentioned.

### 6. Model Architecture

No model architecture is presented. The document does not describe any computational models, neural network structures, ensemble methods, or algorithmic pipelines. It is not a technical modeling paper but rather a governance and strategy document for a professional body.

### 7. Technical Content

The document introduces the **AI, Data Science and Emerging Technologies Practice Board** of the Institute and Faculty of Actuaries (IFoA), established in November 2025. This board is a member-led initiative created in collaboration between the IFoA Council and the IFoA AI and Data Science Community. Its primary mission is to guide the actuarial profession through the transformative impact of artificial intelligence, data science, and emerging technologies on both professional practice and broader societal systems.

#### Historical Context and Evolution

The practice area traces its origins to a working party formed in 2018, which succeeded the earlier **Modelling, Analytics and Insights from Data Working Party**. This working party evolved into the **AI and Data Science Community**, which eventually led to the formal establishment of the Practice Board. This evolution reflects the growing importance and institutional recognition of data-driven methodologies and AI in actuarial science.

The Practice Board is structured to serve as a central hub for members interested in AI, data science, and emerging technologies. Its vision is to become a **centre of excellence** — the primary interface between the IFoA and its members who wish to engage with these fields. The board aims to foster innovation, ensure ethical application of technologies, and maintain the relevance and leadership of actuaries in a rapidly digitizing world.

#### Core Pillars of the Practice Board

The board’s activities are organized around four foundational pillars:

1. **Lifelong Learning**  
   Led by Alexey Mashechkin (Lifelong Learning Lead) and Eilish Bouse (Deputy Lead), this pillar focuses on continuous professional development. It includes the creation of educational content, training modules, webinars, and digital learning platforms to help actuaries stay current with technological advancements. The goal is to ensure that members can adapt their skill sets to new tools and methodologies, such as machine learning, predictive analytics, and big data technologies.

2. **Research**  
   Headed by Alexis Iglauer (Research Lead) and Dylan Liew (Research Deputy Lead), this pillar drives the board’s scholarly output. It oversees active research working parties that produce technical papers, case studies, and white papers on topics at the intersection of actuarial science and emerging technologies. These working parties explore how AI can be applied to risk modeling, pricing, reserving, customer segmentation, and regulatory compliance. The research output is intended to inform both professional practice and public policy.

3. **Engagement**  
   Led by Malgorzata Smietanka (Collaboration and External Engagement Lead), this pillar promotes dialogue between the IFoA, industry stakeholders, academia, and regulatory bodies. It organizes events, conferences, and panel discussions to facilitate knowledge exchange. Engagement also includes outreach to non-actuarial audiences to explain the role of actuaries in the age of AI and to advocate for responsible technology adoption.

4. **Professionalism and Ethics**  
   Matthew Byrne serves as the Professionalism and Ethics Lead. This pillar addresses the ethical, legal, and societal implications of deploying AI and data science in actuarial contexts. It develops guidelines and frameworks for responsible AI use, including transparency, fairness, accountability, and bias mitigation. Given the high-stakes nature of actuarial decisions (e.g., insurance pricing, pension funding, solvency assessments), ethical considerations are paramount. The board works to ensure that actuaries uphold professional standards while leveraging new technologies.

#### Leadership Structure

The board is chaired by **Asif John**, with **John Ng** serving as Deputy Chair. The leadership team is composed of subject matter experts from diverse actuarial domains, ensuring that the board’s initiatives are grounded in practical industry needs as well as academic rigor. The inclusion of both leads and deputy leads across each pillar ensures continuity and collaborative governance.

#### Deliverables and Activities

The Practice Board delivers a range of content and services to its members:

- **Research Papers and Case Studies**: These documents explore real-world applications of AI and data science in actuarial contexts. Examples may include using machine learning to predict claim frequencies, applying natural language processing to policy documents, or using clustering algorithms to segment customer risk profiles.
  
- **Webinars and Events**: Regular virtual and in-person events feature guest speakers from industry, academia, and regulatory bodies. Topics may include “Explainable AI in Insurance,” “Ethics of Algorithmic Pricing,” or “Regulatory Challenges in AI-Driven Actuarial Models.”

- **Education and Training**: The board collaborates with the IFoA’s Virtual Learning Environment to develop courses on data science fundamentals, Python for actuaries, machine learning basics, and ethical AI. These resources are designed to be accessible to actuaries at all career stages.

- **Digital Community Platform**: A new digital platform enables member-to-member engagement, knowledge sharing, and open discussion. This fosters a collaborative ecosystem where actuaries can exchange ideas, ask questions, and collaborate on projects.

#### Strategic Importance

The formation of the Practice Board is positioned as a strategic response to the rapid pace of technological change. Actuaries, traditionally known for their expertise in risk assessment and financial modeling, are now expected to integrate data science and AI into their workflows. The board ensures that the profession remains relevant and competitive by:

- Equipping actuaries with modern analytical tools.
- Promoting innovation in actuarial methodologies.
- Advocating for ethical and responsible use of technology.
- Bridging the gap between actuarial science and data science communities.

#### Broader Societal Impact

Beyond professional development, the board recognizes that AI and data science have profound societal implications. Actuaries play a critical role in designing insurance products, pension schemes, and risk management systems that affect millions of people. The board seeks to ensure that these systems are fair, transparent, and equitable. For example, AI-driven pricing models must not perpetuate discrimination or bias, and predictive models must be explainable to regulators and consumers.

#### Future Directions

While the document does not outline specific technical roadmaps, it implies that future initiatives will likely include:

- Development of AI literacy programs for actuaries.
- Creation of benchmark datasets or synthetic data environments for testing actuarial AI models.
- Collaboration with regulatory bodies to shape AI governance frameworks.
- Publication of open-source tools or templates for common actuarial AI applications (e.g., claim prediction, reserve estimation).

#### Limitations and Gaps

The document is purely organizational and does not provide technical depth on any AI or data science methodologies. It does not discuss:

- Specific algorithms or model architectures.
- Performance metrics or evaluation frameworks.
- Computational infrastructure or scalability considerations.
- Data privacy or security protocols.

These omissions are understandable given the document’s purpose — it is not a technical paper but a strategic overview. However, for readers seeking technical implementation details, this document serves only as a high-level introduction to the IFoA’s institutional response to technological change.

#### Conclusion

The **AI, Data Science and Emerging Technologies Practice Board** represents a forward-looking initiative by the Institute and Faculty of Actuaries to ensure that actuaries remain at the forefront of technological innovation. By focusing on lifelong learning, research, engagement, and ethics, the board aims to equip its members with the knowledge and tools needed to navigate the challenges and opportunities presented by AI and data science. While the document does not delve into technical modeling or code implementation, it lays the groundwork for future technical collaborations, research outputs, and educational initiatives that will likely fill these gaps in the coming years.

The board’s success will depend on its ability to translate strategic vision into actionable technical content — such as open-source code repositories, standardized datasets, and reproducible case studies — that actuaries can directly apply in their work. As AI continues to reshape the financial services industry, the IFoA’s proactive stance positions the actuarial profession as a leader in responsible, data-driven decision-making.

---

**Note:** This summary is based solely on the provided text. No external information was used. Technical details such as modeling techniques, datasets, code, or architecture are not present in the source document and are therefore not included in the summary. The focus remains on the organizational, strategic, and educational aspects of the IFoA’s initiative.


================================================
FILE: data/summaries_235B_samples/DGM_summary_20260125_143906.md
================================================
## Document Metadata

**S/N:** 11  
**Title:** Deep Generative Models  
**Authors:** Ronald Richman, Mario V. Wüthrich  
**Date of publication:** 2025-11-30  
**Topic:** Artificial Intelligence for Actuarial Science  
**Sub-topic:** Deep Generative Modeling Techniques  
**URL:** Not available  

---

## Summary

### 1. Modeling Techniques

This chapter introduces **Deep Generative Models (DGMs)**, which aim to learn the underlying probability distribution of data rather than merely producing point predictions. Four primary modeling techniques are discussed:

- **Variational Auto-Encoders (VAEs)**: A latent factor approach that uses an encoder to map data to a latent space and a decoder to reconstruct data from latent samples. Training is based on maximizing the Evidence Lower Bound (ELBO), which balances reconstruction fidelity and regularization via KL divergence.
- **Generative Adversarial Networks (GANs)**: An adversarial framework where a generator network creates synthetic data to fool a discriminator network, which tries to distinguish real from fake data. The training involves a minimax game between the two networks.
- **Denoising Diffusion Models**: A multi-step generative process that learns to reverse a forward noising process by estimating noise added at each time step. Sampling proceeds by iteratively denoising pure Gaussian noise to generate structured data.
- **Decoder Transformer Models**: An implicit probability distribution approach that uses auto-regressive factorization to model sequences token-by-token. These include architectures like GPT, BART, and T5, which rely on self-attention with causal masking to ensure temporal causality.

These models are contrasted with traditional supervised learning by their ability to generate full predictive distributions, enabling applications such as data augmentation, anomaly detection, and conditional generation.

### 2. Code Availability

Implementation code is provided within the document for both **VAE** and **GAN** models using the **R programming language** with the `keras` and `tensorflow` libraries. Code snippets include:

- Building encoder and decoder networks for VAEs.
- Defining the ELBO loss function.
- Constructing and training a GAN with alternating updates for generator and discriminator.
- Generating synthetic samples from trained models.

The code is presented in the context of a “Sports car” insurance dataset example. No GitHub repository or external code link is provided; all code is embedded in the chapter.

### 3. Learning Type

The chapter focuses on **unsupervised learning** techniques, although some methods (like conditional VAEs or GANs) can be adapted for **conditional or semi-supervised** settings. The goal is to learn the underlying data distribution \(X \sim P\) or joint distribution \((Y, X) \sim P\), without relying on labeled outcomes. The models are trained using unlabeled data to approximate the true data-generating process.

### 4. Dataset

The primary dataset used for demonstration is a **real-world insurance dataset** called “SportsCars,” which contains features related to car specifications such as weight, engine power, torque, and engine speed. The dataset has 475 samples and 5 features after preprocessing (log-transformed and standardized). The data is used to train both VAE and GAN models to generate synthetic samples that mimic the original distribution.

No synthetic datasets are used; all examples rely on this real-world dataset. The dataset is not publicly named beyond “SportsCars.rda” and is assumed to be part of the authors’ internal repository.

### 5. Implementation Details

- **Programming language(s):** R (with `tensorflow` and `keras` wrappers)
- **Key libraries/frameworks:**
  - `tensorflow` and `keras` for building and training neural networks.
  - `MASS` for generating multivariate normal samples.
  - `ggplot2` and `corrplot` for visualization.
  - `reticulate` for interfacing with TensorFlow’s Python API in GAN implementation.

The implementations are designed for educational purposes and are not optimized for large-scale production use. No GPU acceleration or distributed training is discussed.

### 6. Model Architecture

#### Variational Auto-Encoder (VAE)

- **Encoder**: Maps input \(X\) to parameters of a latent Gaussian distribution \(q_\vartheta(Z|X) = \mathcal{N}(\mu_\vartheta(X), \Sigma_\vartheta(X))\). Implemented as a feedforward neural network (FNN) with two dense layers using `silu` activation.
- **Decoder**: Maps latent sample \(Z\) to parameters of the data distribution \(p_\vartheta(X|Z)\). In the example, only the mean \(m_\vartheta(Z)\) is modeled (not covariance), using a linear output layer.
- **Latent Space**: 10-dimensional Gaussian prior \(\pi(Z) = \mathcal{N}(0, I)\).
- **Reparameterization Trick**: Used to enable backpropagation through stochastic nodes by expressing \(Z = \mu_\vartheta(X) + \Sigma_\vartheta^{1/2}(X) \epsilon\), where \(\epsilon \sim \mathcal{N}(0, I)\).

#### Generative Adversarial Network (GAN)

- **Generator**: Takes a 10-dimensional Gaussian noise vector \(Z \sim \mathcal{N}(0, I)\) and maps it to a 5-dimensional synthetic sample \(X' = G(Z; \vartheta_1)\). Implemented as a 3-layer FNN with `tanh` activations.
- **Discriminator**: Takes a 5-dimensional sample and outputs a scalar probability \(D(X''; \vartheta_2) \in [0,1]\) indicating whether the sample is real. Implemented as a 3-layer FNN with `tanh` and `sigmoid` activations.
- **Training**: Alternating updates — discriminator is trained to classify real vs. fake samples, generator is trained to fool the discriminator. Labels are randomized slightly to improve stability.

#### Denoising Diffusion Models

- **Forward Process**: Adds Gaussian noise over \(T\) steps: \(X_t = \sqrt{\alpha_t} X_0 + \sqrt{1 - \alpha_t} \epsilon\), where \(\alpha_t = \prod_{s=1}^t (1 - \beta_s)\).
- **Reverse Process**: Trains a neural network \(\epsilon_\vartheta(X_t, t)\) to predict the noise \(\epsilon\) added at step \(t\). Loss: \(L_{\text{simple}}(\vartheta) = \mathbb{E}_{X_0, \epsilon, t} \left[ \| \epsilon - \epsilon_\vartheta(X_t, t) \|_2^2 \right]\).
- **Sampling**: Starts from \(X_T \sim \mathcal{N}(0, I)\) and iteratively denoises to \(X_0\).

#### Decoder Transformer Models

- **Architecture**: Auto-regressive model that factorizes joint distribution as \(p_\vartheta(X_{1:T}) = \prod_{t=1}^T p_\vartheta(X_t | X_{1:t-1})\).
- **Self-Attention with Causal Masking**: Ensures each token attends only to previous tokens. Mask sets future attention scores to \(-\infty\).
- **Output Layer**: Projects hidden state to vocabulary space, applies softmax with temperature \(\tau\) for calibration.
- **Training**: Uses “teacher forcing” — ground-truth tokens are fed as context during training.
- **Inference**: Generates tokens one-by-one, sampling or selecting the most probable next token.

### 7. Technical Content

Deep Generative Models (DGMs) represent a paradigm shift from traditional supervised learning, which typically produces point estimates, to methods that model full probability distributions. This capability is critical in actuarial science, where understanding uncertainty, simulating scenarios, and generating synthetic data for stress testing or fairness analysis are essential. The chapter systematically introduces four major DGM families: Variational Auto-Encoders (VAEs), Generative Adversarial Networks (GANs), Denoising Diffusion Models, and Decoder Transformer Models. Each method is grounded in probabilistic modeling and leverages deep neural networks to approximate complex, high-dimensional distributions.

#### Variational Auto-Encoders (VAEs)

VAEs are introduced as a probabilistic extension of autoencoders, designed to learn a latent representation \(Z\) of input data \(X\) such that new samples can be generated by sampling \(Z\) from a prior and decoding it. The model consists of two neural networks: an encoder that maps \(X\) to parameters of a posterior distribution \(q_\vartheta(Z|X)\), and a decoder that maps \(Z\) to parameters of the data distribution \(p_\vartheta(X|Z)\). The key innovation is the use of the Evidence Lower Bound (ELBO) as a tractable objective for training, since the true log-likelihood \(\log p_\vartheta(X)\) is intractable due to the integral over latent variables.

The ELBO decomposes into two terms: a **reconstruction term**, which encourages the decoder to faithfully reconstruct the input from the latent code, and a **regularization term**, which aligns the encoder’s posterior with a predefined prior (typically a standard Gaussian). This balance ensures that the latent space is both informative and well-structured, enabling meaningful sampling.

The **reparameterization trick** is crucial for training VAEs. In the Gaussian case, it allows gradients to flow through the stochastic sampling step by rewriting \(Z = \mu_\vartheta(X) + \Sigma_\vartheta^{1/2}(X) \epsilon\), where \(\epsilon\) is noise independent of the model parameters. This enables efficient optimization via stochastic gradient descent using Monte Carlo approximations (often with just one sample per data point).

In the provided R implementation, the VAE is applied to a 5-dimensional insurance dataset. The encoder and decoder are simple feedforward networks with `silu` and linear activations, respectively. The model is trained to maximize the ELBO, with separate loss functions for reconstruction and KL divergence. After training, synthetic samples are generated by sampling from the prior and passing through the decoder. While the generated samples match the original data’s correlation structure, they exhibit reduced variance because only the mean (mode) of the decoder’s Gaussian output is used — the covariance is not modeled.

#### Generative Adversarial Networks (GANs)

GANs operate on a fundamentally different principle: adversarial training. The generator \(G\) learns to map random noise \(Z\) to synthetic data \(X'\) that mimics real data, while the discriminator \(D\) learns to distinguish real from fake samples. The training objective is a minimax game: the generator minimizes the discriminator’s ability to detect fakes, while the discriminator maximizes its accuracy.

The mathematical formulation is:
\[
\min_G \max_D V(D, G) = \mathbb{E}_{X \sim p_{\text{data}}(x)}[\log D(X)] + \mathbb{E}_{Z \sim \pi(z)}[\log(1 - D(G(Z)))]
\]
This objective is optimized via alternating gradient updates: first, the discriminator is updated with fixed generator weights; then, the generator is updated with fixed discriminator weights. The discriminator’s loss is a binary cross-entropy, while the generator’s loss is designed to maximize the discriminator’s error.

GANs are notoriously difficult to train due to issues like **mode collapse** (where the generator produces only a limited set of outputs) and **vanishing gradients**. The chapter notes that techniques such as label smoothing and careful hyperparameter tuning can mitigate these problems. In the R implementation, the GAN is trained on the same insurance dataset. The generator and discriminator are both 3-layer FNNs with `tanh` activations. After training, the discriminator’s loss stabilizes near \(-\log(0.5)\), indicating it can no longer distinguish real from fake samples — a sign of successful training. Generated samples are visually and statistically similar to the original data, though slightly less convincing than VAE outputs in this case.

#### Denoising Diffusion Models

Diffusion models offer a radically different approach: instead of learning a direct mapping from noise to data, they learn to reverse a forward noising process. The forward process gradually adds Gaussian noise to data over \(T\) steps until it becomes indistinguishable from pure noise. The reverse process learns to denoise step-by-step, starting from pure noise and ending with a structured sample.

The key insight is that the reverse process can be learned by training a neural network to predict the noise added at each step. The loss function is a simple mean squared error between the true noise \(\epsilon\) and the predicted noise \(\epsilon_\vartheta(X_t, t)\). Once trained, sampling proceeds by starting from \(X_T \sim \mathcal{N}(0, I)\) and iteratively applying the learned denoising step to obtain \(X_0\).

Diffusion models are particularly effective for image and audio generation, where their multi-step process leads to high-quality, diverse outputs. However, they are computationally expensive due to the need for many sampling steps. The chapter notes that applications in actuarial science are currently scarce, and no implementation is provided for the insurance dataset.

#### Decoder Transformer Models

Decoder transformers, exemplified by GPT, are designed for sequential data generation. They model the joint distribution of a sequence \(X_{1:T}\) via auto-regressive factorization: \(p_\vartheta(X_{1:T}) = \prod_{t=1}^T p_\vartheta(X_t | X_{1:t-1})\). At each step, the model uses self-attention over previous tokens to predict the next token, with a causal mask ensuring that future tokens are not attended to.

The architecture includes:
- **Self-Attention with Causal Masking**: Each token’s query attends only to previous keys, implemented by adding \(-\infty\) to future attention scores.
- **Positional Embeddings**: Static functions (not learned) that encode token positions.
- **Output Layer**: Projects hidden state to vocabulary space, applies softmax with temperature \(\tau\) for calibration. Higher \(\tau\) flattens the distribution (more uncertainty), lower \(\tau\) sharpens it (more confidence).

Training uses “teacher forcing” — ground-truth tokens are fed as context, ensuring the model learns correct sequential dependencies. Inference proceeds token-by-token, sampling or selecting the next token until an end-of-sequence token is reached.

These models scale well with data and parameter size, enabling large language models (LLMs) that can perform in-context learning — generalizing to new tasks with appropriate prompts. Applications include text generation, summarization, translation, and code generation. While the chapter does not implement a transformer on the insurance dataset, it highlights their relevance for actuarial tasks involving sequential or textual data.

#### Comparative Analysis and Practical Considerations

Each DGM has strengths and weaknesses:
- **VAEs** are conceptually simple, stable to train, and provide probabilistic encodings, but may produce blurry outputs due to the averaging effect of the ELBO.
- **GANs** can generate sharp, realistic samples but are unstable to train and prone to mode collapse.
- **Diffusion Models** produce high-fidelity outputs and are stable to train but are slow due to multi-step sampling.
- **Decoder Transformers** excel at sequential data and scale well but require large datasets and compute resources.

In actuarial applications, VAEs and GANs are most immediately applicable for generating synthetic portfolios or simulating claim scenarios. Diffusion models may be useful for image-based actuarial tasks (e.g., damage assessment), while transformers could model temporal claim patterns or generate synthetic policy text.

The chapter emphasizes that DGMs go beyond supervised learning by learning the full data distribution, enabling richer analyses such as anomaly detection (via likelihood evaluation), data augmentation, and conditional generation. The provided R code demonstrates that these models can be implemented with modest computational resources, making them accessible for educational and exploratory purposes in actuarial science.

In conclusion, DGMs represent a powerful toolkit for actuaries seeking to model uncertainty, simulate scenarios, and generate synthetic data. While challenges remain in training stability and computational cost, the methods presented provide a solid foundation for applying generative modeling to real-world actuarial problems.


================================================
FILE: data/summaries_235B_samples/evolution-of-economic-scenario-generators-a-report-by-t_summary_20260125_143906.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** Evolution of economic scenario generators: a report by the Extreme Events Working Party members  
**Authors:** P. Jakhria, R. Frankland, S. Sharp, A. Smith, A. Rowe, T. Wilkins  
**Date of publication:** 2019-02-22 (presented); published in British Actuarial Journal, Vol. 24, e4, 2019  
**Topic:** Actuarial Science / Financial Modeling  
**Sub-topic:** Economic Scenario Generators (ESGs) in Insurance and Pension Fund Modeling  
**URL:** https://doi.org/10.1017/S1357321718000181  

---

## Summary

### 1. Modeling Techniques

The paper does not describe a single modeling technique but rather traces the historical evolution of modeling approaches used in economic scenario generation (ESG) for actuarial applications. The techniques discussed span several decades and include:

- **Random Walk Models**: Early stochastic models based on Brownian motion and geometric Brownian motion, pioneered by Louis Bachelier (1900) and later formalized by Norbert Wiener and Kiyoshi Ito. These models assume asset prices follow a path where future changes are independent of past changes and are normally distributed. They were foundational for modern portfolio theory (Markowitz, 1952) and later option pricing models.

- **Time Series Models**: Primarily represented by the Wilkie model (1984), which uses autoregressive moving average (ARMA) structures to model relationships between economic variables such as inflation, equity returns, and interest rates. The model employs a “cascade” structure where inflation drives other variables. Later extensions include multivariate ARMA, VAR (Vector Auto Regressive), and GARCH (Generalized Autoregressive Conditional Heteroskedasticity) models to capture volatility clustering and non-stationarity.

- **Market-Consistent (MC) and Arbitrage-Free Models**: Driven by financial economics and option pricing theory (Black-Scholes, 1973; Merton, 1973), these models are calibrated to market prices of traded instruments (e.g., options, bonds) and ensure no arbitrage opportunities exist. Examples include Vasicek, Hull-White, Cox-Ingersoll-Ross (CIR) for interest rates, and geometric Brownian motion with stochastic volatility or jumps for equities. These models are used for pricing and valuing liabilities with embedded options or guarantees.

- **Value at Risk (VaR) Models**: Used for capital adequacy calculations under regulatory regimes like ICAS and Solvency II. These are typically based on historical simulation or parametric distributions fitted to historical data (not market prices) and focus on extreme tail events (e.g., 99.5th percentile over 1 year). They often employ copulas to model correlations between risk factors.

The paper emphasizes that these modeling paradigms are not mutually exclusive but have coexisted and evolved in parallel, with different models being preferred for different purposes (e.g., strategic asset allocation vs. regulatory capital calculation).

### 2. Code Availability

**Not available.** The paper is a historical and conceptual review of modeling evolution and does not provide or reference any specific implementation code, GitHub repositories, or software packages. While some models (e.g., Wilkie model) were published with equations and parameters suitable for spreadsheet implementation, no open-source or downloadable code is mentioned.

### 3. Learning Type

**Not applicable.** The paper does not describe machine learning or statistical learning models trained on data in the modern ML sense. The modeling techniques discussed (random walks, time series, arbitrage-free pricing) are based on mathematical finance and econometric theory rather than supervised, unsupervised, or self-supervised learning algorithms. Parameter estimation in time series models (e.g., ARMA) may involve statistical inference, but this is not framed as “learning” in the ML context.

### 4. Dataset

The paper does not describe a single dataset used for model training or validation. Instead, it references:

- **Historical UK time series data** (1919–1982) used by Wilkie to calibrate his model.
- **Historical market data** (e.g., equity indices, interest rates, inflation) used for calibrating time series and VaR models.
- **Option prices and yield curves** used to calibrate market-consistent models.
- **Real-world data** from UK pension funds and insurers for asset allocation examples (e.g., Figure 2).

No specific dataset names (e.g., CRSP, FRED, Bloomberg) are provided. The focus is on the *type* of data used (historical, market-based, macroeconomic) rather than on specific sources or repositories.

### 5. Implementation Details

- **Programming language(s):** Not specified. The paper predates widespread use of Python/R for actuarial modeling. Wilkie’s original model was implemented in spreadsheets; later models may have used MATLAB, SAS, or proprietary software (e.g., Towers Perrin’s system).
- **Key libraries/frameworks:** Not specified. No mention of TensorFlow, PyTorch, scikit-learn, or similar ML libraries. Traditional statistical packages (e.g., R’s `forecast`, `tseries`, or SAS/ETS) may have been used for time series, but this is not stated.

### 6. Model Architecture

The paper does not describe a single composite model architecture but outlines the structure of key historical models:

- **Wilkie Model (1984, 1995)**: A cascade of time series equations. Inflation is the root variable, driving:
  - Equity returns (via dividend yield and payout)
  - Long-term interest rates (via consol yield, modeled as a 3rd-order AR process)
  - Short-term interest rates
  - Property returns
  - Wage inflation
  The model is parsimonious (few variables) but complex in its interdependencies. It was designed for long-term actuarial use, not short-term forecasting.

- **Market-Consistent Models**: Typically multi-factor, arbitrage-free models. For example:
  - **Interest rate models**: One-factor (Vasicek) or multi-factor (Hull-White) models that fit the initial yield curve and ensure no arbitrage.
  - **Equity models**: Geometric Brownian motion with drift, extended with stochastic volatility (Heston) or jumps (Merton).
  These models are often solved via PDEs (Black-Scholes) or Monte Carlo simulation.

- **VaR Models**: Not a single architecture but a methodology. Typically:
  - Simulate or estimate joint distribution of risk factors (interest rates, equities, inflation) using historical data or parametric assumptions.
  - Apply copulas to model dependence.
  - Calculate the 99.5th percentile loss over 1 year (Solvency II requirement).
  - The “critical stress” is the combination of factor movements that maximizes capital shortfall.

### 7. Technical Content

The paper provides a comprehensive historical account of how economic scenario generators (ESGs) have evolved in the UK actuarial profession over the past 50 years, driven by technical innovation, regulatory changes, and shifts in investment practice. The core narrative is that ESGs have progressed from simple random walk models to complex, multi-factor, market-consistent, and regulatory-driven frameworks, each addressing new challenges in modeling the uncertainty of capital markets for long-dated insurance and pension liabilities.

#### Origins of Stochastic Modeling in Actuarial Science

The need for stochastic models arose from the limitations of deterministic approaches, which assumed a single, fixed future (e.g., best-estimate interest rate). Early actuarial work focused on mortality and fixed-income investments, where deterministic models sufficed. However, as UK insurers began investing in equities and property from the 1920s onward (equity allocation rose from 2% in 1920 to 21% by 1952), the volatility and path-dependence of asset returns made deterministic models inadequate. The 1980 Maturity Guarantee Working Party (MGWP) report highlighted this: for maturity guarantees, investment risk cannot be matched, and policies are not independent, leading to high variability in claims. Thus, a distribution of outcomes — not just an expected value — was needed.

The advent of electronic computers (ENIAC, 1945) and the Monte Carlo method (Ulam, von Neumann, Metropolis, 1947–1949) enabled the practical simulation of stochastic paths. Monte Carlo relies on random number generation to simulate many possible future scenarios, allowing actuaries to assess tail risks and the full distribution of outcomes. Early random number generators (e.g., von Neumann’s “middle-square” method) were crude, but the principle — using randomness to explore uncertainty — became foundational.

#### The Challenge of Stochastic Asset Models (SAMs)

Modeling assets for insurers is uniquely challenging due to three factors:

1. **Global Asset Allocation**: UK pension funds and insurers have diversified geographically and across asset classes (equities, bonds, property, alternatives). This diversification improves risk-adjusted returns but complicates modeling, as each asset class has different risk-return characteristics and interdependencies. A truly accurate model would need to capture capital flows within and between asset classes and countries — a task of immense complexity.

2. **Long-Dated Liabilities**: Insurance and pension liabilities often span decades. Models must extrapolate scenarios far into the future, requiring not just accuracy but also stability over long horizons. The 2003 Prudential Sourcebook introduced the “Realistic Balance Sheet,” emphasizing that balance sheet volatility is driven by asset-side fluctuations. This forced insurers to model long-term asset behavior, not just short-term returns.

3. **Capital Regulations**: Regulations like ICAS (2004) and Solvency II (2016) require models to estimate extreme events (e.g., 1-in-200-year losses) that may lie far outside historical experience. This “extrapolation in probability” demands models that can simulate rare, severe scenarios — a challenge for data-driven models that rely on historical data.

#### Evolution of ESGs: Four Stages

The paper identifies four broad stages in ESG evolution:

1. **Random Walk Models (Pre-1980s)**: Based on Bachelier’s (1900) and Wiener’s (1920s) work, these models assume asset prices follow a random path with constant volatility. Markowitz’s (1952) portfolio theory showed how to optimize portfolios based on mean and variance, leading to the efficient frontier and CAPM. Advantages: intuitive, easy to calibrate to historical data, aligned with efficient market hypothesis. Disadvantages: no mean reversion, poor fit for long-term liabilities, ignores correlations between asset classes.

2. **Time Series Models (1980s–1990s)**: Dominated by the Wilkie model (1984), which used ARMA structures to model inflation, equity returns, and interest rates in a “cascade.” Wilkie’s model was designed for actuarial use — long-term, transparent, and practical (implementable in spreadsheets). It gained widespread adoption due to its publication, ease of use, and alignment with actuarial beliefs (e.g., mean reversion in equities). However, it had limitations: strong mean reversion could lead to unrealistic results (e.g., underestimating tail risk), and it was not arbitrage-free. The 1992 Geoghegan Working Party report critiqued its statistical foundations, warning against “enthroning” a single model without understanding its weaknesses.

3. **Market-Consistent (MC) and Arbitrage-Free Models (Late 1990s–2000s)**: Driven by financial economics (Black-Scholes, 1973) and regulatory demand for fair value accounting, these models are calibrated to market prices (e.g., option implied volatilities, yield curves) to ensure no arbitrage. They are used for pricing liabilities with embedded options (e.g., with-profits guarantees). Examples include Vasicek (interest rates) and geometric Brownian motion with stochastic volatility (equities). Advantages: consistent with market prices, suitable for valuation. Disadvantages: complex to calibrate, sensitive to parameter choices, may not reflect real-world probabilities (requires “risk premium” adjustments).

4. **VaR Models (2000s–Present)**: Required by ICAS and Solvency II for capital adequacy, these models focus on the 99.5th percentile loss over 1 year. They typically use historical simulation or parametric distributions fitted to historical data (not market prices) and employ copulas to model correlations. Advantages: regulatory compliance, focus on tail risk. Disadvantages: poor at modeling contagion (e.g., 2008 crisis), assumes constant volatility, may overfit to recent data (e.g., falling rates), and lacks economic plausibility in extreme scenarios.

#### Key Transitions and Drivers

The shift from one model type to another was rarely a clean replacement. Instead, models coexisted, each serving different purposes:

- **Wilkie to MC Models**: Driven by the need for market-consistent valuations (e.g., for with-profits liabilities) and regulatory changes (e.g., 2003 Prudential Sourcebook). MC models were not “better” but addressed different problems (valuation vs. strategic planning).

- **MC to VaR Models**: Driven by Solvency II’s 1-year VaR requirement. VaR models are not a replacement for MC models but a regulatory add-on for capital calculation. Interviewees noted that insurers often use MC models for business decisions and VaR models for compliance.

Key drivers of evolution include:

- **Regulation**: The largest driver historically (e.g., ICAS, Solvency II, Realistic Balance Sheet).
- **Technology**: Increased computing power enabled Monte Carlo simulation and complex models.
- **Investment Practice**: Globalization and diversification increased model complexity.
- **Academic Influence**: Financial economics (Black-Scholes, CAPM) gradually permeated actuarial thinking.
- **User and Developer Dissatisfaction**: Limitations in existing models (e.g., Wilkie’s mean reversion) spurred innovation.

#### Current State and Future Directions

As of 2019, the insurance industry uses a mix of models:

- **Strategic Asset Allocation**: Often uses time series or MC models for long-term projections.
- **Regulatory Capital**: Uses 1-year VaR models (Solvency II).
- **Product Pricing**: Uses MC models for liabilities with guarantees.

Interviewees emphasized the importance of understanding model limitations and communicating them to users. There is a growing need for:

- **Longer-Term Real-World Modeling**: Driven by ORSA (Own Risk and Solvency Assessment), which requires multi-year projections.
- **Greater Transparency**: To customers and regulators, requiring models that can explain portfolio outcomes.
- **Integration with Financial Planning**: Actuarial education may need to include lifetime financial planning.
- **Technological Advancements**: Cloud computing and big data enable more granular modeling, but Occam’s razor (simplicity) remains important. Machine learning is seen as a potential disruptor but is constrained by the need for real-time market data.

International perspectives vary: Canada uses regime-switching models (Hardy), the US has a minimal profession-wide ESG, and Switzerland has regulator-led ESG development. The paper suggests the term “ESG” may be confusing due to its use in “Environmental, Social, Governance” and proposes “CMM” (Causal Market Model) or “SAM” (Stochastic Asset Model) as alternatives.

In conclusion, the evolution of ESGs reflects a journey from simple, intuitive models to complex, regulatory-driven frameworks. The future will likely see a blend of real-world and market-consistent models, driven by both regulation and the need for better long-term decision-making in an increasingly uncertain world.


================================================
FILE: data/summaries_235B_samples/ifoa-response-to-treasury-select-committee-consultation_summary_20260125_143906.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** Treasury Committee Inquiry: AI in Financial Services — IFoA Response  
**Authors:** Institute and Faculty of Actuaries (IFoA), Steven Graham (Technical Policy Manager, on behalf of IFoA)  
**Date of publication:** 2025-04-11  
**Topic:** Artificial Intelligence in Financial Services  
**Sub-topic:** Regulatory, Ethical, and Operational Implications of AI Adoption in Insurance, Pensions, Investment, and Risk Management  
**URL:** Not available (document appears to be an official submission to the UK Treasury Committee; no direct public URL provided)

---

## Summary

### 1. Modeling Techniques

The document does not describe specific machine learning or AI modeling architectures (e.g., CNNs, LSTMs, Transformers) in technical detail. Instead, it broadly references **narrow AI** (traditional machine learning and data science) and **Generative AI (GenAI)** as the two main categories of AI currently in use within financial services. The IFoA’s focus is on the *application domains* and *risk implications* of these technologies rather than algorithmic specifics.

Examples of AI applications mentioned include:
- **Predictive analytics** for underwriting, pricing, and customer segmentation.
- **Fraud detection models** using anomaly detection techniques.
- **Natural Language Processing (NLP)** for analyzing unstructured data (news, emails, social media, earnings calls).
- **Reinforcement Learning from Human Feedback (RLHF)** as a technique to mitigate herding behavior in financial models.
- **Retrieval Augmented Generation (RAG)** to ground GenAI outputs and reduce hallucinations.

No specific model types (e.g., XGBoost, Random Forest, BERT, GPT) are named, nor are hyperparameters, loss functions, or training procedures described. The emphasis is on *functional use cases* rather than *technical implementation*.

### 2. Code Availability

**Not available.** The document is a policy and risk assessment response from a professional body (IFoA), not a research paper or technical report. No code repositories, GitHub links, or open-source implementations are referenced or provided.

### 3. Learning Type

The document does not specify whether the AI systems discussed use **supervised**, **unsupervised**, or **self-supervised** learning. However, based on the described applications — such as underwriting, fraud detection, pricing, and claims processing — it is implied that **supervised learning** dominates in most use cases, where labeled historical data (e.g., claims history, customer outcomes) is used to train models.

For GenAI applications (e.g., chatbots, document summarization), **self-supervised learning** (as used in large language models) is likely, though not explicitly stated. No mention is made of reinforcement learning beyond RLHF as a mitigation technique.

### 4. Dataset

The document references **real-world data** extensively, particularly in financial services contexts:
- **Insurance data** (mortality, claims, underwriting, customer behavior).
- **Investment data** (market data, news articles, earnings transcripts, social media sentiment).
- **Pension and retirement data** (member engagement, savings behavior).
- **Environmental and ESG data** for climate risk modeling.
- **Customer vulnerability indicators** (behavioral patterns, voice recognition, neurodiversity profiles).

No specific public datasets (e.g., Kaggle, UCI, Bloomberg, Refinitiv) are named. The IFoA mentions **Federated Learning** as a technique to enable model training across insurers without sharing raw data, implying that proprietary, siloed datasets are the norm. Data quality, bias, and representativeness are highlighted as critical concerns, but no dataset names or sources are provided.

### 5. Implementation Details

- **Programming language(s):** Not specified. Given the context of financial services and actuarial work, Python is likely the dominant language, but no explicit mention is made.
- **Key libraries/frameworks:** Not specified. No references to TensorFlow, PyTorch, scikit-learn, Hugging Face, or other ML libraries. The focus is on high-level applications and governance, not technical stack.

### 6. Model Architecture

No detailed model architectures are described. The document discusses **composite or system-level applications** rather than component models. For example:
- **AI-driven underwriting systems** combine predictive modeling, data segmentation, and possibly NLP for document analysis.
- **GenAI chatbots** are described as tools for customer service, with human oversight for complex queries — implying a hybrid human-AI workflow, but no architecture (e.g., encoder-decoder, RAG pipeline) is detailed.
- **Federated Learning** is mentioned as a technique to train models across institutions without data sharing — this implies a distributed architecture, but no technical breakdown is given.

The term “black-box” is used to describe opaque AI models, highlighting the lack of interpretability, but no specific architectures (e.g., deep neural networks) are identified as the source of this opacity.

### 7. Technical Content

The IFoA’s response to the UK Treasury Committee’s inquiry on AI in financial services provides a comprehensive, high-level assessment of the current and future landscape of AI adoption, focusing on **risk, ethics, regulation, and productivity**. While not a technical paper, it offers valuable insights into how AI is being applied in actuarial domains and the challenges associated with its deployment.

#### Current AI Adoption in Financial Services

AI is already embedded across multiple financial sectors, with actuaries playing a central role in its application. Key areas include:

- **General Insurance**: AI enhances underwriting precision, automates claims processing, and improves fraud detection. Granular data allows for better risk segmentation and personalized pricing. Customer segmentation and targeted marketing are also AI-driven.
- **Health & Care Insurance**: AI supports personalized disease management programs, improving patient outcomes and reducing claims costs.
- **Life Insurance**: More accurate mortality and longevity predictions, along with automated underwriting triage, increase efficiency.
- **Pensions**: AI aids in investment analysis, long-term risk scenario modeling, fraud detection, and personalized retirement planning.
- **Investments**: AI processes vast amounts of structured and unstructured data (news, social media, earnings calls) to detect patterns, optimize asset allocation, and personalize investment strategies.
- **Risk Management**: AI monitors large volumes of data to inform risk strategies, including climate and ESG risk analysis.
- **Regulatory Compliance**: AI automates reporting, audit trails, and real-time risk identification (RegTech).

Fintech firms are noted as early adopters due to their agility, cloud-native infrastructure, and technical expertise. Traditional firms face challenges due to legacy systems (e.g., Excel-based workflows) and slower integration cycles.

#### Productivity Gains and Use Cases

AI offers significant potential to improve productivity by automating routine, low-risk tasks such as:
- Claims processing and underwriting
- Data cleansing and assembly
- Compliance checks and reserving
- Meeting notes and model documentation
- Customer service via chatbots (with human escalation for complex issues)
- Fraud and anomaly detection
- Personalized financial advice and retirement planning

However, these gains are contingent on overcoming barriers such as cost, data quality, regulatory uncertainty, and workforce upskilling. The document emphasizes that AI should free humans for higher-value work, such as interpreting AI outputs and managing model governance.

#### Risks and Challenges

##### Cybersecurity Risks
AI increases exposure to cyber threats. AI-driven attacks can exploit vulnerabilities at scale, and interconnected systems amplify potential damage. Open-source AI models may contain hidden vulnerabilities. Mitigation includes upgrading cyber resilience, using AI-driven security tools, and training users.

##### Third-Party Dependencies
Reliance on external AI providers (e.g., large tech firms) introduces systemic risks. Firms may lack transparency into model complexity, data usage, and parameterization. There is a risk of data harvesting by providers. Mitigations include:
- Independent AI audits
- Stress testing and explainability requirements
- Clear contractual agreements

##### GenAI Hallucinations and Herding Behavior
GenAI models can produce plausible but false outputs (“hallucinations”), which are unacceptable in financial contexts where precision is critical. Hallucinations arise from incomplete or inaccurate data. Techniques like RAG and prompt engineering are used to ground outputs, but they require significant effort.

Herding behavior occurs when multiple firms use similar AI models trained on the same data, leading to market instability. RLHF is proposed as a mitigation technique.

##### Bias and Discrimination
AI can replicate or amplify existing biases, particularly if trained on flawed or unrepresentative data. This can lead to discriminatory outcomes, especially for vulnerable consumers or protected characteristics (e.g., using postcode as a proxy for ethnicity). Mitigations include:
- Robust bias testing
- Using protected attributes in mitigation (though data collection is often lacking)
- “Ethics by Design” principles
- Transparency and explainability requirements

The document notes that AI is not inherently more biased than humans but can be more scalable in its bias. AI can also help identify bias more objectively than humans.

##### Job Displacement and Reskilling
AI is likely to displace administrative and manual roles. However, it also creates demand for new roles in AI governance, ethics, and model interpretability. Reskilling is essential, with a focus on critical thinking, ethics, and understanding model limitations — skills actuaries already possess.

##### Energy Consumption and Climate Impact
AI’s energy use is a growing concern. The International Energy Agency predicts global data center energy consumption could double by 2030. This conflicts with climate goals. Mitigations include using renewables, locational flexibility, and operational efficiency.

#### Regulatory and Governance Considerations

The IFoA advocates for a **principles-based, rather than rules-based**, approach to AI regulation. This is due to the rapid pace of AI evolution, which makes specific rules quickly obsolete. Existing UK conduct and prudential frameworks are deemed adaptable to AI if principles-based.

Key regulatory priorities include:
- Balancing innovation with consumer protection
- Avoiding conflicting or duplicative regulations
- Ensuring global alignment to maintain a level playing field
- Incorporating ethical guardrails and accountability
- Requiring transparency, explainability, and redress mechanisms for consumers

The regulatory sandbox is seen as a useful tool for responsible innovation. Firms must ensure adequate AI governance, particularly in second and third lines of defense. Training staff on AI ethics and model limitations is essential.

#### Consumer Benefits and Vulnerabilities

AI offers benefits such as:
- More accurate risk assessment (potentially lowering premiums for low-risk consumers)
- Holistic financial services (e.g., health incentives via fitness trackers)
- Improved affordability and accessibility of financial advice
- Enhanced fraud detection
- Personalized retirement and investment planning

For vulnerable consumers, AI can:
- Identify vulnerabilities in real-time
- Provide accessible interfaces (e.g., for neurodiverse individuals)
- Detect financial exploitation

However, risks include:
- Financial exclusion if AI deems some consumers “high risk”
- Mis-selling due to automated sales processes
- Lack of understanding or challengeability of AI-driven decisions
- Voice recognition systems misclassifying atypical voices as fraudulent

#### Data Sharing and Privacy

Federated Learning is proposed as a solution to enable AI training without sharing sensitive data. Insurers are reluctant to share data due to commercial and privacy concerns, which stifles innovation. Legislative changes may be needed to facilitate secure data sharing while protecting privacy.

Data protection concerns include:
- Lack of consumer awareness about data usage
- Risk of sophisticated AI-driven cyberattacks
- Data retention and secure deletion policies

Safeguards needed:
- Comprehensive data governance frameworks
- Mandatory bias testing
- Transparency and explainability requirements
- Clear redress mechanisms
- Robust compliance to prevent mis-selling

#### Future Outlook

Over the next decade, AI adoption is expected to accelerate due to:
- Increasing data availability and computing power
- Multimodality (text, audio, image, video)
- Autonomous AI agents
- Fintech and tech giant competition

AI-driven RegTech, personalized investment tools, and enhanced customer service will become more prevalent. The UK is well-positioned to lead in responsible AI adoption, given its Fintech ecosystem and AI research expertise. Open finance, building on open banking, could be “turbocharged” by AI.

#### Conclusion

The IFoA’s response provides a balanced, public-interest perspective on AI in financial services. It recognizes AI’s transformative potential while emphasizing the need for robust governance, ethical oversight, and regulatory adaptability. Actuaries, with their skills in risk analysis, critical thinking, and model evaluation, are well-suited to navigate the AI landscape. The document serves as a call to action for policymakers to foster innovation while protecting consumers and ensuring financial stability.

The IFoA positions itself as a key stakeholder in the ongoing debate, offering actuarial expertise to inform the evolution of AI regulation and practice. Collaboration across academia, industry, government, and regulators is essential to address the global and interdisciplinary nature of AI challenges.

In summary, while the document does not delve into technical modeling details, it offers a rich, nuanced understanding of AI’s role, risks, and opportunities in financial services — particularly from an actuarial and regulatory standpoint. Its insights are highly relevant for students and professionals seeking to understand the broader implications of AI beyond algorithms and code.


================================================
FILE: data/summaries_235B_samples/IFoA_WP_DSSCC_-_Green_Bonds_Paper_2__5BAugust_2024_5D_summary_20260125_143906.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** Time series analysis of GSS bonds: Part 2 – Further univariate analysis of S&P Green Bond Index  
**Authors:** D Dey (on behalf of the IFoA Data Science, Sustainability & Climate Change Working Party)  
**Date of publication:** 2024-08  
**Topic:** Time series forecasting  
**Sub-topic:** Neural network architectures for green bond index prediction  
**URL:** Not available  

---

## Summary

### 1. Modeling Techniques

The study employs multiple neural network architectures for univariate time series forecasting of the S&P Green Bond Index. The primary models analyzed include:

- **N-BEATS (Neural Basis Expansion Analysis for interpretable Time Series forecasting)**: A deep feedforward architecture composed of stacks of blocks that use doubly residual stacking to iteratively refine backcasts and forecasts. The architecture includes fully connected layers and basis function layers to capture trends and seasonality without explicit feature engineering. The model was implemented with one stack (M=1), varying numbers of blocks per stack (2–30), and varying hidden layers per block (1–4). The authors opted for a generic configuration without explicit trend/seasonality decomposition.

- **Deep Neural Networks (DNNs)**: Feedforward networks with 1–3 hidden dense layers. Models labeled DNN0, DNN1, and DNN2 correspond to 1, 2, and 3 hidden layers respectively.

- **Convolutional Neural Networks (CNNs)**: 1D convolutional networks with one Conv1D layer, optionally followed by a dense layer (CNN0 and CNN1).

- **Long Short-Term Memory (LSTMs)**: Recurrent networks with one LSTM layer, return_sequences=False, with or without an additional dense layer (LSTM_0HL_F, LSTM_1HL_F).

- **Gated Recurrent Units (GRUs)**: Similar to LSTMs but with simpler gating mechanisms. Models GRU_0HL_F and GRU_1HL_F use one GRU layer with or without an additional dense layer.

All models were trained to predict the next day’s index value (horizon = 1 day) using input windows of 1, 2, or 5 prior days. The baseline model simply uses yesterday’s value as today’s prediction.

### 2. Code Availability

Code availability is **not explicitly stated** in the document. While the authors mention using Google Colab and open-source libraries (TensorFlow, Keras, Optuna, statsmodels), no public repository (e.g., GitHub) is provided. The implementation is based on code from the “Zero to Mastery TensorFlow for Deep Learning Book” (Bourke, 2023), but no direct link to the adapted code is given.

### 3. Learning Type

The approach uses **supervised learning**. The models are trained to predict the next day’s index value based on historical values. The training data consists of labeled sequences: input windows (past index values) paired with target outputs (next day’s index value). No self-supervised or unsupervised techniques are employed.

### 4. Dataset

- **Dataset name/source**: S&P Green Bond Index (Total Performance, USD), sourced from S&P Global’s website via a free subscription account.
- **Time range**: 31 January 2013 to 17 February 2023 (2,615 daily observations).
- **Data splits**: Chronological split into training (70%, 1,830 days: 2013-01-31 to 2020-02-16), validation (20%, 523 days: 2020-02-17 to 2022-02-16), and test (10%, 262 days: 2022-02-17 to 2023-02-17). This preserves temporal order for time series modeling.
- **Data type**: Real-world financial time series data.
- **Preprocessing**: No normalization, scaling, or log transformation was applied to the index values. The authors note this may be explored in future work.
- **Volatility**: The test set exhibits higher volatility (standard deviation = 8.04) compared to training (5.32) and validation (5.70) sets, posing a challenge for model generalization.

### 5. Implementation Details

- **Programming language(s)**: Python (implied by use of TensorFlow, Keras, Optuna, statsmodels, and Google Colab).
- **Key libraries/frameworks**:
  - **TensorFlow** and **Keras**: For building and training neural network models.
  - **Optuna**: For hyperparameter optimization using the Tree-structure Parzen Estimator (TPE) algorithm.
  - **statsmodels**: For generating ACF/PACF plots and seasonal decomposition.
  - **Matplotlib/Seaborn** (implied): For visualizing loss curves and model architectures (via Keras visualizer).
  - **NumPy/Pandas** (implied): For data manipulation and preprocessing.

### 6. Model Architecture

#### N-BEATS Architecture

The N-BEATS model is a deep feedforward network structured in stacks of blocks. Each block contains:

1. **Fully Connected Section**: Consists of 4 hidden layers (configurable in this study) with ReLU activation. The final layer projects to two sets of expansion coefficients: θ_b (backcast) and θ_f (forecast) via linear layers.

2. **Basis Layer Section**: Maps θ_b and θ_f to outputs using basis functions (g_b and g_f). In this study, no explicit basis functions (e.g., Fourier, polynomial) were used; the basis layer was implemented as a generalized dense layer.

3. **Doubly Residual Stacking**: Each block produces a backcast residual (x̂_l) and a partial forecast (ŷ_l). The residual x_l = x_{l-1} - x̂_{l-1} is passed to the next block, while ŷ_l is summed to form the global forecast ŷ = Σ ŷ_l. This allows iterative refinement of the forecast.

The overall architecture used in this study:
- **Stacks (M)**: Fixed at 1 (for computational efficiency; training took 3–4 hours).
- **Blocks per stack (K)**: Optimized between 2 and 30.
- **Hidden layers per block**: Optimized between 1 and 4.
- **Neurons per hidden layer**: Optimized between 4 and 512.
- **Output layer**: Dense layer with neurons equal to the combined length of input window and output horizon (e.g., 1+1=2 for window=1, horizon=1).

The model has over 4 million trainable parameters for the 1-day window case, vastly more than simpler DNNs (625 parameters) or LSTMs (47,629 parameters).

#### Other Architectures (from Appendix 3)

- **DNNs**: Simple feedforward networks with dense layers. DNN0 (1 layer), DNN1 (2 layers), DNN2 (3 layers).
- **CNNs**: 1D convolutional layer (Conv1D) followed by optional dense layers (CNN0: no dense, CNN1: one dense).
- **LSTMs/GRUs**: Single recurrent layer (LSTM or GRU) with return_sequences=False, optionally followed by a dense layer (e.g., LSTM_1HL_F has one LSTM layer + one dense layer).

All models used the same hyperparameter optimization framework (Optuna) and training setup (MAE loss, Adam optimizer, L2 regularization).

### 7. Technical Content

This paper extends a prior study (Dey, 2024) on forecasting the S&P Green Bond Index using neural networks. The goal is to evaluate whether more complex architectures (specifically N-BEATS) or wider input windows (2 or 5 days) can outperform a simple baseline (yesterday’s value = today’s value) or the simpler neural networks from the initial study. The analysis is strictly univariate and does not incorporate traditional methods like ARIMA or external variables.

#### N-BEATS Analysis (Section 3)

The N-BEATS architecture was chosen for its state-of-the-art performance in the M4 forecasting competition and its interpretability. The authors implemented a generic version (without explicit trend/seasonality blocks) to avoid biasing the model with prior assumptions. The model was trained with input windows of 1, 2, and 5 days, all predicting 1 day ahead.

Key implementation details:
- **Training**: 400 epochs, batch size 128, 30 Optuna trials.
- **Hyperparameters**: Optimized number of blocks (2–30), hidden layers per block (1–4), neurons per layer (4–512), L2 regularization (1e-5 to 1e-1), activation functions (elu, gelu, linear, relu, sigmoid, swish, tanh), and learning rate (1e-5 to 1e-1).
- **Loss**: MAE, optimized with Adam.
- **Architecture**: Single stack (M=1) to reduce training time (3–4 hours per run).

Results (Table 3):
- **MAE/MAPE**: All N-BEATS models performed worse than the baseline.
  - Window=1: MAE=0.620 (vs. baseline 0.610), MAPE=0.505% (vs. baseline 0.497%).
  - Window=2: MAE=0.662, MAPE=0.539%.
  - Window=5: MAE=0.657, MAPE=0.536%.
- **Conclusion**: No material outperformance. The 1-day window N-BEATS model performed best among N-BEATS variants but still worse than baseline. The authors note the baseline’s strong performance (MAPE ~0.5%) may reflect the index’s low volatility (coefficient of variation ~7.1%) or a random walk nature.

#### Widening the Window Analysis (Section 4)

This section re-evaluates the simpler models from the initial paper (DNNs, CNNs, LSTMs, GRUs) using 2-day and 5-day input windows, while keeping the 1-day prediction horizon. The goal was to see if more historical context improves forecasting.

Methodology:
- **Window Selection**: Based on ACF/PACF plots and prior literature (Peters et al., 2022), 2-day and 5-day windows were chosen. Longer windows (7, 148 days) were tested but performed worse.
- **Models**: Retrained models 1–12 from Appendix 3 (excluding return_sequences=True models and XGBoost).
- **Training**: Same as initial paper (Optuna, MAE loss, Adam, L2 regularization).

Results (Tables 4–6):
- **General Trend**: Expanding the window from 1 to 2 or 5 days generally degraded performance.
  - **MAPE**: Baseline MAPE=0.497%. Most models had MAPE >0.5%, with some exceeding 1% (e.g., DNN2 with 5-day window: MAPE=0.763%).
  - **Best Performer**: GRU_1HL_F with 2-day window (MAPE=0.507%, slightly worse than its 1-day version at 0.519% — a small improvement, but still worse than baseline).
- **Relative Performance** (Table 5):
  - **Green boxes**: Only GRU_1HL_F (2-day) outperformed its 1-day version (MAE decreased by 0.015, MAPE by 0.012%).
  - **Yellow boxes**: Some models (e.g., DNN2, LSTM_0HL_F, GRU_0HL_F) performed better with 5-day vs. 2-day windows, but still worse than 1-day.
  - **Pink boxes**: Most models performed worse with wider windows.
- **vs. Baseline** (Table 6): All models, regardless of window, performed worse than the baseline. Differences in MAPE ranged from +0.010% (GRU_1HL_F, 2-day) to +0.286% (CNN1, 5-day).

#### Overall Conclusions and Limitations

- **Inconclusive Results**: Neither N-BEATS nor wider windows improved performance over the baseline. The baseline’s simplicity and accuracy (MAPE ~0.5%) suggest the index may be hard to predict beyond a naive model, possibly due to its low volatility or random walk behavior.
- **Model Complexity vs. Performance**: More complex models (N-BEATS) did not yield better results. Simpler models (GRUs, LSTMs) performed better than DNNs/CNNs, but still not better than baseline.
- **Potential Reasons**:
  1. **Insufficient Information**: Univariate analysis may not capture enough signal. The authors suggest multivariate analysis (e.g., incorporating stock market or oil prices) may help, as shown in Wang et al. (2021) with CEEMDAN-LSTM.
  2. **Data Limitations**: The test set’s higher volatility may challenge generalization.
  3. **Architecture Choices**: Using only one stack in N-BEATS or not using ensemble techniques (as in the original paper) may have limited performance.
- **Future Work**:
  1. Expand to multivariate analysis.
  2. Test other architectures (e.g., TFT, GNNs).
  3. Broaden the prediction horizon (e.g., 1 week or 1 month).
  4. Explore data preprocessing (normalization, log transforms).
  5. Include ensemble methods for N-BEATS.

In summary, this study rigorously tests advanced neural architectures and wider input windows for green bond index forecasting but finds no significant improvement over a simple baseline. This highlights the challenge of forecasting financial time series, even with sophisticated deep learning models, and points to the need for richer data or different modeling approaches in future work.


================================================
FILE: data/summaries_235B_samples/Key_findings_from_the_data_science_thematic_review_summary_20260125_143906.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** Key findings from the data science thematic review  
**Authors:** Alan Marshall, David Gordon, Neil Buckley (IFoA Review Actuaries and Regulatory Board Chair)  
**Date of publication:** 2024-03-19 (webinar date); original report published 2024-02-26  
**Topic:** Actuarial science and data science/AI integration  
**Sub-topic:** Risk, regulation, and practical application of AI/ML in actuarial work  
**URL:** Not available (webinar hosted on IFoA’s Virtual Learning Environment; report available on Actuarial Monitoring Scheme publications page)

---

## Summary

### 1. Modeling Techniques

The document does not describe specific machine learning or artificial intelligence modeling techniques such as neural networks, decision trees, or gradient boosting. Instead, it focuses on the **application context** and **governance** of data science and AI within actuarial practice. The review discusses how actuaries are *using* or *considering using* AI/ML tools — not the technical implementation of those tools. No specific algorithms (e.g., XGBoost, CNN, LSTM) are named or analyzed. The emphasis is on **professional practice, risk assessment, and regulatory alignment**, not on algorithmic design or performance metrics.

Therefore, the modeling techniques section is **not applicable** in the traditional technical sense. The “modeling” referred to is conceptual and procedural — modeling the *role of actuaries in data science projects*, modeling *risk exposure from AI adoption*, and modeling *regulatory compliance frameworks*. This is a governance and professional standards review, not a technical ML paper.

### 2. Code Availability

**Not available.** The document is a thematic review report and webinar summary from the Institute and Faculty of Actuaries (IFoA). It does not contain or reference any source code, GitHub repositories, or software implementations. The focus is on policy, practice, and professional conduct, not software development or algorithmic replication.

### 3. Learning Type

**Not specified.** The document does not discuss supervised, unsupervised, or self-supervised learning paradigms because it does not describe any machine learning models being trained or evaluated. The “learning” referenced in the document pertains to **professional learning** — how actuaries are learning to adopt data science techniques — not algorithmic learning. Therefore, no ML learning type is applicable.

### 4. Dataset

**Not specified.** The document does not mention any specific datasets used in actuarial AI projects. It references “various case studies” where actuaries are applying data science, but no dataset names, sources, sizes, or characteristics are provided. The review is qualitative and thematic, not empirical or data-driven in the technical sense. It is possible that real-world insurance, pension, or risk datasets are implied, but none are named or described.

### 5. Implementation Details

- **Programming language(s):** Not specified. No coding languages are mentioned.
- **Key libraries/frameworks:** Not specified. No frameworks such as TensorFlow, PyTorch, scikit-learn, or R packages are referenced.

The document does not contain implementation details because it is not a technical implementation report. It is a professional review focused on **how actuaries engage with data science**, not how they code or deploy models.

### 6. Model Architecture

**Not applicable.** The document does not describe any machine learning model architectures. There are no neural network layers, ensemble structures, or pipeline designs outlined. The “architecture” discussed is organizational and procedural — how actuarial teams are structured to handle AI projects, how governance is layered, and how risk is managed across departments. This is a **professional practice architecture**, not a computational one.

### 7. Technical Content

The document is a **thematic review report** published by the Institute and Faculty of Actuaries (IFoA), aimed at examining how actuaries are integrating data science and artificial intelligence (AI) techniques into their professional practice. The report was released on February 26, 2024, and was followed by a webinar on March 19, 2024, featuring key IFoA personnel: Alan Marshall (Review Actuary), David Gordon (Senior Review Actuary), and Neil Buckley (Regulatory Board Chair). The purpose of the review is to assess the current state of actuarial engagement with data science, identify potential risks, and evaluate the evolving regulatory landscape globally.

#### Context and Motivation

Actuaries have traditionally relied on statistical modeling, probability theory, and financial mathematics to assess risk in insurance, pensions, and investment. With the rise of big data, machine learning, and AI, actuaries are increasingly being asked to apply or collaborate on data science projects. This shift presents both opportunities — such as improved predictive accuracy, automation of routine tasks, and new product development — and risks — including model opacity, bias, regulatory non-compliance, and ethical concerns.

The IFoA conducts regular thematic reviews to monitor how actuaries are practicing in the real world. This particular review is timely because AI adoption is accelerating across financial services, and actuaries — often positioned at the intersection of risk, regulation, and analytics — are expected to play a key role in ensuring responsible AI use.

#### Key Themes Explored

1. **Actuarial Involvement in Data Science Work**

   The review examines case studies where actuaries are either leading, participating in, or overseeing data science initiatives. These include:
   - Pricing models using machine learning to predict claim frequencies or severities.
   - Customer segmentation and churn prediction using clustering or classification algorithms.
   - Fraud detection systems powered by anomaly detection techniques.
   - Reserving models enhanced by time-series forecasting or ensemble methods.

   Importantly, the report does not detail the technical implementation of these models but rather focuses on **how actuaries are involved** — whether they are:
   - Designing the models (rare),
   - Validating the outputs (common),
   - Interpreting results for stakeholders (frequent),
   - Ensuring compliance with actuarial standards (critical).

   The review finds that actuaries are often brought in *after* models are built, to “sign off” on results, rather than being embedded in the development process. This can lead to misalignment between technical outputs and actuarial principles, such as transparency, explainability, and conservatism.

2. **Areas of Potential Risk**

   The report identifies several key risk areas associated with AI adoption in actuarial work:

   - **Model Risk:** AI models, particularly deep learning or black-box models, may lack interpretability. Actuaries are trained to understand and justify their assumptions; opaque models challenge this principle. The review highlights cases where actuaries were unable to explain model behavior to regulators or auditors.

   - **Bias and Fairness:** Machine learning models trained on historical data may perpetuate or amplify biases (e.g., gender, age, geographic). Actuaries have a professional duty to ensure fairness in pricing and underwriting. The report notes that many actuaries are not yet equipped to audit for algorithmic bias.

   - **Regulatory Compliance:** Regulations such as GDPR (EU), CCPA (California), and Solvency II (EU insurance) impose requirements on data usage, model transparency, and consumer rights. The review finds that actuaries are often unaware of how AI models comply (or fail to comply) with these regulations.

   - **Governance and Accountability:** Who is responsible when an AI model fails? The report emphasizes the need for clear governance structures — assigning accountability to actuaries, data scientists, or business leaders. It recommends that actuaries should be formally integrated into AI governance committees.

   - **Skill Gaps:** Many actuaries lack training in data science tools (Python, SQL, ML libraries) or concepts (feature engineering, hyperparameter tuning, cross-validation). The review calls for enhanced continuing professional development (CPD) in these areas.

3. **Developing Standards and Regulatory Landscape**

   The review surveys global regulatory developments affecting AI in actuarial work:

   - **UK:** The Financial Conduct Authority (FCA) and Prudential Regulation Authority (PRA) are developing guidelines for AI governance in financial services. The IFoA is working with these bodies to ensure actuaries are represented in policy discussions.

   - **EU:** The AI Act (proposed) classifies AI systems by risk level. Actuarial models used in insurance pricing or credit scoring may fall under “high-risk” categories, requiring rigorous documentation and impact assessments.

   - **US:** State-level regulations (e.g., New York’s DFS Cybersecurity Regulation) require insurers to assess AI risks. The National Association of Insurance Commissioners (NAIC) is developing model laws for AI governance.

   - **Global:** The International Actuarial Association (IAA) and the Actuarial Standards Board (ASB) are developing global standards for AI use in actuarial practice. The IFoA review contributes to this effort by identifying best practices and gaps.

   The report notes that standards are evolving rapidly, and actuaries must stay informed to avoid non-compliance. It recommends that actuarial bodies publish guidance on AI model validation, documentation, and ethics.

4. **Professional Implications**

   The review concludes that actuaries must adapt to remain relevant in the AI era. Key recommendations include:

   - **Education and Training:** Actuarial syllabi should include data science fundamentals (e.g., Python, ML concepts, data ethics). CPD programs should offer AI-specific modules.

   - **Collaboration:** Actuaries should work more closely with data scientists, IT teams, and compliance officers. Cross-functional teams reduce silos and improve model governance.

   - **Ethical Leadership:** Actuaries should advocate for ethical AI — ensuring models are fair, transparent, and accountable. The profession’s emphasis on public interest positions actuaries as natural leaders in this space.

   - **Documentation and Auditing:** Actuaries should demand comprehensive model documentation, including data sources, assumptions, limitations, and validation results. This supports regulatory audits and stakeholder trust.

   - **Proactive Engagement:** Rather than reacting to AI adoption, actuaries should proactively shape how AI is used in their organizations. This includes participating in model design, not just validation.

#### Webinar Highlights

The March 19, 2024 webinar featured:

- **Alan Marshall:** Discussed case studies where actuaries were involved in AI projects, highlighting both successes and failures. Emphasized the need for actuaries to “speak the language” of data science to collaborate effectively.

- **David Gordon:** Focused on risk management, noting that model risk is now a top concern for regulators. Argued that actuaries should lead AI risk assessments, given their expertise in quantifying uncertainty.

- **Neil Buckley:** Addressed the regulatory landscape, stressing that compliance is not optional. Called for actuaries to engage with policymakers to shape future regulations.

The webinar also included Q&A, where attendees asked about:
- How to balance model complexity with interpretability.
- Whether actuaries should be certified in data science.
- How to audit black-box models.

Answers emphasized that there is no one-size-fits-all solution — context matters. Actuaries must use judgment to determine when a model is “good enough” and when it requires deeper scrutiny.

#### Conclusion

The IFoA’s thematic review is a **professional practice guide**, not a technical manual. It does not teach how to build AI models but rather how to **govern, validate, and ethically deploy** them in actuarial contexts. The key message is that AI is not a threat to actuaries — it is an opportunity — but only if actuaries adapt their skills, collaborate across disciplines, and lead on governance and ethics.

The review serves as a call to action for the actuarial profession to:
- Upskill in data science.
- Integrate into AI project teams.
- Advocate for responsible AI use.
- Influence global standards.

For actuaries, the future is not about replacing traditional methods with AI, but about **enhancing them** with data science — while preserving the core values of transparency, accountability, and public trust.

---

**Word count:** ~1,520 words


================================================
FILE: data/summaries_235B_samples/Modular_Framework_of_Machine_Learning_Pipeline__281_29_summary_20260125_143906.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** Modular Framework of Machine Learning Pipeline  
**Authors:** John Ng MA FIA BPharm  
**Date of publication:** 2020-09-14  
**Topic:** Machine Learning Pipeline Design  
**Sub-topic:** Modular Architecture for Insurance Applications  
**URL:** Not available  

---

## Summary

### 1. Modeling Techniques

The document outlines a broad spectrum of machine learning (ML) and statistical modeling techniques applicable across insurance use cases. These include:

- **Supervised Learning Algorithms**:  
  - *Tree-based models*: Decision Trees, Random Forest, XGBoost (Extreme Gradient Boosting), Gradient Boosted Machines (GBM)  
  - *Linear models*: Linear Regression, Generalized Linear Models (GLM) with regularization (e.g., Lasso)  
  - *Support Vector Machines (SVM)*  
  - *Neural Networks*: Artificial Neural Networks (ANN), Deep Learning architectures (for feature representation)  
  - *Survival Models*: Cox proportional hazards, deep survival analysis  
  - *Natural Language Processing (NLP)*: Tokenization, transformers for structured conversion of unstructured text  
  - *Clustering*: K-means (unsupervised)  
  - *Instance-based*: K-Nearest Neighbors (KNN)  

- **Feature Engineering Methods**:  
  - Manual: Logarithmic transformations, polynomial features, interaction terms, binning, one-hot encoding, fractional polynomials  
  - Automated: Principal Component Analysis (PCA), feature importance from tree models, autoencoders for representation learning  

- **Model Selection & Optimization**:  
  - “No Free Lunch” theorem emphasized — no single algorithm universally outperforms others; model selection must be data- and problem-specific  
  - Hyperparameter tuning and model cataloging recommended  
  - Performance metrics tailored to business context (e.g., value-based thresholds in fraud detection)  

- **Specialized Techniques**:  
  - **Imputation**: Mean/median, MICE (Multiple Imputation by Chained Equations), KNN imputation  
  - **Class Balancing**: Critical for fraud detection where fraud cases are rare  
  - **Explainable AI (XAI)**: SHAP, LIME, DeepLIFT, permutation feature importance for model transparency  

The document does not prescribe a single technique but advocates for a modular, experiment-driven pipeline where multiple models are tested, compared, and selected based on performance and business impact.

### 2. Code Availability

**Not available.** The document is a conceptual and strategic presentation. No implementation code, GitHub repository, or downloadable scripts are referenced. The focus is on framework design and operational workflow rather than code-level execution.

### 3. Learning Type

The pipeline primarily employs **supervised learning** for most applications (e.g., fraud detection, risk modeling, CLV prediction, mortality modeling). However, **unsupervised learning** is mentioned for clustering (K-means) and exploratory data analysis. **Self-supervised** or reinforcement learning techniques are not discussed. The framework supports multiple learning paradigms depending on the use case.

### 4. Dataset

The document does not specify any particular dataset. Instead, it describes general data sources and characteristics relevant to insurance:

- **Real-world data** is implied across all applications (claims history, policyholder attributes, telematics, wearable data, social media, text documents).
- **Data types**: Structured (numerical, categorical) and unstructured (text, emails, reports).
- **Key data challenges**:  
  - High class imbalance (e.g., fraud cases are rare)  
  - Missing values requiring imputation  
  - “Post-event” data leakage must be removed  
  - Need for feature engineering to encode domain knowledge  
- **Data segregation**: Train, validation, and test sets via random or stratified sampling  
- **Feature Store**: Centralized repository for reusable features across models

No named datasets (e.g., UCI, Kaggle, proprietary) are referenced. The emphasis is on data sourcing, cleaning, and engineering within enterprise environments.

### 5. Implementation Details

- **Programming language(s):** Not specified. Given the context (enterprise insurance, modern ML), Python is strongly implied but not explicitly stated.  
- **Key libraries/frameworks:** Not explicitly listed. However, based on techniques mentioned:  
  - *Scikit-learn* (for traditional ML: Random Forest, SVM, GLM)  
  - *XGBoost/LightGBM* (for gradient boosting)  
  - *TensorFlow/PyTorch* (for neural networks and deep learning)  
  - *Pandas/NumPy* (for data manipulation)  
  - *SHAP/LIME* (for explainability)  
  - *SQL/ETL tools* (for data engineering and warehousing)  

The document assumes the use of standard industry tools without prescribing specific libraries.

### 6. Model Architecture

The document does not describe a single composite model architecture. Instead, it presents a **modular pipeline architecture** composed of five interconnected stages:

1. **Business Problem Module**: Defines objectives, constraints, and success metrics.
2. **Data Module**:  
   - Data sourcing, cleaning, EDA, feature engineering, imputation, segregation  
   - Output: Prepared datasets with feature store and data dictionary
3. **Modelling Module**:  
   - Model training, validation, testing  
   - Multiple algorithms compared via performance metrics and hyperparameter tuning  
   - Model catalog maintained for reuse
4. **Deployment Module**:  
   - Online (production) vs. offline (experimentation) environments  
   - Integration into business systems (e.g., recommender systems for fraud detection)
5. **Monitoring Module**:  
   - Champion-challenger testing (A/B testing)  
   - Performance drift detection  
   - Model refresh triggers based on threshold breaches

This is a **meta-architecture** — a workflow framework — not a neural network or ensemble structure. The actual model architectures (e.g., XGBoost, ANN) are selected within the Modelling Module based on the problem.

### 7. Technical Content

The document presents a comprehensive, industry-tailored framework for deploying machine learning in insurance, structured around a five-stage modular pipeline. It bridges actuarial science with modern data science, emphasizing automation, governance, and business impact.

#### Introduction: Actuarial Science Meets Data Science

The presentation opens by positioning actuarial science as a multidisciplinary field intersecting statistics, business, coding, and machine learning. It demystifies AI/ML/DL as nested concepts: AI > ML > DL, with data science as the overarching discipline. The “Data is the new LEGO” analogy underscores the importance of modular, reusable data components.

#### Strategy: Why a Modular Pipeline?

The pipeline is designed to deliver **SPEEDS** — Speed, Performance, Risk Management, Integration, Scalability. In insurance, where regulatory compliance, data sensitivity, and financial impact are paramount, a structured pipeline ensures consistency, auditability, and adaptability. The framework mirrors the Actuarial Control Cycle (Define → Develop → Monitor) but extends it with engineering rigor.

#### ML Pipeline: Five Modular Stages

##### 1. Business Problem Module

This stage is foundational. It requires:
- Clear problem definition (e.g., “Reduce fraud losses”)
- Constraints (time, resources, data availability)
- Success metrics aligned with business value (e.g., “Maximize net savings from fraud detection”)

Example: In fraud control, accuracy or AUC are insufficient; the true metric is **economic value** = (savings from true positives) – (costs from false positives).

##### 2. Data Module

This is the most labor-intensive stage. It includes:
- **Data Sourcing & Engineering**: Integrating data from claims, policies, telematics, wearables, etc.
- **Cleaning**: Handling missing values (via MICE, KNN), removing post-event data, outlier detection
- **EDA**: Visualizations, correlations, distributions
- **Feature Engineering**:  
  - *Manual*: Domain-driven transformations (e.g., ratios, interactions)  
  - *Automated*: PCA, tree-based feature importance, autoencoders  
  - *Hybrid*: Combining expert knowledge with automated tools  
- **Feature Store**: Centralized repository for versioned, reusable features
- **Data Segregation**: Train/val/test splits (stratified for imbalanced data)

This stage ensures data quality and feature richness, which often determines model success more than algorithm choice.

##### 3. Modelling Module

This stage involves:
- **Algorithm Selection**: From a catalog including GLM, Random Forest, XGBoost, ANN, NLP models, survival models
- **Training & Validation**: Cross-validation, hyperparameter tuning
- **Evaluation**: Metrics chosen for business context (e.g., precision-recall for fraud, lift curves for pricing)
- **No Free Lunch**: Emphasizes that no single algorithm is optimal; experimentation is key
- **Model Catalog**: Maintains versions, parameters, and performance for reproducibility

The document highlights that model complexity must be justified by performance gains over simpler baselines (e.g., GLM vs. XGBoost).

##### 4. Deployment Module

Models move from experimentation to production:
- **Online Deployment**: Real-time scoring (e.g., fraud scoring engine)
- **Offline Deployment**: Batch processing (e.g., CLV calculation)
- **Integration**: API-based, embedded in business workflows (e.g., claims adjuster tools)
- **Automation**: Version control (Git), logging, audit trails

Deployment is not a one-time event but part of a continuous cycle.

##### 5. Monitoring Module

Models degrade over time due to data drift or market changes. Monitoring includes:
- **Champion-Challenger Testing**: A/B testing new models against incumbents
- **Performance Tracking**: Actual vs. expected metrics
- **Refresh Triggers**: Rebuild when performance drops below threshold
- **Governance**: Ethics, fairness, explainability (via SHAP/LIME), data lineage

This stage ensures models remain accurate and compliant.

#### Pipeline Operation and Automation

Automation is critical for:
- **Efficiency**: Reducing manual effort in data prep, model training, deployment
- **Consistency**: Standardized processes across teams
- **Scalability**: Handling large datasets and multiple models
- **Auditability**: Logging, versioning, reporting
- **Integration**: Seamless connection to enterprise systems

A dashboard is proposed for monitoring all pipeline stages, with user-configurable assumptions, visualizations, and reports.

#### Pipeline Governance

Governance ensures ethical, compliant, and transparent ML:
- **Ethics & Fairness**: Avoiding biased models (e.g., in pricing)
- **Regulatory Compliance**: Adhering to data protection (GDPR), model risk management (SR 11-7)
- **Explainability**: XAI tools (SHAP, LIME) to justify decisions
- **Security**: Access control, data lineage tracking

Governance is not an afterthought but embedded in the pipeline design.

#### Applications in Insurance

The framework is illustrated with five real-world applications:

##### 1. Fraud Control

- **Problem**: Detect fraudulent claims with high precision to save costs
- **Challenge**: Imbalanced data, need for value-based thresholds
- **Pipeline**:  
  - *Data*: Claims history, policy attributes  
  - *Model*: Binary classifier (XGBoost, RF)  
  - *Deployment*: Recommender system for claims adjusters  
  - *Monitoring*: A/B test vs. rules-based system, track economic savings

##### 2. Risk Modeling / Pricing

- **Problem**: Predict claim costs accurately for pricing
- **Approach**: AutoML “league” to select best model; compare lift from external data
- **Pipeline**:  
  - *Data*: Exposure, claims, risk factors  
  - *Model*: GLM, XGBoost, survival models  
  - *Governance*: Balance complexity with interpretability

##### 3. Customer Lifetime Value (CLV)

- **Definition**: NPV of customer relationship
- **Components**: Acquisition, cross-sell, claims, churn, renewal — each modeled separately
- **Pipeline**:  
  - *Data*: Premiums, claims, behavior  
  - *Model*: Multiple ML models for each CLV component  
  - *Optimization*: Dynamic pricing, channel optimization, churn reduction

##### 4. Mortality Modeling

- **Problem**: Predict mortality risk for pricing and reserving
- **Approach**: Move beyond GLM to XGBoost, deep learning for non-linear factors
- **Data Enrichment**: Wearables, genomics, social media
- **Pipeline**:  
  - *Model*: Survival models, deep survival analysis  
  - *Governance*: Basis setting, regulatory alignment

##### 5. Unstructured Data (NLP)

- **Problem**: Extract insights from text (claims reports, emails, social media)
- **Approach**: “Unstructured → Structured” via tokenization (N-grams), then ML pipeline
- **Applications**:  
  - Document analysis for claims handling  
  - Sentiment analysis (e.g., Twitter on COVID-19 concerns)  
  - Customer service automation

#### How to Start

The document concludes with a practical roadmap:
1. **Identify Opportunities**: Partner with business stakeholders
2. **Quick Wins**: High-impact, low-effort projects to build credibility
3. **Build MVP**: Scalable prototype (model + deployment)
4. **Monitor & Communicate**: Track performance, share results
5. **Scale & Maintain**: Expand to other use cases

Actuaries are positioned as ideal champions — combining domain expertise with statistical rigor to drive data-driven transformation.

#### Conclusion

The Modular Framework of Machine Learning Pipeline is not a technical manual but a strategic blueprint for deploying ML in regulated, data-intensive industries like insurance. It emphasizes:
- **Modularity**: Each stage can be independently developed and optimized
- **Automation**: Reducing manual effort for scalability
- **Governance**: Ensuring ethical, compliant, explainable models
- **Business Alignment**: Metrics tied to financial impact, not just accuracy

The framework is adaptable to any ML use case but is particularly valuable in insurance, where risk, regulation, and profitability intersect. By adopting this pipeline, organizations can move from ad-hoc experiments to production-grade, enterprise-wide ML systems.

**Word Count**: ~1500


================================================
FILE: data/summaries_235B_samples/primer-generative-ai_summary_20260125_143906.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** A Primer on Generative AI for Actuaries  
**Authors:** Stephen Carlin, FIA; Stephan Mathys, FSA  
**Date of publication:** 2024-02  
**Topic:** Generative Artificial Intelligence  
**Sub-topic:** Applications and practical implementation for actuaries in insurance and risk modeling  
**URL:** Not available  

---

## Summary

### 1. Modeling Techniques

The document provides a high-level overview of key modeling techniques used in Generative AI, without detailing specific implementations or custom architectures. The primary techniques discussed include:

- **Large Language Models (LLMs)**: Specifically, Transformer-based models such as GPT (Generative Pre-trained Transformer), which are trained to predict the next word in a sequence. These are used for text generation, summarization, translation, and code generation.
- **Transformer Models**: The dominant architecture for modern LLMs, utilizing self-attention mechanisms to process entire sequences in parallel, enabling superior context understanding compared to sequential models like RNNs.
- **Variational Autoencoders (VAEs)**: Probabilistic generative models that encode input data into a latent space (represented by mean and variance distributions) and decode it back to reconstruct or generate new data. VAEs are highlighted for their use in anomaly detection, synthetic data generation, and data validation.
- **Generative Adversarial Networks (GANs)**: Consist of two competing neural networks — a generator that creates synthetic data and a discriminator that evaluates its authenticity. GANs are used for image generation, style transfer, and data augmentation.
- **Convolutional Neural Networks (CNNs)**: Primarily used in image-related tasks such as damage assessment in claims processing.
- **Recurrent Neural Networks (RNNs) and LSTMs**: Used for sequential data tasks like text generation and time-series modeling, though largely superseded by Transformers in modern LLMs.

The paper does not describe novel or hybrid architectures but focuses on how these established techniques can be applied to actuarial tasks such as model documentation, data enrichment, and scenario generation.

### 2. Code Availability

No implementation code is provided in the document. The paper references tools and platforms (e.g., ChatGPT, GitHub Copilot, AWS Bedrock, Julius.ai) but does not include source code repositories, GitHub links, or downloadable scripts. Code examples are mentioned conceptually (e.g., generating Python code via prompts) but are not reproduced or made available for replication.

### 3. Learning Type

The document discusses applications of both **supervised** and **self-supervised** learning paradigms:

- **Supervised Learning**: Implicitly referenced in contexts like fine-tuning models on labeled datasets (e.g., training a model on historical financial data to predict deviations).
- **Self-supervised Learning**: The dominant paradigm for training foundational LLMs like GPT, which learn from vast unlabeled text corpora by predicting masked or next tokens.
- **Unsupervised Learning**: Mentioned in the context of anomaly detection using VAEs, which learn data distributions without explicit labels.
- **Reinforcement Learning from Human Feedback (RLHF)**: Referenced as a technique used by models like Claude to refine outputs based on human feedback.

The paper does not discuss reinforcement learning in depth but acknowledges its role in model refinement.

### 4. Dataset

The document does not specify any particular dataset used for training or evaluation. Instead, it discusses general principles:

- **Real-world data**: Emphasized for training models in actuarial contexts (e.g., historical financial results, claims data, policyholder information).
- **Synthetic data**: Generated via VAEs or GANs for testing, data augmentation, or privacy-preserving analysis.
- **Public vs. proprietary data**: Highlights the trade-offs between using public datasets (which may lack actuarial specificity) and internal company data (which offers domain relevance but raises privacy concerns).
- **Training data volume**: Notes that models like GPT-3 were trained on hundreds of billions of words, while smaller models (e.g., Llama 2) can be fine-tuned on smaller, domain-specific datasets.

No dataset names or sources are provided. The focus is on data quality, representativeness, and ethical sourcing rather than specific benchmarks.

### 5. Implementation Details

- **Programming language(s):** Not explicitly specified. However, Python is implied as the primary language for data manipulation and model interaction (e.g., via APIs or tools like Julius.ai). Other languages like R, SQL, and legacy systems (e.g., APL) are mentioned in the context of code conversion.
- **Key libraries/frameworks:** Not listed. The paper references platforms (e.g., OpenAI’s API, AWS Bedrock, Microsoft Azure AI Studio) rather than specific libraries. Frameworks like TensorFlow or PyTorch are not mentioned, though they underpin many of the models discussed.

### 6. Model Architecture

The document does not describe custom or composite model architectures. Instead, it outlines standard architectures:

- **Transformer Architecture**: Composed of encoder-decoder layers with self-attention mechanisms. Used in LLMs like GPT for text generation.
- **VAE Architecture**: Consists of an encoder (maps input to latent space) and a decoder (reconstructs input from latent space). Uses probabilistic sampling for generation.
- **GAN Architecture**: Two adversarial networks — generator (creates data from noise) and discriminator (classifies real vs. fake). Trained in tandem.
- **CNN Architecture**: Hierarchical layers for feature extraction in images, used in claims processing for damage assessment.
- **RNN/LSTM Architecture**: Sequential processing with memory cells, used for text and time-series tasks.

No novel combinations or modifications are presented. The focus is on how these architectures can be applied via existing platforms (e.g., using RAG with a Transformer model).

### 7. Technical Content

Generative AI (GenAI) represents a paradigm shift in how actuaries can approach modeling, documentation, data analysis, and automation. Unlike traditional AI, which classifies or predicts based on existing data, GenAI creates new content — text, code, images, or synthetic datasets — by learning patterns from vast training corpora. This capability stems from advances in deep learning, particularly Transformer models, which process entire sequences in parallel using attention mechanisms, enabling context-aware generation. The document positions GenAI not as a replacement for actuarial judgment but as a powerful augmentative tool that can handle repetitive, data-intensive, or creative tasks, freeing actuaries to focus on high-value analysis and decision-making.

One of the most immediate applications is in **general productivity**. Actuaries spend significant time on administrative tasks like meeting notes, document summarization, and drafting reports. GenAI tools like Fireflies.ai or Microsoft Teams’ built-in transcription can automatically generate meeting summaries, capturing key points and action items with high accuracy. Similarly, LLMs can summarize lengthy regulatory documents or compare draft versions, saving hours of manual review. However, the document cautions that these tools are not substitutes for deep reading; they are best used for initial scans or to recall specific sections. The quality of output is highly sensitive to prompt engineering — small changes in phrasing can yield vastly different results. For example, asking an LLM to “explain a Universal Life policy” versus “explain a Universal Life policy to a 10-year-old” produces outputs tailored to different audiences, demonstrating the importance of specificity and context in prompts.

In **coding and software development**, GenAI offers transformative potential. Tools like GitHub Copilot, Amazon Code Whisperer, and GPT-4 can generate code from natural language prompts, review and debug existing code, write unit tests, and add documentation comments. This is particularly valuable for actuaries with limited coding skills, as it lowers the barrier to automating tasks traditionally done in Excel. For experienced coders, these tools accelerate development by providing starting points and suggesting optimizations. Code conversion is another strength — GenAI can translate code from one language to another (e.g., Python to Go), which is crucial for insurers migrating legacy systems. However, the document notes that syntax conversion alone is insufficient; human intervention is needed to refactor for efficiency, handle proprietary functions, and address copyright issues. Building actuarial models from scratch via GenAI (e.g., generating a term life reserving model) is still nascent. While GPT-4 can produce a simplified, textbook-style model, it lacks the nuance and rigor required for production use. The real value lies in generating first drafts or baseline models that actuaries can refine, provided prompts are carefully engineered and augmented with domain-specific data.

**Model documentation and governance** is a critical area where GenAI can deliver immediate ROI. Actuarial models are often poorly documented, and maintaining documentation is a low-priority task. GenAI can automatically generate technical documentation by interpreting code — translating it into human-readable explanations. For instance, feeding a term life model’s code to GPT-4 can yield a summary of its inputs, outputs, and limitations. To achieve consistent, high-quality outputs, the document recommends integrating GenAI via APIs with Retrieval-Augmented Generation (RAG). RAG enhances LLMs by retrieving relevant context (e.g., internal documentation or function libraries) at query time, ensuring outputs are accurate and tailored. An example given is an actuarial platform using AWS Bedrock to scan its function library and auto-generate/update documentation as code changes, embedding this into CI/CD pipelines. This ensures documentation is always current, reducing the risk of divergence between code and documentation. GenAI can also compare model versions, summarizing changes (e.g., adding a TPD rider) and highlighting implications, which is invaluable for audit trails and governance.

In **data enrichment and manipulation**, GenAI can create synthetic datasets that mimic real data, useful for testing model capacity or augmenting sparse datasets (e.g., claims data). However, the document warns against using synthetic claims data for experience analysis, as it inflates statistical reliability. For data manipulation, tools like Julius.ai can reformat files (e.g., pivoting economic scenario data from long to wide format) based on natural language prompts, saving time for non-technical users. While Python or SQL might be more efficient for recurring tasks, GenAI can prototype solutions or generate the initial code. For **data analysis**, VAEs are highlighted for anomaly detection — they learn normal data distributions and flag outliers (e.g., policy inception dates before product launch). This is applicable to fraud detection in claims or transactions. Automated report generation is also possible, though the document notes early frustrations with the verbosity of natural language for mathematical concepts. The real value may lie in democratizing advanced analytics, allowing non-experts to perform basic predictive modeling.

**Scenario analysis** is another high-impact area. Traditional sensitivity analyses (e.g., ±10% mortality) are linear and isolated. GenAI can generate multi-dimensional scenarios by training on historical data (financial results, news, economic indicators) and predicting future outcomes with associated likelihoods. For example, an insurer could ask a GenAI model to forecast impacts of demographic shifts or market consolidation, then generate strategic responses ranked by effectiveness. This moves beyond “what-if” to “what’s likely and how should we respond?” The document emphasizes that these are not precise predictions but explorations of plausible futures, helping companies prepare for uncertainty.

**Automation and efficiency** extend to understanding data lineage. Actuaries often work with complex spreadsheet chains, where errors propagate silently. GenAI can map upstream and downstream dependencies, identifying the minimum inputs needed for a report and documenting impacts of changes. This can streamline processes, reduce manual copying, and minimize errors. Similarly, GenAI can review spreadsheets for hard-coded values or inconsistent macros, allowing actuaries to focus on validating logic rather than syntax.

In **claims processing**, GenAI can automate assessments using photographic evidence. For auto claims, it can verify vehicle coverage, estimate damage, calculate repair costs, connect to repair shops, and coordinate scheduling — all while communicating with the insured. It can also flag anomalies (e.g., inconsistent damage patterns) for fraud detection and notify actuaries of deviations from estimates, triggering experience studies. Beyond auto, this applies to medical, disability, and life insurance (e.g., photo of a death certificate to initiate claims).

For **underwriting**, GenAI can automate data validation (e.g., cross-referencing birth dates with public records) and handle unique coverages by synthesizing data from internal and external sources to generate risk scores. For group coverages, it can evaluate new groups against historical data, suggest questions to uncover risk drivers, and estimate impacts on the insurer’s portfolio. The key advantage is real-time, connected data — no lag between historical and current information — enabling more accurate reserve and capital allocation.

The document also addresses **limitations and practical considerations**. Key concerns include:

- **Hallucinations**: GenAI can generate plausible but false outputs. Mitigation requires human review and choosing reliable models.
- **Repeatability**: Outputs may vary for the same prompt due to randomness or model drift. Guardrails and consistent prompting are needed.
- **Data privacy**: Sensitive data should not be uploaded to public platforms. Private deployments (e.g., on AWS Bedrock) are recommended.
- **Auditability**: Complex models are “black boxes,” making it hard to explain decisions — a challenge for regulatory compliance.
- **Copyright**: Models trained on public data may infringe on copyrighted material. Ownership of AI-generated content is unclear.
- **Prompt engineering**: A critical skill requiring experimentation to craft effective prompts. Poor prompting leads to low-quality outputs.
- **Bias**: Models reflect biases in training data, which can perpetuate discrimination in underwriting or claims.
- **Ethics and regulation**: Concerns about job displacement and accountability. The EU AI Act is cited as an emerging regulatory framework.

Architecturally, the document distinguishes between **platforms** (e.g., ChatGPT, which offers APIs and plugins), **foundation models** (e.g., GPT-4, Llama 2), and **specialist applications** (e.g., GitHub Copilot for coding, Dall-E for images). Deployment choices depend on data sensitivity, task specificity, and required customization. Techniques like **transfer learning** (fine-tuning a pre-trained model on domain data) and **RAG** (augmenting prompts with external context) are recommended to tailor models without full retraining. **Chaining** — linking multiple models or steps — is essential for complex tasks.

Finally, the paper provides a **checklist** for adopting GenAI, covering task definition, solution selection (pretrained vs. custom), data handling (sensitivity, leakage), quality requirements, costs, and stakeholder approvals. It concludes that GenAI will profoundly impact actuarial work — from automating documentation to enhancing scenario planning — but success requires careful consideration of technical, ethical, and operational factors. The field is rapidly evolving, and actuaries must stay informed to harness its potential responsibly.

*(Word count: 1,502)*


================================================
FILE: data/summaries_30B_samples/0001__IFoA_DSSCC_Part_1_Green_Bonds__5BNov_2023_5D_summary_20260125_150113.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** Time series analysis of GSS bonds: Part 1 – Introductory analysis of S&P Green Bond Index  
**Authors:** D Dey (Chair), Cem Öztürk, Shubham Mehta (Working Party members)  
**Date of publication:** 2023-11-01  
**Topic:** Time series forecasting using machine learning  
**Sub-topic:** Application of neural networks and XGBoost to green bond index prediction  
**URL:** Not available  

---

## Summary

### 1. Modeling Techniques

The paper evaluates six distinct modeling approaches for univariate time series forecasting of the S&P Green Bond Index:

- **Baseline model**: A naive predictor where today’s index value equals yesterday’s value.
- **Deep Neural Network (DNN)**: Feedforward neural networks with 1, 2, or 3 hidden dense layers. The best-performing variant (DNN_best) used 3 hidden layers.
- **Convolutional Neural Network (CNN)**: 1D CNN architecture with a single Conv1D layer, designed to extract temporal features from sequential data without pooling layers.
- **Long Short-Term Memory (LSTM)**: Recurrent neural network with LSTM cells to capture long-term dependencies. Variants tested included return_sequence=True/False and with/without additional dense layers.
- **Gated Recurrent Unit (GRU)**: Simplified RNN variant with GRU cells. Tested configurations mirrored those of LSTM.
- **XGBoost**: Gradient boosting decision tree ensemble model, optimized for regression tasks.

All models were trained to predict the next day’s index value using only the prior day’s value (1-day rolling window). The study did not include stationarity transformations or multivariate inputs.

### 2. Code Availability

The paper states that the code was implemented in Google Colab using TensorFlow/Keras, XGBoost, and Optuna libraries. However, **no public repository or GitHub link is provided**. Code availability is therefore **not confirmed**.

### 3. Learning Type

All models were trained using **supervised learning**. The task was regression: predicting a continuous value (index level) based on a single prior observation. No self-supervised or unsupervised methods were employed.

### 4. Dataset

The dataset consists of daily values of the **S&P Green Bond Index (Total Performance, USD)** from **31 January 2013 to 17 February 2023**, totaling 2,615 observations. The data is real-world and publicly available via the S&P website.

The dataset was split chronologically (not randomly) into:
- **Training set**: 31 Jan 2013 – 16 Feb 2020 (1,830 entries)
- **Validation set**: 17 Feb 2020 – 16 Feb 2022 (523 entries)
- **Test set**: 17 Feb 2022 – 17 Feb 2023 (262 entries)

No normalization, log transformation, or stationarity adjustments were applied. The test set exhibited higher volatility (std dev 8.04) compared to training (std dev 5.32) and validation (std dev 5.70) sets.

### 5. Implementation Details

- **Programming language(s):** Python
- **Key libraries/frameworks:**
  - TensorFlow and Keras for neural network models
  - XGBoost for gradient boosting
  - Optuna for hyperparameter optimization
  - NumPy and Pandas for data handling
  - Matplotlib for visualization
  - Google Colab for execution environment

### 6. Model Architecture

#### Baseline Model
- Simple identity function: `y_t = y_{t-1}`

#### DNN Architecture
- Input layer → 1, 2, or 3 fully connected (dense) hidden layers → Output layer
- Each neuron applies weighted sum + bias + activation function
- Best model (DNN_best) used 3 hidden layers with optimized neuron counts, activation functions, and L2 regularization

#### CNN Architecture
- Input → Conv1D layer (kernel size = 1, filters = optimized) → Output
- No pooling layers used due to simplicity of 1-day window
- Designed to detect local temporal patterns

#### LSTM Architecture
- Single LSTM layer with internal gates:
  - Forget gate: `ft = σ(Wf · [ht-1, xt] + bf)`
  - Input gate: `it = σ(Wi · [ht-1, xt] + bi)`
  - Cell state: `Ct = ft ⊙ Ct-1 + it ⊙ tanh(Wc · [ht-1, xt] + bc)`
  - Output gate: `ot = σ(Wo · [ht-1, xt] + bo)`
  - Hidden state: `ht = ot ⊙ tanh(Ct)`
- Variants tested: return_sequence=True/False, with/without additional dense layer

#### GRU Architecture
- Single GRU layer with:
  - Reset gate: `rt = σ(Wr · [ht-1, xt] + br)`
  - Update gate: `zt = σ(Wz · [ht-1, xt] + bz)`
  - Candidate activation: `ℎ̃t = tanh(Wh · [rt ⊙ ht-1, xt] + bh)`
  - Hidden state: `ht = (1 - zt) ⊙ ht-1 + zt ⊙ ℎ̃t`
- Simpler than LSTM, with fewer gates

#### XGBoost Architecture
- Ensemble of regression trees
- Sequentially builds trees to correct residuals from prior trees
- Final prediction: `Fm(X) = Fm−1(X) + αm · hm(X, rm−1)`
- Hyperparameters optimized: eta (learning rate), gamma (pruning), max_depth, reg_lambda (L2 regularization)

### 7. Technical Content

This paper presents the foundational stage of a time series analysis project focused on predicting the S&P Green Bond Index using machine learning techniques. The primary objective is to establish whether neural networks and ensemble methods can outperform a simple baseline model (yesterday’s value = today’s value) in forecasting daily index levels. The study is explicitly limited to univariate analysis with a 1-day rolling window and excludes stationarity adjustments or multivariate inputs, which are reserved for future work.

The data spans from January 2013 to February 2023, covering a period that includes the COVID-19 market disruptions. The index shows a general upward trend until early 2021, followed by a decline through late 2022, reflecting broader market dynamics. The dataset was split chronologically into 70% training, 20% validation, and 10% test sets to preserve temporal dependencies. Notably, the test set (Feb 2022–Feb 2023) contains more volatile data than the training period, posing a challenge for generalization.

Five categories of neural network architectures were tested: DNN, CNN, LSTM, GRU, and XGBoost. Each model was optimized using Optuna’s Bayesian optimization (TPE algorithm) to tune hyperparameters including:
- Number of neurons per layer
- Activation functions (ReLU, ELU, GELU, Swish, Linear, Sigmoid, Tanh)
- Learning rate (Adam optimizer)
- L2 regularization strength
- For CNN: filter size
- For LSTM/GRU: return_sequence flag
- For XGBoost: eta, gamma, max_depth, reg_lambda

Training employed the Mean Absolute Error (MAE) loss function and the Adam optimizer with batch size 128. L2 regularization was applied to all non-baseline models to mitigate overfitting. The Adam optimizer combines momentum (exponential moving average of gradients) and RMSProp (exponential moving average of squared gradients) to adaptively adjust learning rates per parameter.

Results were evaluated on the test set using MAE and Mean Absolute Percentage Error (MAPE). The baseline model achieved MAE = 0.610 and MAPE = 0.4969%. The best-performing models were:
- **DNN_best**: MAE = 0.607 (−0.003 vs baseline), MAPE = 0.4947% (−0.0022%)
- **LSTM_best**: MAE = 0.609 (−0.001), MAPE = 0.4966% (−0.0003%)

These represent marginal improvements over the baseline. The CNN_best model performed slightly worse (MAE = 0.611, MAPE = 0.4977%), while GRU_best performed worst among neural networks (MAE = 0.615, MAPE = 0.5011%). The XGBoost model failed catastrophically, with MAE = 2.840 and MAPE = 2.4445%, primarily because decision trees cannot extrapolate beyond the range of training data (minimum training value ~122, but test data dropped to ~110).

The paper concludes that, given the 1-day window constraint, the models do not materially outperform the baseline. This is attributed to insufficient historical context being provided to the models—each prediction uses only one prior value, limiting the ability to learn complex temporal patterns. The authors hypothesize that expanding the input window (e.g., using 5 or 10 prior days) and output horizon (predicting multiple days ahead), introducing stationarity (e.g., via ARIMA or differencing), and incorporating multivariate inputs (e.g., oil prices, stock indices) will yield more significant improvements.

Future work will explore:
1. **Stationary models**: ARIMA, SARIMAX, and hybrid models like SARIMAX-LSTM.
2. **Advanced architectures**: CEEMDAN-LSTM (decomposes signal before LSTM) and N-BEATS (designed specifically for interpretable time series forecasting).
3. **Multivariate analysis**: Correlations with commodities, equity markets, and macroeconomic indicators.
4. **Broader GSS bond universe**: Extending analysis to Bloomberg MSCI Green Bond Index and other social/sustainability bonds.
5. **Greenium analysis**: Studying the yield premium of green bonds pre- and post-COVID.

The study serves as a methodological foundation, demonstrating the feasibility of applying modern ML techniques to green bond forecasting. While initial results are inconclusive, the authors argue that neural networks offer potential for complex time series problems where traditional statistical models may fall short. The paper emphasizes that the choice of architecture (CNN, LSTM, etc.) may become more impactful once richer input windows and multivariate features are incorporated.

In summary, this work provides a rigorous, reproducible framework for time series forecasting in sustainable finance. It highlights the importance of data preprocessing, hyperparameter tuning, and model selection in financial applications. The modest performance gains underscore the challenge of predicting financial time series with minimal input, while the planned extensions promise more meaningful insights in subsequent papers.

**Word count: ~1,500**


================================================
FILE: data/summaries_30B_samples/actuaries-using-data-science-and-artificial-intelligenc_summary_20260125_150113.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** Actuaries using data science and artificial intelligence techniques  
**Authors:** Alan Marshall  
**Date of publication:** 2024-02  
**Topic:** Actuarial science and artificial intelligence  
**Sub-topic:** Application of data science and AI techniques in actuarial practice, regulatory considerations, and professional development  
**URL:** Not available  

---

## Summary

### 1. Modeling Techniques

The document does not focus on specific algorithmic implementations or model architectures in the technical sense, but rather surveys the *types* of modeling techniques actuaries are adopting or experimenting with across various domains. The following techniques are mentioned:

- **Generalized Linear Models (GLM)**: Traditionally dominant in actuarial pricing, especially in General Insurance (GI), now considered one among many regression/forecasting tools.
- **Gradient Boosting Machines (GBM)**: Increasingly adopted in GI pricing due to superior predictive performance over GLMs.
- **Random Forests**: Used in multiple case studies including earnings progression modeling and life underwriting decision prediction. Ensemble learning approaches are highlighted.
- **Markov Models**: Applied in loan portfolio analysis to predict borrower earnings transitions.
- **Large Language Models (LLMs)**: Used experimentally to parse product specification documents (PDF, Word, web) and auto-generate actuarial models. Also used to build capital models from scratch via prompting.
- **Deep Learning-based Semantic Search**: Applied to process unstructured text (reports, documents) by understanding context rather than keyword matching.
- **Explainable AI (XAI)**: Explicitly mentioned in the context of lapse modeling to interpret model outputs and gain insights beyond traditional assumptions.
- **Neural Networks**: Mentioned in the context of constraints (smoothness, monotonicity) for model behavior, though no specific architecture is detailed.
- **Reinforcement Learning**: Cited in a conference topic for dynamic hedging of variable annuities, indicating exploratory use.

The report emphasizes that actuaries are moving beyond “vanilla” statistical models toward more complex, often ensemble or machine learning-based techniques, particularly where predictive accuracy or automation is valued. However, the report does not delve into hyperparameter tuning, loss functions, or training procedures — it remains at a conceptual and applied level.

### 2. Code Availability

**Not available.** The document does not mention any publicly available code repositories, GitHub links, or open-source implementations of the models or techniques described. Case studies are presented as organizational or individual applications without code sharing. The report focuses on governance, ethics, and professional practice rather than technical reproducibility.

### 3. Learning Type

The document does not specify the learning paradigms (supervised, unsupervised, self-supervised) for the models described. However, based on the applications:

- **Supervised Learning** is implied in most cases: pricing models (predicting premiums), underwriting models (predicting approval/rejection), lapse modeling (predicting policy termination), and earnings prediction (regression task).
- **Unsupervised or Self-supervised Learning** is not explicitly mentioned, though semantic search systems may leverage self-supervised pre-training (e.g., via LLMs), this is not elaborated.
- **Reinforcement Learning** is referenced as a topic for dynamic hedging, which is inherently an online, reward-based learning paradigm.

In summary, the dominant learning type inferred from use cases is **supervised learning**, though the report does not classify models by learning type.

### 4. Dataset

The datasets referenced are primarily **real-world, proprietary, and industry-specific**. No public datasets are named. Examples include:

- **GI Claims Triangles**: Paid claims, provisions, reported but unsettled reserves across firms and lines of business.
- **Pension Scheme Data**: Active, pensioner, and dependent member data including date of birth, gender, movement dates, pension/salary amounts.
- **Loan Portfolio Data**: Borrower demographics (date of birth, gender, loan inception year), historical earnings, field of work.
- **Life Insurance Applications**: Customer, policy, medical, and family history data collected at application.
- **Unstructured Text Data**: Reports and documents requiring semantic search.
- **Weather and Agricultural Data**: Satellite and farmer data for weather-indexed crop insurance in Africa.
- **Product Specification Documents**: PDFs, Word files, web pages used as input for LLM-based model generation.

Datasets are typically internal to the organizations involved and not publicly accessible. The report emphasizes data quality, bias, and GDPR compliance as key concerns, but does not provide dataset statistics (size, features, time periods).

### 5. Implementation Details

- **Programming language(s):**  
  - **Python**: Used for dashboarding and visualization in pension experience analysis.  
  - **R**: Used for data mining/analysis in earnings progression modeling (random forests), and planned migration for pension experience analysis.  
  - **Proprietary Software**: Mentioned for GI claims triangle analysis and actuarial model building (via LLMs).  
  - **Open-source frameworks**: Referenced in the context of building capital models, though no specific language is named.

- **Key libraries/frameworks:**  
  - **R packages**: Specifically mentioned for data mining and random forests (e.g., `randomForest`, `caret`, or similar — though not named).  
  - **Python libraries**: Not explicitly listed, but implied for dashboarding (e.g., `Dash`, `Streamlit`, `Plotly`) and machine learning (e.g., `scikit-learn`, `XGBoost`).  
  - **Third-party AI libraries**: Used to analyze documents for model generation (e.g., likely Hugging Face, spaCy, or proprietary NLP tools).  
  - **LLM APIs**: Implied for document parsing and capital model generation (e.g., OpenAI GPT, Anthropic Claude, or internal LLMs).

No specific versions or dependencies are provided. The focus is on functional outcomes rather than technical stack.

### 6. Model Architecture

The report does not provide detailed architectural diagrams or component breakdowns of composite models. However, the following architectural patterns are implied:

- **Ensemble Models**: In life underwriting, multiple random forest models are combined to predict decisions — this suggests a voting or stacking ensemble.
- **Pipeline Architecture**: In pension experience analysis, a two-stage pipeline: (1) data cleaning and exposure-to-risk derivation, (2) dashboard visualization in Python.
- **LLM-Powered Model Generation**: A hybrid system where LLMs parse unstructured inputs (documents) → output structured data → proprietary software builds actuarial models. This is a “prompt-to-model” architecture.
- **Semantic Search System**: Likely based on transformer-based embeddings (e.g., BERT, Sentence-BERT) for vector retrieval, paired with a vector database (e.g., FAISS, Pinecone) and possibly a reranker.
- **Markov Transition Models**: Used for earnings progression, implying a state-space model with transition probabilities between earnings bands.

No neural network architectures (e.g., CNN, LSTM, Transformer layers) are detailed beyond the mention of “neural networks” in the context of constraints. The report avoids technical depth on architecture, favoring functional descriptions.

### 7. Technical Content

The report “Actuaries using data science and artificial intelligence techniques” by Alan Marshall, published by the Institute and Faculty of Actuaries (IFoA) in February 2024, is a thematic review examining the evolving role of actuaries in the age of data science and AI. It is not a technical paper presenting novel algorithms or empirical results, but rather a comprehensive survey of industry practices, regulatory landscapes, ethical considerations, and professional development needs. The document synthesizes insights from case studies, regulatory research, conference proceedings, and stakeholder interviews to provide a holistic view of how actuaries are engaging with AI/ML technologies across traditional and emerging domains.

#### Context and Motivation

The report opens with a recognition that while actuaries have historically been “data scientists” in spirit — analyzing large datasets to infer future risks — the modern tools of AI and machine learning offer unprecedented capabilities. The rapid proliferation of generative AI (especially LLMs) since late 2022 has accelerated adoption and raised new ethical, governance, and transparency challenges. The IFoA’s Regulatory Board, which commissioned the review, is particularly concerned with ensuring that AI applications in actuarial work balance commercial interests with consumer fairness and public interest.

#### Key Findings and Case Studies

The review identifies eight key findings, supported by a range of case studies that illustrate practical applications:

1. **Rapidly Changing Environment**: The increasing capacity and accessibility of AI tools, coupled with growing data volumes, are transforming actuarial work. This introduces new risks (bias, lack of explainability, reputational damage) alongside opportunities (better risk modeling, product innovation).

2. **Growing Actuarial Involvement**: Actuaries are increasingly involved in AI/ML across domains beyond traditional GI pricing — including pensions, life underwriting, claims analysis, capital modeling, and even non-traditional areas like African agriculture insurance. Many organizations report plans to expand usage further.

3. **Broadening Application Scope**: Applications now span risk management, reporting, retention, marketing, and model validation. For example, a case study describes using ML to analyze GI claims triangles for trend detection across firms — a task previously done manually or with basic statistical methods.

4. **Collaboration with Data Scientists**: Actuaries often work alongside data scientists and other experts. Organizations prioritize skills over professional qualifications, which may challenge the traditional actuarial value proposition. This necessitates continuous upskilling.

5. **Global Regulatory Activity**: There is significant and accelerating regulatory activity worldwide, with jurisdictions like the EU (AI Act), China (specific regulations), and the UK (pro-innovation white paper) taking divergent approaches. Common themes include fairness, transparency, accountability, and safety.

6. **Existing Standards Are Relevant**: The IFoA’s Actuaries’ Code (especially Principles 2 and 6 on competence and communication) and existing Technical Actuarial Standards (TASs) provide a foundation for ethical AI use. However, there is a tension between creating specific AI standards (which may become obsolete) and adapting existing guidance.

7. **Education and Lifelong Learning**: The IFoA curriculum includes data science elements in Actuarial Statistics, Specialist Principles, and Specialist Advanced modules. The IFoA Data Science Certificate (with University of Southampton) offers upskilling for qualified members. Plans are underway to expand both pathways.

8. **Opportunities for Collaboration**: The IFoA has a strong track record of collaboration with bodies like the Royal Statistical Society and the Alan Turing Institute. There are opportunities to engage with global actuarial associations and regulators to shape responsible AI practices.

#### Technical Case Studies in Detail

- **GI Personal Lines Pricing**: Gradient Boosting Machines (e.g., XGBoost, LightGBM) are replacing GLMs due to better handling of non-linear relationships and interactions. This reflects a broader industry shift toward ensemble methods for pricing accuracy.

- **GI Claims Triangle Analysis**: An off-the-shelf ML software package uses pattern recognition to identify trends in claims development triangles. The output is consistent and rapid, though validation and ranking of trends are still underway. This highlights the move toward automation in reserving.

- **Pension Experience Analysis**: A two-stage pipeline: first, data is cleaned and exposure-to-risk is calculated; second, outputs are visualized in a Python dashboard. The intention is to migrate to R for faster processing and enhanced modeling (e.g., workforce dynamics, lump sum propensity).

- **Model Build from Documents**: Generative AI (LLMs) parses product specs (PDF, Word, web) and feeds into proprietary software to auto-generate actuarial models. This reduces manual model-building time and allows for rapid iteration, though human review is still required.

- **Loan Portfolio Earnings Modeling**: A Markov model predicts earnings transitions. A sub-project uses random forests to validate and improve the model. Feature importance from random forests identifies key predictors (e.g., field of work, historical earnings), affirming the Markov model’s robustness while suggesting enhancements.

- **Life Underwriting**: An ensemble of random forest models predicts underwriting decisions for 25% of applications (the 75% are handled by rules). The model optimizes risk cost loading and has been in production for five years, demonstrating long-term operational viability.

- **Semantic Search for Unstructured Text**: A deep learning system (likely transformer-based) enables context-aware retrieval from reports. This moves beyond keyword search to understand user intent, handling ambiguity and complexity in natural language. It is in user acceptance testing.

- **Lapse Modeling with XAI**: Machine learning models (likely tree-based or neural) are applied to lapse rates, with explainable AI techniques used to interpret outputs. Comparisons with traditional models reveal trade-offs between accuracy and interpretability.

- **Capital Model in Open-Source Framework**: An actuary with no coding experience built a capital model using an LLM by asking questions. The model was then showcased in a dashboard. This demonstrates the potential of LLMs as “co-pilots” for complex actuarial tasks, though reproducibility and governance remain concerns.

- **AI for African Crop Insurance**: Weather-indexed insurance for small-scale farmers uses satellite and farm data with ML algorithms to reduce costs and increase uptake. This is an example of AI for social good, enhancing food security and resilience.

#### Ethical and Regulatory Considerations

Ethics and fairness are central to the report. Key challenges include:

- **Bias and Discrimination**: Models may perpetuate or amplify biases in training data. Solutions include fairness testing at development and pre-implementation stages, and redeveloping models if bias is found.
- **Data Privacy and GDPR**: Organizations are advised to use reputable AI providers or private LLM instances to ensure compliance.
- **Explainability**: AI/ML models are often less explainable than traditional actuarial models, posing a challenge for governance and stakeholder communication. Explainable AI (XAI) techniques are being explored.
- **Professional Competence**: Actuaries must ensure they have the necessary skills (Principle 2 of the Code). Organizations are encouraged to demonstrate competence via accreditation (e.g., IFoA’s Quality Assurance Scheme).

The report notes that ethical considerations are often discussed at the team level, with a need for standardized definitions and internal policies. Actuaries’ existing ethical code provides a strong foundation, but firms with mixed professional teams need consistent internal frameworks.

#### Global Regulatory Landscape

The report provides a snapshot of global AI regulation as of end-2023:

- **EU**: AI Act (provisional agreement Dec 2023) — strict, with bans on social scoring, biometric ID by law enforcement, and requirements for consumer explanations.
- **UK**: Pro-innovation approach, with discussion papers and a white paper. Focus on principles rather than prescriptive rules.
- **China**: Already has specific regulations (Ethical Norms, Generative AI Measures) — one of the few jurisdictions with enforceable rules.
- **US**: Mix of executive orders (White House), bipartisan frameworks, and NAIC principles. Regulatory activity is fragmented across agencies.
- **Singapore**: MAS toolkit for responsible AI, with a planned generative AI risk framework.
- **Australia**: AI Ethics Principles (non-binding).
- **Global**: OECD AI Principles, Bletchley Declaration, and NCSC/CISA guidelines for secure AI development.

Common themes across jurisdictions include **fairness, transparency, accountability, robustness, and safety**. The report notes that while principles are converging, implementation may vary, creating challenges for multinational firms.

#### Professional Development and Education

The IFoA’s education pathway includes data science in:

- **Actuarial Statistics**: Introduction to large datasets, ML, and software tools.
- **Specialist Principles**: Domain-specific applications (e.g., GI, Life).
- **Specialist Advanced**: Ethical and regulatory aspects.

Lifelong learning is supported via the Data Science Certificate and ongoing CPD. The report acknowledges a risk to the profession’s relevance if learning resources do not keep pace with technological change. Plans are underway to enhance both curriculum and CPD offerings.

#### Conclusion and Future Directions

The report concludes that the increasing use of data science and AI presents both opportunities and risks for actuaries. To remain competitive, actuaries need evolving resources for professional development and standards that support them as both builders and users of AI systems. The IFoA Regulatory Board supports a review of ethical guidance and continued development of professional skills material. Collaboration with global actuarial associations and other agencies is encouraged to drive responsible and ethical use of AI.

The report does not prescribe specific technical solutions but emphasizes the need for:

- **Ethical frameworks** tailored to actuarial practice.
- **Governance processes** that ensure model explainability and fairness.
- **Education** that keeps pace with technological change.
- **Collaboration** across professions and borders.

In essence, the report serves as a call to action for the actuarial profession to proactively engage with AI/ML technologies, ensuring that their application is safe, transparent, and in the public interest — while preserving the profession’s core values of competence, integrity, and public trust.

--- 

**Word Count:** ~1520 words


================================================
FILE: data/summaries_30B_samples/actuary-gpt-applications-of-large-language-models-to-in_summary_20260125_150113.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** ACTUARYGPT: APPLICATIONS OF LARGE LANGUAGE MODELS TO INSURANCE AND ACTUARIAL WORK  
**Authors:** Caesar Balona  
**Date of publication:** 2023-08-17  
**Topic:** Artificial Intelligence in Actuarial Science  
**Sub-topic:** Large Language Models for Insurance Claims, Underwriting, Compliance, and Workflow Automation  
**URL:** https://ssrn.com/abstract=4543652  

---

## Summary

### 1. Modeling Techniques

The document primarily employs **Large Language Models (LLMs)**, specifically **GPT-4** and **GPT-3.5** from OpenAI, as the core AI technique. These models are transformer-based architectures trained on massive text corpora and are used for:

- **Natural Language Processing (NLP)** tasks including sentiment analysis, named entity recognition, text summarization, and classification.
- **Structured data extraction** from unstructured text (e.g., claims descriptions, reinsurance treaties, medical reports).
- **Prompt engineering** and **few-shot learning** to guide model outputs toward domain-specific tasks.
- **Context-aware querying** via vector databases to overcome context-length limitations of LLMs for regulatory compliance tasks.

No traditional machine learning models (e.g., decision trees, SVMs, random forests) are used as primary modeling tools. The focus is on leveraging pre-trained LLMs via API calls rather than training or fine-tuning models from scratch.

### 2. Code Availability

**Yes**, implementation code is available.

- **GitHub Repository:** https://github.com/cbalona/actuarygpt-code
- The repository contains Python scripts for:
  - Parsing claims descriptions (Case Study 1)
  - Extracting structured data from reinsurance treaties (Case Study 4)
  - Automating regulatory compliance via vector databases (Case Study 3)
  - Demonstrating prompt engineering and few-shot learning (Appendix C)

Code is provided for educational and demonstrative purposes and is not production-grade. It relies on the OpenAI API and libraries like `PyPDF2`, `chainladder`, and `pandas`.

### 3. Learning Type

The approach uses **supervised learning** in a narrow sense — the LLMs are pre-trained on massive datasets (unsupervised pre-training) and then **prompted or fine-tuned via instructions** (supervised fine-tuning via prompt design) to perform specific tasks.

However, the document does **not** describe traditional supervised training with labeled datasets. Instead, it relies on **in-context learning** (zero-shot or few-shot) where the model is guided by example inputs and desired output formats within the prompt.

Thus, the learning type is best described as:  
> **Prompt-based supervised inference using pre-trained LLMs**

### 4. Dataset

The datasets used are **synthetic or fictional**, generated for demonstration purposes:

- **Claims data** (JSON format) with policyholder interactions, medical reports, and police reports (Case Study 1).
- **Reinsurance treaties** (PDFs converted to text) for structured extraction (Case Study 4).
- **Regulatory documents** (FSI and GOI standards under SAM framework) for compliance querying (Case Study 3).
- **Automatically scraped news snippets** related to cyber risks (Case Study 2).

No real-world, proprietary, or public datasets are used. All data is fabricated to illustrate LLM capabilities without exposing sensitive information.

### 5. Implementation Details

- **Programming language(s):** Python
- **Key libraries/frameworks:**
  - `openai` (for API access to GPT-4 and GPT-3.5)
  - `PyPDF2` (for extracting text from PDFs)
  - `pandas` (for data manipulation)
  - `chainladder` (for IBNR reserving examples in Appendix C)
  - `json` (for structured output formatting)
  - `pinecone` (for vector database in regulatory knowledgebase — mentioned but not fully implemented in code)
  - `requests` (for API calls in some examples)

No custom model training frameworks (e.g., TensorFlow, PyTorch) are used. The implementation is entirely API-driven with minimal local computation.

### 6. Model Architecture

The document does not describe custom model architectures. It uses **off-the-shelf, pre-trained LLMs**:

- **GPT-4** and **GPT-3.5** (OpenAI models)
- These are **decoder-only transformer models** with billions of parameters, trained on internet-scale text data.

In Case Study 3, a **hybrid architecture** is proposed:

- **Vector Database (Pinecone)** stores embeddings of regulatory documents.
- **LLM (GPT-4)** queries the vector DB to retrieve relevant context before generating a response.
- This forms a **Retrieval-Augmented Generation (RAG)** pipeline, though not explicitly named.

No ensemble models, custom neural networks, or fine-tuned variants are implemented. The focus is on leveraging existing LLMs via API with prompt engineering.

### 7. Technical Content

The paper explores how Large Language Models (LLMs) can be integrated into actuarial and insurance workflows, both as direct automation tools and as assistance aids. The author, Caesar Balona, presents a structured framework for evaluating LLM suitability and demonstrates practical applications through four case studies.

#### Background and Motivation

Actuarial work traditionally relies on structured, numerical data. However, modern insurance operations generate vast amounts of unstructured text — claims descriptions, medical reports, emails, regulatory documents — which are difficult to process manually. LLMs offer a solution by enabling:

- **Natural Language Understanding (NLU)**: Extracting entities, sentiments, and inconsistencies from text.
- **Structured Output Generation**: Converting free-form text into JSON, tables, or summaries.
- **Workflow Automation**: Reducing manual labor in claims processing, underwriting, and compliance.

The paper emphasizes that LLMs are not replacements for actuaries but **augmentation tools** that enhance efficiency, reduce error, and free actuaries to focus on high-value reasoning tasks.

#### Direct Applications: Claims Process

The claims management process is broken down into 7 stages (Reporting, Adjusting, Investigating, Negotiating, Agreements, Payments, Compliance). LLMs can be embedded at each stage:

- **Reporting**: Extract incident details (date, location, type) from phone call transcripts or emails. Perform sentiment analysis to detect fraud signals (e.g., angry policyholders may exaggerate claims).
- **Adjusting**: Summarize all collected documents (medical, police, witness statements) for adjusters. Classify claims by severity (low/medium/high) to prioritize investigations.
- **Investigating**: Identify inconsistencies across documents (e.g., claimant says “stationary” but police report mentions “brake marks”). Flag potential fraud based on emotional tone or factual contradictions.
- **Compliance**: Query regulatory documents (e.g., SAM framework) to ensure claims handling meets legal requirements — using a vector database to overcome context-length limits.

A key innovation is the use of **NoSQL databases** to store flexible, unstructured claim data (e.g., sentiment scores, extracted entities, summaries) for cross-referencing and trend analysis over time.

#### Direct Applications: Other Insurance Functions

Beyond claims, LLMs can assist in:

- **Emerging Risk Identification**: Scrape news articles and summarize trends (e.g., cyber risks, climate change impacts) for risk committees. Generate action points and project plans from summaries.
- **Commercial Underwriting**: Extract key terms from technical reports (e.g., engineering safety assessments) and output them in structured formats (JSON) for underwriting systems.
- **Regulatory Compliance**: Build a “regulatory knowledgebase” using vector embeddings to answer queries about specific regulations (e.g., “How should real estate investments be stressed under SAM?”).

#### Decision Framework for LLM Suitability

The paper introduces a two-part decision tree to evaluate whether an LLM is appropriate for a given task:

1. **Technical Assessment Tree**:
   - Is the data unstructured or requires context/pattern recognition? → LLM suitable.
   - Does the task require generating new content or restructuring existing content? → LLM suitable.
   - Do benefits (automation, accuracy) outweigh costs (API fees, maintenance)? → Proceed if yes.
   - Do you have resources (data, expertise, compute)? → Proceed if yes.

2. **Risk Assessment Tree**:
   - Could bias affect outcomes? → Mitigate or avoid.
   - Is interpretability required? → Consider if LLM can explain outputs.
   - Is data leakage a risk? → Use private or ring-fenced models.
   - Is output variability acceptable? → Use low-temperature parameters.
   - Are adversarial inputs likely? → Validate outputs.
   - Are error consequences severe? → Implement human oversight.

An example application — extracting data from reinsurance treaties — passes both trees, making it a viable LLM use case.

#### Assistance Applications

LLMs can also serve as “assistants” to actuaries:

- **Coding Assistant**: Help write, debug, and optimize Python code (e.g., IBNR reserving using `chainladder` library). The paper includes a conversation where ChatGPT explains Python syntax and generates working code.
- **Problem Solving**: Actuaries can describe complex problems (e.g., pricing insurance for LLM-generated misinformation) and receive structured feedback, including scenario analysis and expert judgment suggestions.
- **Drafting Reports**: Summarize technical findings, generate executive summaries, or translate actuarial jargon into layman’s terms. Can also suggest visualizations or improve grammar.
- **Education**: Teach new statistical methods or programming concepts via interactive Q&A, analogies, and exercises.
- **Data Cleaning**: Use ChatGPT’s Code Interpreter plugin to upload datasets and generate cleaning code.
- **Model Interpretation**: Explain hyperparameters, validate assumptions, or translate model outputs for non-technical stakeholders.

#### Case Studies

**Case Study 1: Parsing Claims Descriptions**  
- Input: JSON with claim details, phone/email transcripts, medical/police reports.
- Prompt: Instructs GPT-4 to extract claim ID, policy ID, inconsistencies, claimant emotion, summary, and whether further assessment is needed.
- Output: Structured JSON with reasoning (e.g., “claimant is angry because they described the accident as ‘horrifying’”).
- Result: Demonstrates LLM’s ability to synthesize multi-source text and flag potential fraud.

**Case Study 2: Identifying Emerging Risks**  
- Input: Automatically scraped Google News results on cyber risks.
- Process: GPT-4 summarizes trends, generates action points, then creates project plans for each action.
- Output: Detailed project plan for “Cybersecurity Strategy and Continuous Monitoring” with phases, timeline, resources.
- Result: Shows LLMs can turn raw data into actionable business intelligence.

**Case Study 3: Regulatory Knowledgebase**  
- Problem: LLMs have limited context (e.g., GPT-4 handles ~12k words; IFRS 17 is 102 pages).
- Solution: Use Pinecone vector DB to store embeddings of regulatory docs (FSI, GOI).
- Query: “How should real estate investments be stressed?” → LLM retrieves relevant sections and answers precisely.
- Result: More accurate, domain-specific answers than generic ChatGPT responses.

**Case Study 4: Parsing Reinsurance Treaties**  
- Input: PDF reinsurance treaty.
- Process: Extract text → Prompt GPT-3.5 to output JSON with treaty details (type, reinsurer, layers, exclusions, etc.).
- Output: Structured JSON with loss layers, commissions, currency, arbitration clauses.
- Result: Automates a tedious manual task; useful for complex, variable contracts.

#### Ethical and Professional Considerations

The paper acknowledges significant risks:

- **Bias**: LLMs trained on internet data may reflect societal biases. Actuaries must validate outputs and avoid using LLMs in sensitive areas (e.g., pricing, underwriting) without oversight.
- **Hallucinations**: LLMs may invent facts or use non-existent libraries. Outputs must be reviewed.
- **Data Privacy**: Using public LLMs (e.g., ChatGPT) risks leaking confidential data. Solutions: Use private APIs, local LLMs, or ring-fenced models.
- **Professional Responsibility**: Actuaries remain accountable for LLM outputs. LLMs are tools, not decision-makers.
- **Interpretability**: LLMs are “black boxes.” Actuaries must ensure they can explain results to stakeholders.

The author stresses that LLMs should complement, not replace, human judgment. Actuaries must continuously monitor, validate, and understand LLM limitations.

#### Impact on the Actuarial Profession

LLMs will not replace actuaries but will transform their roles:

- **Efficiency Gains**: Automate routine tasks (data extraction, report drafting) to free time for strategic analysis.
- **Skill Shift**: Actuaries need to learn prompt engineering, AI ethics, and model validation.
- **Interdisciplinary Collaboration**: Work with data scientists, ethicists, and software engineers.
- **Communication**: Translate LLM outputs for non-technical stakeholders while acknowledging uncertainties.

The paper concludes that LLMs are powerful tools for enhancing actuarial work — if used responsibly, ethically, and with professional oversight.

#### Limitations and Future Work

- **No Fine-Tuning**: The paper uses only API-based LLMs without domain-specific fine-tuning.
- **Synthetic Data**: All case studies use fictional data; real-world validation is needed.
- **Cost**: API usage can become expensive at scale.
- **Context Limits**: Even with vector DBs, handling very long documents remains challenging.
- **Regulatory Uncertainty**: AI regulations are evolving; actuaries must stay informed.

Future research should explore fine-tuning LLMs on actuarial corpora, integrating LLMs with traditional actuarial models, and developing validation frameworks for LLM outputs.

---

**Word Count**: ~1500 words (excluding metadata and headers)


================================================
FILE: data/summaries_30B_samples/AI,_data_science_and_emerging_technologies_summary_20260125_150113.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** AI, Data Science and Emerging Technologies Practice Board  
**Authors:** Not specified  
**Date of publication:** Not specified (document references formation in November 2025)  
**Topic:** Actuarial science and emerging technologies  
**Sub-topic:** Integration of AI and data science into actuarial practice  
**URL:** Not available  

---

## Summary

### 1. Modeling Techniques

The document does not describe any specific machine learning or artificial intelligence modeling techniques such as neural networks, decision trees, or ensemble methods. Instead, it outlines the organizational and strategic framework of a professional practice board focused on the adoption and ethical implementation of AI and data science within the actuarial profession. No technical algorithms, model architectures, or computational methods are detailed. The emphasis is on governance, education, and research coordination rather than applied modeling. Therefore, **no modeling techniques are specified**.

### 2. Code Availability

No implementation code is referenced or made available in the document. The content is purely organizational and descriptive, focused on the structure, leadership, and mission of the AI, Data Science and Emerging Technologies Practice Board. There is no mention of GitHub repositories, open-source tools, or code-sharing platforms. **Code availability: Not available**.

### 3. Learning Type

The document does not discuss any machine learning models or training paradigms such as supervised, unsupervised, or self-supervised learning. The term “learning” appears in the context of “lifelong learning” for professional development, not in the technical sense of model training. Therefore, **learning type: Not applicable**.

### 4. Dataset

No datasets are mentioned, referenced, or described. The document does not involve empirical analysis, data-driven modeling, or experimental results that would require dataset specification. The focus is on professional practice, community engagement, and research coordination. **Dataset: Not available**.

### 5. Implementation Details

- **Programming language(s):** Not specified. The document does not involve software implementation or coding.
- **Key libraries/frameworks:** Not specified. No technical tools, libraries (e.g., TensorFlow, scikit-learn), or frameworks are referenced.

### 6. Model Architecture

No model architecture is described. The document does not contain any technical diagrams, component breakdowns, or system designs for AI or data science models. It is purely organizational in nature. **Model architecture: Not applicable**.

### 7. Technical Content

The document presents the foundational structure, mission, and operational pillars of the “AI, Data Science and Emerging Technologies Practice Board” under the Institute and Faculty of Actuaries (IFoA). It is not a technical research paper or implementation report but rather a strategic and administrative overview of a professional initiative aimed at integrating emerging technologies into actuarial science.

#### Background and Formation

The practice board was formally established in November 2025 as a member-led initiative developed in partnership between the IFoA Council and the IFoA AI and Data Science Community. Its creation represents a strategic response to the accelerating pace of technological change and its impact on actuarial practice. The board’s formation is positioned as a critical step to ensure that actuaries remain at the forefront of innovation, equipped to leverage AI, data science, and other emerging technologies to enhance professional relevance and societal impact.

The origins of the board trace back to 2018, when it began as a working party known as the “Modelling, Analytics and Insights from Data Working Party.” This group evolved into the “AI and Data Science Community,” which eventually matured into the current practice board. This evolution reflects the growing importance and institutionalization of data-driven methodologies within the actuarial profession.

#### Organizational Structure and Leadership

The board is governed by a leadership team with clearly defined roles:

- **Board Chair:** Asif John
- **Board Deputy Chair:** John Ng
- **Research Lead:** Alexis Iglauer
- **Research Deputy Lead:** Dylan Liew
- **Lifelong Learning Lead:** Alexey Mashechkin
- **Lifelong Learning Deputy Lead:** Eilish Bouse
- **Professionalism and Ethics Lead:** Matthew Byrne
- **Collaboration and External Engagement Lead:** Malgorzata Smietanka

These roles indicate a structured approach to managing the board’s activities across key domains: research, education, ethics, and external collaboration. The leadership team is composed of experienced professionals who are likely to have backgrounds in actuarial science, data science, or related technical fields, although their specific qualifications are not detailed.

#### Core Pillars of Operation

The board operates under four foundational pillars:

1. **Lifelong Learning**  
   This pillar focuses on continuous professional development for actuaries. It aims to equip members with the skills and knowledge needed to adapt to evolving technological landscapes. Activities likely include training programs, workshops, and certification pathways in AI and data science. The emphasis is on upskilling and reskilling to maintain professional relevance.

2. **Research**  
   The research pillar is responsible for generating and disseminating knowledge through working parties, research papers, case studies, and other scholarly outputs. The board supports active research working parties that explore specific topics at the intersection of actuarial science and emerging technologies. These working parties serve as incubators for new ideas and practical applications.

3. **Engagement**  
   Engagement involves fostering dialogue and collaboration among members, industry stakeholders, and external partners. The board promotes member-to-member interaction through digital platforms, webinars, events, and conferences. This pillar aims to build a vibrant community that shares expertise, discusses challenges, and co-creates solutions.

4. **Professionalism and Ethics**  
   Given the societal implications of AI and data science, this pillar ensures that technological adoption is guided by ethical principles and professional standards. It addresses issues such as algorithmic bias, data privacy, transparency, and accountability. The board likely develops guidelines, position papers, and educational materials to help actuaries navigate ethical dilemmas in technology-driven practice.

#### Strategic Vision and Goals

The overarching vision of the practice board is to become a “centre of excellence” and the primary point of engagement between the IFoA and its members interested in AI, data science, and emerging technologies. This vision implies a commitment to thought leadership, innovation, and influence within the actuarial profession and beyond.

Key strategic goals include:

- **Enhancing Actuarial Relevance:** By integrating AI and data science into actuarial workflows, the board seeks to enhance the value and impact of actuarial services in areas such as risk modeling, pricing, underwriting, and customer analytics.

- **Driving Innovation:** The board encourages experimentation and the adoption of cutting-edge technologies to solve complex actuarial problems. This includes exploring applications of machine learning, natural language processing, computer vision, and other AI techniques.

- **Promoting Ethical Practice:** As AI systems become more pervasive, the board emphasizes the importance of ethical considerations. It aims to ensure that actuaries are equipped to design, implement, and audit AI systems in a manner that is fair, transparent, and accountable.

- **Building Community:** The board fosters a sense of community among actuaries with shared interests in technology. This includes creating spaces for collaboration, mentorship, and knowledge exchange.

#### Activities and Outputs

The board delivers a range of activities and outputs through its working parties and initiatives:

- **Research Papers:** These provide in-depth analysis of specific topics, such as the use of AI in insurance pricing or the ethical implications of algorithmic decision-making.

- **Case Studies:** Real-world examples illustrate how AI and data science are being applied in actuarial practice. These case studies serve as practical guides for implementation.

- **Webinars and Events:** Regular webinars and events feature expert speakers, panel discussions, and interactive sessions. These activities promote knowledge sharing and professional networking.

- **Education and Training:** The board offers educational resources, including online courses, workshops, and certification programs. These resources are designed to build technical skills and conceptual understanding.

- **Digital Community Platform:** A new digital platform facilitates member-to-member engagement, enabling discussions, resource sharing, and collaboration. This platform is likely to include forums, blogs, and project spaces.

#### Broader Context and Implications

The establishment of the AI, Data Science and Emerging Technologies Practice Board reflects a broader trend in professional bodies to adapt to technological disruption. Similar initiatives exist in other fields, such as medicine, law, and engineering, where professional organizations are creating dedicated units to address the impact of AI and data science.

For actuaries, this initiative is particularly significant because the profession has traditionally relied on statistical modeling and risk assessment. The integration of AI and data science represents a paradigm shift, requiring actuaries to expand their skill sets beyond traditional actuarial science into areas such as machine learning, data engineering, and computational statistics.

The board’s focus on ethics and professionalism is also noteworthy. As AI systems are increasingly used to make high-stakes decisions in areas such as insurance, healthcare, and finance, there is a growing need for professionals who can ensure that these systems are fair, transparent, and accountable. Actuaries, with their strong foundation in risk management and ethics, are well-positioned to play a key role in this space.

#### Challenges and Opportunities

The board faces several challenges in achieving its goals:

- **Skill Gaps:** Many actuaries may lack the technical skills needed to work with AI and data science tools. Bridging this gap requires significant investment in education and training.

- **Ethical Complexity:** The ethical implications of AI are complex and evolving. Developing clear guidelines and standards will require ongoing dialogue and collaboration.

- **Interdisciplinary Collaboration:** Effective integration of AI and data science into actuarial practice requires collaboration across disciplines, including computer science, statistics, and ethics. Building these collaborations will be essential.

Despite these challenges, the board also has significant opportunities:

- **Leadership Position:** By taking a proactive approach, the IFoA can position itself as a leader in the integration of AI and data science into professional practice.

- **Innovation Potential:** The application of AI and data science in actuarial science has the potential to unlock new insights, improve decision-making, and create new business models.

- **Societal Impact:** Actuaries can play a key role in ensuring that AI systems are used responsibly and ethically, contributing to broader societal goals such as fairness, transparency, and accountability.

#### Conclusion

In summary, the AI, Data Science and Emerging Technologies Practice Board is a strategic initiative by the IFoA to ensure that actuaries remain relevant and influential in a rapidly changing technological landscape. While the document does not contain technical details about modeling techniques, datasets, or code, it provides a comprehensive overview of the board’s structure, mission, and activities. The board’s focus on lifelong learning, research, engagement, and professionalism and ethics reflects a holistic approach to integrating emerging technologies into actuarial practice. By fostering innovation, promoting ethical practice, and building a strong community, the board aims to position actuaries as leaders in the age of AI and data science.

This initiative underscores the importance of professional adaptation in the face of technological disruption and highlights the potential for actuaries to play a pivotal role in shaping the future of AI and data science in society. As the board continues to evolve, it will likely produce more technical outputs, research findings, and practical tools that will further advance the integration of these technologies into actuarial science.


================================================
FILE: data/summaries_30B_samples/DGM_summary_20260125_150113.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** AI Tools for Actuaries — Chapter 11: Deep Generative Models  
**Authors:** Ronald Richman, Mario V. Wüthrich  
**Date of publication:** 2025-11-30  
**Topic:** Deep Generative Models (DGMs)  
**Sub-topic:** Application in actuarial science, including VAEs, GANs, diffusion models, and decoder transformers  
**URL:** Not available  

---

## Summary

### 1. Modeling Techniques

This chapter introduces **deep generative models (DGMs)**, which go beyond point prediction to model full probability distributions over data. The techniques discussed are:

- **Variational Auto-Encoders (VAEs)**: A latent factor model that uses an encoder to map data to a latent space and a decoder to reconstruct data from latent samples. The model is trained by maximizing the Evidence Lower Bound (ELBO), which balances reconstruction fidelity and regularization against a prior distribution.
- **Generative Adversarial Networks (GANs)**: A two-player minimax game between a generator (which creates synthetic data from noise) and a discriminator (which classifies real vs. fake data). The generator aims to fool the discriminator, while the discriminator improves its classification ability.
- **Denoising Diffusion Models**: A multi-step generative process that learns to reverse a forward noising process. Starting from pure Gaussian noise, the model iteratively denoises to generate realistic samples. The training objective is to predict the noise added at each step.
- **Decoder Transformer Models**: An implicit probability distribution approach that models sequences auto-regressively — predicting each token conditioned on all previous tokens. Models like GPT, BART, and T5 use causal self-attention to ensure temporal causality.

These models are categorized into two broad paradigms: **latent factor approaches** (VAEs, GANs, diffusion models) and **implicit probability distribution approaches** (decoder transformers).

### 2. Code Availability

Yes, implementation code is available in the form of R/Keras scripts embedded within the chapter. The code examples are provided for:

- **VAE implementation** using `tensorflow` and `keras` libraries for the Sports Car dataset.
- **GAN implementation** using a custom `GAN` class derived from TensorFlow/Keras, with separate training steps for generator and discriminator.
- **Diffusion models** and **decoder transformers** are discussed theoretically but no code is provided for these in the chapter.

The code is presented as executable R chunks, indicating that the authors intend for readers to run and experiment with the models. However, no public GitHub repository or external codebase is referenced.

### 3. Learning Type

The learning type is **unsupervised** for all models discussed. DGMs aim to learn the underlying data distribution \( p(X) \) from unlabeled data without explicit target labels. Although some models (like conditional VAEs or GANs) can be adapted for conditional generation, the core framework presented here is unsupervised.

### 4. Dataset

The primary dataset used for demonstration is the **Sports Car dataset**, which contains 475 observations of 5 engineered features derived from car insurance data (e.g., log-transformed ratios of weight, power, torque, engine speed, and cubic capacity). The data is standardized before modeling.

This is a **real-world dataset** from actuarial applications, though it is small and likely preprocessed for educational purposes. No external dataset names (e.g., MNIST, CIFAR) are used, and no large-scale actuarial datasets are referenced.

### 5. Implementation Details

- **Programming language(s):** R (with `tensorflow` and `keras` wrappers)
- **Key libraries/frameworks:**
  - `tensorflow` and `keras` (for building and training neural networks)
  - `MASS` (for multivariate normal sampling)
  - `ggplot2` and `corrplot` (for visualization)
  - `reticulate` (for interfacing with Python/TensorFlow operations in R)

The code is written in R using Keras 2, which suggests compatibility with TensorFlow 2.x. The implementation is pedagogical, with detailed step-by-step construction of models and loss functions.

### 6. Model Architecture

#### Variational Auto-Encoder (VAE)

- **Encoder**: Maps input \( X \) to parameters of a latent Gaussian distribution \( q_\vartheta(Z|X) = \mathcal{N}(\mu_\vartheta(X), \Sigma_\vartheta(X)) \). Implemented as a feedforward neural network with two dense layers (32 units each, SiLU activation) followed by two output layers for mean and log-variance.
- **Decoder**: Maps latent sample \( Z \) to reconstructed data \( X' \). Implemented as a feedforward network with two dense layers (32 units, SiLU) and a linear output layer. Only the mean is modeled (no covariance matrix).
- **Latent Space**: 10-dimensional Gaussian prior \( \pi(Z) = \mathcal{N}(0, I) \).
- **Training Objective**: ELBO, decomposed into reconstruction loss (mean squared error) and KL divergence regularization.

#### Generative Adversarial Network (GAN)

- **Generator**: Takes 10D Gaussian noise and maps to 5D output via three dense layers (15 → 10 → 5 units, tanh/linear activations).
- **Discriminator**: Takes 5D input and outputs a scalar probability via three dense layers (20 → 10 → 1 units, tanh/sigmoid activations).
- **Training**: Alternating updates — discriminator trained to classify real vs. fake, generator trained to fool discriminator. Loss is binary cross-entropy.
- **Trick**: Labels are randomized slightly (±0.05) to stabilize training.

#### Denoising Diffusion Models

- **Forward Process**: Adds Gaussian noise over T steps: \( X_t = \sqrt{\alpha_t} X_0 + \sqrt{1-\alpha_t} \epsilon \), where \( \alpha_t = \prod_{s=1}^t (1-\beta_s) \).
- **Reverse Process**: Trains a neural network \( \epsilon_\vartheta(X_t, t) \) to predict the noise \( \epsilon \) added at step t. Sampling starts from \( X_T \sim \mathcal{N}(0, I) \) and iteratively denoises.
- **Architecture**: Not specified — the chapter notes that diffusion models are typically implemented with U-Net architectures in image tasks, but no code or specific network is provided for actuarial data.

#### Decoder Transformer

- **Architecture**: Auto-regressive model that factorizes \( p(X_{1:T}) = \prod_{t=1}^T p(X_t | X_{1:t-1}) \).
- **Self-Attention**: Uses causal masking to ensure each token attends only to previous tokens. Positional embeddings are static (not learned).
- **Output**: Projects hidden state to vocabulary space, applies softmax with temperature \( \tau \) for calibration.
- **Training**: Teacher forcing — uses ground-truth tokens during training.
- **Inference**: Autoregressive sampling — generates one token at a time.

### 7. Technical Content

Deep generative models (DGMs) represent a paradigm shift from traditional supervised learning in actuarial science. While supervised models predict point estimates (e.g., claim amounts), DGMs aim to learn the full probability distribution \( p(X) \) of the data or joint distributions \( p(Y, X) \). This enables richer applications such as data augmentation, anomaly detection, scenario simulation, and fairness analysis.

The chapter begins by contrasting DGMs with supervised learning, emphasizing that DGMs do not rely on labeled outcomes but instead learn the underlying data-generating process. The goal is to approximate \( p(X) \) from a finite sample \( \{X_i\}_{i=1}^n \), allowing for sampling new instances \( X' \sim p(X) \) or evaluating likelihoods \( p(X) \) for existing data points.

Two main categories of DGMs are introduced: **latent factor approaches** and **implicit probability distribution approaches**. Latent factor models assume an unobserved variable \( Z \) that explains variability in \( X \), with \( p_\vartheta(X) = \int p_\vartheta(X|z) \pi(z) dz \). Implicit models directly learn the conditional distribution \( p_\vartheta(X_{1:T}) = \prod_{t=1}^T p_\vartheta(X_t | X_{1:t-1}) \) without explicit latent variables.

#### Variational Auto-Encoders (VAEs)

VAEs were introduced by Kingma and Welling (2013) and are among the most stable and interpretable DGMs. The model consists of an encoder that maps data \( X \) to a latent distribution \( q_\vartheta(Z|X) \) and a decoder that maps latent samples \( Z \) to data \( p_\vartheta(X|Z) \). The encoder typically outputs mean and log-variance parameters of a Gaussian distribution, while the decoder outputs the mean of a Gaussian (or Bernoulli for binary data).

Training is based on maximizing the Evidence Lower Bound (ELBO), derived via Jensen’s inequality:

\[
\log p_\vartheta(X) \geq \mathbb{E}_{q_\vartheta(Z|X)}[\log p_\vartheta(X|Z)] - D_{KL}(q_\vartheta(\cdot|X) \| \pi)
\]

The first term is the reconstruction loss, encouraging the decoder to reproduce the input. The second term is a KL divergence that regularizes the encoder’s posterior to match the prior \( \pi(Z) \). This balance ensures that the latent space is well-structured and allows for sampling new data.

A key innovation is the **reparameterization trick**, which allows gradients to flow through stochastic nodes. For Gaussian distributions, \( Z = \mu_\vartheta(X) + \Sigma_\vartheta^{1/2}(X) \epsilon \), where \( \epsilon \sim \mathcal{N}(0, I) \) is independent of \( \vartheta \). This enables backpropagation through the encoder.

In the Sports Car example, a VAE with 5 input dimensions, 32 hidden units, and 10 latent dimensions is trained. The model achieves good reconstruction and captures correlations in the data, though the generated samples are less volatile because only the mean (mode) of the Gaussian decoder is used — the covariance matrix is not modeled.

#### Generative Adversarial Networks (GANs)

GANs, introduced by Goodfellow et al. (2014), frame generative modeling as a game between two networks: a generator \( G \) that creates fake data from noise, and a discriminator \( D \) that classifies real vs. fake. The objective is a minimax game:

\[
\min_G \max_D V(D, G) = \mathbb{E}_{X \sim p_{data}}[\log D(X)] + \mathbb{E}_{Z \sim \pi}[\log(1 - D(G(Z)))]
\]

The discriminator maximizes its ability to distinguish real from fake, while the generator minimizes this ability — effectively trying to fool the discriminator. Training alternates between updating the discriminator (with generator weights frozen) and updating the generator (with discriminator weights frozen).

In practice, GANs are notoriously difficult to train due to issues like mode collapse (generator produces limited variety) and vanishing gradients. The chapter implements a simple GAN in R using Keras, with a 10D latent space and 5D output. The discriminator is a 3-layer network (20 → 10 → 1 units), and the generator is 3-layer (15 → 10 → 5 units). Training uses binary cross-entropy loss and label smoothing (adding ±0.05 noise to labels) to stabilize learning.

The generated samples from the GAN are less convincing than VAE samples, though both replicate the correlation structure of the original data. This highlights a common trade-off: GANs can generate sharper samples but are harder to train, while VAEs are more stable but may produce blurrier outputs.

#### Denoising Diffusion Models

Diffusion models, popularized by Song and Ermon (2019) and Ho et al. (2020), generate data by reversing a noising process. The forward process adds Gaussian noise over T steps, transforming data \( X_0 \) into nearly pure noise \( X_T \). The reverse process learns to denoise, starting from \( X_T \sim \mathcal{N}(0, I) \) and iteratively generating \( X_{t-1} \) from \( X_t \).

The training objective is to predict the noise \( \epsilon \) added at step t:

\[
L_{simple}(\vartheta) = \mathbb{E}_{X_0, \epsilon, t}[\| \epsilon - \epsilon_\vartheta(X_t, t) \|_2^2]
\]

This is equivalent to learning the score function (gradient of log-density) and allows for high-fidelity generation, especially in images. However, sampling is slow because it requires T steps (vs. one step in VAEs/GANs).

The chapter notes that diffusion models are rarely applied in actuarial contexts and refers readers to Keras tutorials for implementation. No code or actuarial example is provided, suggesting that this is an emerging area for future research.

#### Decoder Transformer Models

Decoder transformers, such as GPT, model sequences auto-regressively. The joint distribution is factorized as \( p(X_{1:T}) = \prod_{t=1}^T p(X_t | X_{1:t-1}) \). At each step, the model uses self-attention over previous tokens to predict the next token.

The architecture includes:
- **Causal self-attention**: Each token attends only to previous tokens, enforced by a causal mask (setting future attention scores to -∞).
- **Positional embeddings**: Static functions (not learned) to encode token positions.
- **Output layer**: Projects hidden state to vocabulary space, applies softmax with temperature \( \tau \) for calibration.

Training uses **teacher forcing** — feeding ground-truth tokens during training to capture sequential dependencies. Inference is autoregressive: start with a seed token, sample next token, append, and repeat.

These models scale well with data and parameters, enabling large language models (LLMs) that can perform in-context learning (ICL) — generalizing to new tasks with appropriate prompts. Applications include text generation, summarization, translation, and code generation.

In actuarial contexts, decoder transformers could model sequences of claims, policy events, or text-based risk factors. However, the chapter does not provide an actuarial example, focusing instead on theoretical foundations.

#### Comparative Summary

- **VAEs**: Stable, interpretable, good for latent representation learning and reconstruction. Weakness: blurry samples if only mean is modeled.
- **GANs**: Can generate sharp, realistic samples. Weakness: unstable training, mode collapse.
- **Diffusion Models**: High-quality generation, stable training. Weakness: slow sampling, computationally expensive.
- **Decoder Transformers**: Excellent for sequential data, scalable. Weakness: requires large datasets, no direct application shown in actuarial context.

The chapter concludes by emphasizing that DGMs are foundational for modern LLMs and offer powerful tools for actuarial applications beyond point prediction — enabling simulation, risk assessment, and synthetic data generation. While code is provided for VAEs and GANs, diffusion models and transformers are discussed theoretically, indicating areas for future exploration.

Overall, this chapter provides a comprehensive, technically rigorous introduction to DGMs tailored for actuaries, balancing theory with practical implementation. The focus on real-world actuarial data (Sports Car dataset) grounds the concepts in domain-relevant applications, making it accessible to undergraduate STEM students with a background in probability and neural networks.


================================================
FILE: data/summaries_30B_samples/evolution-of-economic-scenario-generators-a-report-by-t_summary_20260125_150113.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** Evolution of economic scenario generators: a report by the Extreme Events Working Party members  
**Authors:** P. Jakhria, R. Frankland, S. Sharp, A. Smith, A. Rowe, T. Wilkins  
**Date of publication:** 2019-01-01 (as per journal volume; original presentation: 2018-02-22)  
**Topic:** Actuarial Science  
**Sub-topic:** Economic Scenario Generators (ESGs) and their historical evolution in insurance and pension modeling  
**URL:** https://doi.org/10.1017/S1357321718000181

---

## Summary

### 1. Modeling Techniques

The document traces the evolution of Economic Scenario Generators (ESGs) used in actuarial science, particularly in the UK insurance and pension industries. The modeling techniques discussed span multiple paradigms:

- **Random Walk Models**: Early stochastic models based on Brownian motion and geometric Brownian motion, inspired by Louis Bachelier’s 1900 thesis and later formalized by Norbert Wiener and Kiyoshi Ito. These models assume asset prices follow a random path with no memory, often used in early financial theory and option pricing.

- **Time Series Models**: Specifically, the Wilkie model (1984) is highlighted as the dominant actuarial ESG for decades. It is an ARMA (AutoRegressive Moving Average) model with a cascade structure linking inflation, equity returns, and interest rates. The model uses historical UK data (1919–1982) to estimate parameters and assumes stationarity and mean reversion in key variables.

- **Market-Consistent (MC) and Arbitrage-Free Models**: Introduced in the 1990s–2000s, these models are grounded in financial economics and option pricing theory (Black-Scholes, Merton). They ensure that asset prices are consistent with current market prices (no-arbitrage) and often use stochastic differential equations (SDEs) to model asset dynamics. Examples include Vasicek, Hull-White, and LIBOR market models for interest rates, and geometric Brownian motion with stochastic volatility or jumps for equities.

- **Value at Risk (VaR) Models**: Used primarily for regulatory capital calculations (e.g., Solvency II), these models focus on estimating the 99.5th percentile loss over a 1-year horizon. They often rely on historical data to estimate distributions and use copulas to model correlations between risk factors. Unlike full stochastic models, VaR models are typically calibrated to historical stress events rather than market prices.

The paper does not describe machine learning or deep learning techniques, as the focus is on historical evolution up to the early 2010s. The modeling approaches are primarily statistical and econometric, with increasing sophistication over time.

### 2. Code Availability

**Not available.** The document is a historical and conceptual review of ESG evolution. It does not provide implementation code, GitHub repositories, or software packages. While some models (e.g., Wilkie) were published with formulas and parameters, no executable code or open-source implementations are referenced. The paper notes that proprietary models (e.g., Towers Perrin’s system) were developed but not fully disclosed.

### 3. Learning Type

**Not applicable.** The document does not describe machine learning models trained on data. Instead, it discusses statistical and econometric models calibrated to historical data or market prices. The learning type (supervised, unsupervised, etc.) is not relevant to the techniques described, which are parametric and theory-driven rather than data-driven in the ML sense.

### 4. Dataset

The paper references several datasets and data sources, but does not provide direct access to them:

- **Historical UK economic data (1919–1982)**: Used by Wilkie to calibrate his model, including inflation, equity prices, and interest rates.
- **De Zoete equity index**: Used by the Maturity Guarantee Working Party (1980) to model equity prices.
- **ONS (Office for National Statistics) data**: Cited for UK inflation figures (Figure 5).
- **Historical market prices**: Used for calibrating arbitrage-free models (e.g., option prices for Black-Scholes).
- **Historical stress events**: Used for VaR models, though the paper notes limitations due to limited data (e.g., 10 years of falling interest rates may bias stress tests).

The datasets are real-world, not synthetic. Specific dataset names (e.g., “De Zoete index”) are mentioned, but no direct links or access instructions are provided. The paper emphasizes that data availability and quality have historically constrained model development, especially for extreme events.

### 5. Implementation Details

- **Programming language(s):** Not specified. The paper notes that the Wilkie model could be coded in spreadsheets, implying Excel or similar tools were used in practice. No specific languages (Python, R, MATLAB) are mentioned.
- **Key libraries/frameworks:** Not specified. The document predates modern ML frameworks (TensorFlow, PyTorch) and focuses on statistical methods (ARMA, GARCH, SDEs) implemented via custom code or spreadsheets. Libraries like scikit-learn or statsmodels are not referenced.

### 6. Model Architecture

The paper describes several model architectures, with the Wilkie model being the most detailed:

- **Wilkie Model (1984, updated 1995)**: A cascade-structured ARMA model with:
  - **Primary variables**: Inflation (entry point), equity returns.
  - **Secondary variables**: Long-term fixed interest yield, dividend yield.
  - **Structure**: Inflation drives equity returns and interest rates. Equity returns are modeled as a function of dividend payout (random walk) and dividend yield (mean-reverting). The Consol yield is a third-order autoregressive process.
  - **Extensions (1995)**: Added monthly time steps, full yield curve term structures, and parameter estimation error handling.

- **Arbitrage-Free Models**: Typically based on SDEs, e.g.:
  - **Black-Scholes PDE**: ∂V/∂t + ½σ²S²∂²V/∂S² + rS∂V/∂S − rV = 0, where V is option value, S is asset price, σ is volatility, r is risk-free rate.
  - **Interest rate models**: Vasicek (mean-reverting), Hull-White (extended Vasicek), Cox-Ingersoll-Ross (positive rates), LIBOR market model (for forward rates).
  - **Equity models**: Geometric Brownian motion with drift, stochastic volatility (e.g., Heston), or jumps (e.g., Merton).

- **VaR Models**: Not a single architecture but a methodology:
  - **Inputs**: Multiple risk factors (asset returns, liability stresses, correlations).
  - **Method**: Use historical data to estimate distributions, apply copulas for correlations, identify the 99.5th percentile stress (critical stress).
  - **Output**: Capital requirement to cover the worst 0.5% outcome over 1 year.

The paper notes that models are often composite, e.g., combining time series with arbitrage-free components, but does not provide architectural diagrams or code.

### 7. Technical Content

The paper provides a comprehensive historical review of Economic Scenario Generators (ESGs) in actuarial science, tracing their evolution from simple random walks to complex, regulation-driven models. The core narrative is that ESGs evolved in response to changing regulatory, technological, and economic environments, with key milestones driven by innovations in financial theory and computing power.

#### Origins and Motivation

The need for stochastic models in insurance arose from the complexity of long-dated liabilities, particularly with-profits policies, which required modeling surplus distribution. Early models were deterministic, relying on single-point estimates (e.g., best-estimate interest rates) and were adequate when insurers invested primarily in fixed-income assets with low volatility. However, as insurers began investing in equities and property (from 2% equity allocation in 1920 to 21% by 1952), the need for stochastic modeling increased to capture market risk.

The advent of electronic computers (ENIAC, 1945) and the Monte Carlo method (developed by Ulam, von Neumann, and Metropolis in the 1940s) enabled the practical simulation of stochastic paths. Monte Carlo methods rely on random number generation to simulate multiple scenarios, with early implementations using von Neumann’s “middle-square” generator. This laid the groundwork for modern ESGs, which simulate thousands of economic paths to assess risk.

#### Key Stages in ESG Evolution

The paper identifies four broad stages in ESG development:

1. **Random Walk Models (Early 20th Century)**: Based on Bachelier’s work on Brownian motion and Markowitz’s Modern Portfolio Theory (MPT). These models assumed asset prices follow a random path with no memory, capturing risk, return, and correlation. They were intuitive and easy to calibrate to historical data but failed to capture mean reversion or extreme events.

2. **Time Series Models (1980s–1990s)**: Dominated by the Wilkie model (1984), which introduced a cascade structure linking inflation, equity returns, and interest rates. Wilkie’s model was ARMA-based, with inflation as the entry point, and was designed for long-term actuarial use. It was widely adopted due to its transparency, practicality (could be coded in spreadsheets), and alignment with actuarial beliefs (e.g., mean reversion in equities). However, it was criticized for overemphasizing mean reversion and assuming normality, which limited its ability to model extreme events. The 1992 Geoghegan Working Party report highlighted these limitations, noting that actuaries without statistical training might misuse the model.

3. **Market-Consistent and Arbitrage-Free Models (1990s–2000s)**: Driven by the Black-Scholes option pricing model (1973) and the rise of financial economics. These models ensure that asset prices are consistent with current market prices (no-arbitrage) and are often used for valuing complex liabilities with embedded options (e.g., with-profits policies). Key models include Vasicek and Hull-White for interest rates, and geometric Brownian motion with stochastic volatility for equities. These models were initially slow to adopt in actuarial circles due to their complexity and the alien concept of no-arbitrage, but gained traction with regulatory changes (e.g., MC valuation requirements).

4. **VaR Models (2000s–Present)**: Introduced for regulatory capital calculations (e.g., Solvency II), these models focus on estimating the 99.5th percentile loss over 1 year. They use historical data to estimate distributions and copulas to model correlations, identifying the “critical stress” that maximizes capital requirements. While practical for regulation, they are criticized for ignoring contagion (e.g., 2008 crisis) and assuming constant volatility over time. The paper notes that VaR models are often “flat with respect to time” and may overstate risks based on limited historical data.

#### Technical Challenges and Limitations

The paper highlights several recurring challenges in ESG development:

- **Data Limitations**: Historical data is often insufficient, especially for extreme events. For example, calibrating interest rate stresses from 10 years of falling rates may overstate the risk of further declines and understate the risk of sharp rises. VaR models are particularly vulnerable to this, as they rely on historical data rather than market prices.

- **Model Complexity vs. Parsimony**: Wilkie’s model was praised for its simplicity but criticized for oversimplifying real-world behavior (e.g., ignoring jumps, negative rates). Arbitrage-free models are more sophisticated but harder to calibrate and require more judgment. The paper notes a trade-off between complexity and parsimony, with Occam’s razor often guiding model design.

- **Regulatory vs. Economic Reality**: Regulatory requirements (e.g., Solvency II’s 1-year VaR) often drive model development, but may not reflect economic reality. For example, 1-year VaR models ignore long-term risks and may not capture the true nature of liabilities. The paper suggests that ORSA (Own Risk and Solvency Assessment) may shift focus back to longer-term real-world modeling.

- **Social and Technical Criteria**: Model adoption is influenced by social factors (e.g., ease of use, auditability) as well as technical criteria (e.g., goodness of fit, back-testing). The paper notes that social criteria are rarely discussed in academic papers but are crucial in practice.

#### Current and Future Directions

The paper concludes with a discussion of future trends:

- **Regulatory Influence**: ORSA and MiFID II may drive a renewed focus on longer-term real-world modeling and greater transparency to customers. This could lead to more sophisticated ESGs that incorporate GDP growth (rather than inflation) as a key variable.

- **Technology**: Advances in cloud computing and big data may enable greater granularity, but complexity must be balanced with parsimony. Machine learning is mentioned as a potential disruptor, though constrained by the need for real-time data.

- **International Perspectives**: The paper notes differences in ESG use across countries, e.g., Canada’s regime-switching models (Mary Hardy), the US’s minimal profession-wide ESG, and Switzerland’s regulator-led development. These could inform future UK models.

- **Terminology**: The acronym “ESG” is noted to be at risk due to its association with Environmental, Social, and Governance factors. The paper suggests alternatives like CMM (Causal Capital Markets Model) or SAM (Stochastic Asset Model).

In summary, the paper provides a rich historical account of ESG evolution, emphasizing the interplay between theory, regulation, and practice. It highlights the ongoing tension between model sophistication and practicality, and suggests that future developments will be shaped by regulatory changes, technological advances, and a renewed focus on long-term economic reality.


================================================
FILE: data/summaries_30B_samples/ifoa-response-to-treasury-select-committee-consultation_summary_20260125_150113.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** Treasury Committee Inquiry: AI in Financial Services — IFoA Response  
**Authors:** Institute and Faculty of Actuaries (IFoA), Steven Graham (Technical Policy Manager)  
**Date of publication:** 2025-04-11  
**Topic:** Artificial Intelligence in Financial Services  
**Sub-topic:** Regulatory, Ethical, and Operational Implications of AI Adoption in Insurance, Pensions, Investment, and Risk Management  
**URL:** Not available (document provided as text; original source likely actuaries.org.uk)

---

## Summary

### 1. Modeling Techniques

The document does not describe specific machine learning or AI algorithms (e.g., CNNs, LSTMs, XGBoost) in technical detail. Instead, it broadly references **narrow AI** (primarily data science and machine learning) and **Generative AI (GenAI)** as the two main categories of AI currently in use within financial services. The focus is on *application domains* rather than model architectures.

Examples of AI applications mentioned include:
- **Predictive analytics** for underwriting, pricing, and customer segmentation in general and life insurance.
- **Fraud detection models** using anomaly detection techniques.
- **Personalized disease management programs** in health insurance.
- **Sentiment analysis** of unstructured data (news, social media, earnings calls) for investment decisions.
- **Climate risk modeling** and ESG impact analysis using AI-enhanced catastrophe models.
- **RegTech applications** for automated compliance, contract analysis, and real-time risk monitoring.
- **Chatbots and voice assistants** for customer service (with human oversight for complex queries).
- **Retrieval Augmented Generation (RAG)** and **prompt engineering** to mitigate hallucinations in GenAI outputs.

No specific neural network architectures, ensemble methods, or deep learning frameworks are detailed. The emphasis is on *functional use cases* rather than algorithmic implementation.

### 2. Code Availability

**Not available.** The document does not mention any publicly available code repositories, GitHub links, or open-source implementations of the AI systems discussed. The IFoA’s work is policy-oriented and focuses on guidance, ethics, and governance rather than technical code sharing.

### 3. Learning Type

The document does not specify whether the AI applications described use **supervised, unsupervised, or self-supervised learning**. However, based on the context of use cases (e.g., fraud detection, underwriting, pricing), it is reasonable to infer that most applications rely on **supervised learning**, where models are trained on labeled historical data (e.g., claims outcomes, customer churn, market movements).

Generative AI (e.g., chatbots, document summarization) likely employs **self-supervised or foundation models** (like LLMs), but this is not explicitly stated.

### 4. Dataset

The document references **real-world data** used in financial services, including:
- Customer transaction records
- Claims data
- Health and mortality records
- Market data (news, earnings calls, social media)
- Environmental and climate risk datasets
- Pension fund performance data

No specific dataset names or public sources (e.g., Kaggle, UCI, FRED) are provided. The IFoA mentions **Federated Learning** as a technique to train models without sharing raw data across insurers, implying that data is proprietary and not publicly available.

The document also highlights **data quality** and **bias in training data** as critical concerns, particularly when datasets are historic, incomplete, or unrepresentative of protected groups.

### 5. Implementation Details

- **Programming language(s):** Not specified.
- **Key libraries/frameworks:** Not specified.

The document does not discuss technical implementation tools. Instead, it references **cloud-based systems** (used by Fintech firms) and **legacy systems** (used by traditional insurers, often Excel-based). It also mentions **proprietary third-party AI tools** from large tech firms (e.g., ChatGPT), but does not name specific frameworks like TensorFlow or PyTorch.

### 6. Model Architecture

No detailed model architectures are provided. The document refers to:
- **“Black-box” AI models** — implying complex, non-interpretable models (e.g., deep neural networks or ensemble models) whose internal logic is not transparent.
- **Autonomous AI agents** — mentioned as a future trend, but not defined technically.
- **Reinforcement Learning from Human Feedback (RLHF)** — referenced as a technique to mitigate herding behavior in financial models, but no architecture details are given.

The focus is on *functional outcomes* (e.g., “AI improves pricing accuracy”) rather than *technical design*.

### 7. Technical Content

The IFoA’s response to the UK Treasury Committee’s inquiry on AI in financial services is a comprehensive, policy-oriented analysis of the opportunities, risks, and governance challenges associated with AI adoption. It is written from the perspective of the actuarial profession, which the IFoA positions as uniquely qualified to navigate the ethical, technical, and regulatory complexities of AI due to its long-standing expertise in risk modeling, data analysis, and ethical decision-making.

#### AI Adoption Across Financial Services

AI is already embedded across multiple domains within financial services, with actuaries playing a central role in its application. Key areas include:

- **General Insurance**: AI enhances underwriting by analyzing granular datasets to improve risk segmentation and pricing. It also automates claims processing and detects fraud through pattern recognition. Customer segmentation and targeted marketing are optimized using predictive analytics.
  
- **Health & Care Insurance**: AI enables personalized disease management programs, helping both patients and insurers by improving health outcomes and reducing claims costs.

- **Life Insurance**: AI refines mortality and longevity predictions, allowing for more accurate pricing. Underwriting triage systems speed up the application process by flagging high-risk cases for human review.

- **Pensions**: AI analyzes investment performance, assesses long-term risk scenarios, and improves member engagement by providing personalized retirement planning insights.

- **Investments**: AI processes vast amounts of unstructured data (news, social media, earnings calls) to detect market trends, optimize asset allocation, and personalize investment strategies.

- **Risk Management**: AI monitors news and data feeds to identify emerging risks, helping risk managers adjust strategies proactively.

- **Environmental Risk Analysis**: AI models assess climate risks, perform catastrophe modeling, and evaluate ESG impacts, aiding firms in managing sustainability-related exposures.

Additionally, AI is used to automate financial reporting, audit trails, and compliance processes, reducing manual effort and improving consistency.

#### Productivity Gains and Use Cases

AI has significant potential to improve productivity by automating routine, low-risk tasks such as:
- Generating meeting minutes and model documentation
- Processing claims and underwriting applications
- Cleansing and assembling data
- Performing compliance checks and reserving calculations
- Providing 24/7 customer service via chatbots (with human oversight for complex issues)
- Detecting fraud and anomalies in transactions
- Personalizing user interfaces and financial advice
- Improving risk assessment and capital modeling

The IFoA emphasizes that while AI can automate repetitive tasks, it also creates opportunities for employees to focus on higher-value activities, such as interpreting AI outputs, managing model governance, and ensuring ethical compliance.

#### Barriers to Adoption

Despite its potential, AI adoption faces several barriers:
- **Cost**: AI transformation requires significant investment in infrastructure, consultancy, and training.
- **Workforce Training**: Financial professionals need upskilling to work effectively with AI tools.
- **Data Quality**: AI models depend on high-quality, unbiased data; poor data leads to flawed outputs.
- **Regulatory Uncertainty**: Firms are cautious due to unclear governance frameworks and the risk of regulatory lag.
- **Cultural Resistance**: Some firms are hesitant due to fear of cyber threats, data leaks, or ethical concerns.
- **Legacy Systems**: Many insurers still rely on Excel spreadsheets and non-cloud systems, making AI integration difficult.
- **Explainability**: “Black-box” models lack transparency, complicating regulatory approval.
- **GenAI Hallucinations**: Generative AI can produce plausible but incorrect outputs, requiring safeguards like RAG and prompt engineering.

#### Job Displacement and Reskilling

AI is likely to displace administrative and manual processing roles, particularly in claims, underwriting, and data entry. However, the IFoA highlights opportunities for reskilling into roles focused on:
- Interpreting AI outputs
- Model governance and risk management
- AI ethics and bias mitigation
- Data quality and explainability

New roles will require multidisciplinary expertise in algorithms, governance, and ethics.

#### Risks to Financial Stability

AI introduces several systemic risks:
- **Cybersecurity**: AI-driven attacks can exploit vulnerabilities at scale, amplified by interconnected systems. Open-source AI models may contain hidden vulnerabilities.
- **Third-Party Dependencies**: Reliance on a few large tech firms (e.g., for LLMs) creates systemic risk. Firms may lack clarity on model complexity or data usage.
- **Model Complexity**: “Black-box” models are hard to audit, increasing the risk of unintended consequences.
- **Herding Behavior**: Financial models trained on similar data may lead to market instability. RLHF is proposed as a mitigation.
- **GenAI Hallucinations**: Outputs that are plausible but incorrect can undermine trust and require costly verification.

Mitigations include:
- Upgrading cyber resilience
- Performing independent AI audits
- Stress testing models
- Requiring explainability
- Diversifying AI providers

#### Consumer Benefits and Risks

AI offers consumers:
- More accurate risk assessments (potentially lowering premiums for low-risk individuals)
- Personalized financial products and advice
- Improved fraud detection
- Better engagement with pensions and investments
- Identification of vulnerabilities through real-time data analysis

However, risks include:
- **Bias and Discrimination**: AI may replicate or amplify existing biases, particularly if trained on historic, unrepresentative data. For example, using postcode as a proxy for ethnicity can lead to unfair pricing.
- **Financial Exclusion**: Overly accurate risk assessment may render products unaffordable for high-risk individuals.
- **Lack of Transparency**: Consumers may not understand or challenge AI-driven decisions.
- **Mis-selling**: AI-driven sales processes may target vulnerable consumers without adequate safeguards.

The IFoA recommends:
- **Ethics by Design**: Embedding ethical principles into model development.
- **Bias Testing**: Mandatory testing for discrimination, even if using third-party tools.
- **Transparency**: Clear communication of AI decisions to consumers.
- **Redress Mechanisms**: Easy avenues for consumers to challenge AI outcomes.
- **Federated Learning**: Training models on pooled data without sharing raw data, preserving privacy.

#### Regulatory Approach

The IFoA advocates for a **principles-based, rather than rules-based**, regulatory framework to keep pace with AI’s rapid evolution. Key recommendations include:
- Adapting existing conduct and prudential regulations to cover AI, avoiding duplication.
- Focusing on how AI is used in context (e.g., consumer fairness, financial inclusion).
- Balancing risk management with innovation to maintain trust.
- Using regulatory sandboxes to test AI innovations responsibly.
- Aligning with global standards to ensure a level playing field.

The IFoA also emphasizes the need for:
- **Firm Governance**: Ensuring AI governance is embedded in second and third lines of defense.
- **Staff Training**: Equipping employees with knowledge of AI ethics and limitations.
- **Multidisciplinary Collaboration**: Engaging academia, industry, and regulators to develop best practices.

#### Energy and Climate Impact

AI’s energy consumption is a growing concern. The International Energy Agency predicts global data center energy use could double by 2030, conflicting with climate goals. The IFoA calls for solutions like:
- Locational flexibility (placing data centers near renewable sources)
- Operational flexibility (adjusting usage based on energy availability)
- Increased use of renewables

#### Conclusion

The IFoA positions actuaries as essential stakeholders in the responsible adoption of AI in financial services. Their skills in critical thinking, ethics, and risk management are crucial for navigating AI’s complexities. The document concludes that while AI offers transformative benefits, its risks — including bias, opacity, and systemic fragility — require proactive governance, ethical oversight, and global collaboration. The UK, with its strong Fintech ecosystem and AI research, is well-placed to lead in responsible AI adoption, provided regulation balances innovation with consumer protection.

The IFoA invites further engagement with policymakers to ensure AI evolves in a way that serves the public interest, promotes financial stability, and enhances consumer welfare.


================================================
FILE: data/summaries_30B_samples/IFoA_WP_DSSCC_-_Green_Bonds_Paper_2__5BAugust_2024_5D_summary_20260125_150113.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** Time series analysis of GSS bonds: Part 2 – Further univariate analysis of S&P Green Bond Index  
**Authors:** D Dey (on behalf of the IFoA Data Science, Sustainability & Climate Change Working Party)  
**Date of publication:** 2024-08  
**Topic:** Time series forecasting using neural networks  
**Sub-topic:** Univariate prediction of S&P Green Bond Index using N-BEATS and extended input windows  
**URL:** Not available  

---

## Summary

### 1. Modeling Techniques

The paper employs a suite of neural network architectures for univariate time series forecasting of the S&P Green Bond Index. The primary techniques include:

- **N-BEATS (Neural Basis Expansion Analysis for interpretable Time Series forecasting)**: A deep feedforward architecture composed of stacks of blocks that perform doubly residual stacking—producing both backcasts (residuals) and partial forecasts. Each block contains fully connected layers followed by basis function layers that map expansion coefficients to outputs. The architecture is designed to be interpretable and was originally shown to outperform M4 competition winners by 3% (Oreshkin et al., 2020). In this study, a generic (non-interpretable) variant was used with one stack (M=1), varying numbers of blocks (K=2–30), and hidden layers (1–4) per block.

- **Deep Neural Networks (DNNs)**: Feedforward networks with 1 to 3 hidden dense layers. These were retrained with input windows of 1, 2, and 5 days to assess the impact of longer historical context.

- **Convolutional Neural Networks (CNNs)**: 1D CNNs with one convolutional layer, optionally followed by one dense hidden layer. These were also retrained with 2- and 5-day input windows.

- **Long Short-Term Memory (LSTM) networks**: Recurrent networks with one LSTM layer (return_sequences=False) and optionally one additional dense layer. Variants include 0 or 1 hidden layer(s) after the LSTM.

- **Gated Recurrent Units (GRUs)**: Similar to LSTMs, with one GRU layer and optionally one dense layer. These were also retrained with extended input windows.

All models were trained to predict a 1-day-ahead horizon. The baseline model used for comparison is a naive persistence model: “today’s index value equals yesterday’s value.”

### 2. Code Availability

Implementation code is **not publicly available** in the document. The authors mention using Google Colab for training and reference code from the “Zero to Mastery TensorFlow for Deep Learning Book” (Bourke, 2023), but no GitHub repository or public code link is provided. Hyperparameter tuning was performed using Optuna, but no configuration files or scripts are shared.

### 3. Learning Type

All models use **supervised learning**. The task is framed as a regression problem: given a sequence of past index values (input window), predict the next day’s index value (output horizon). Labels are the actual daily index values from the S&P Green Bond Index dataset.

### 4. Dataset

- **Dataset name/source**: S&P Green Bond Index (Total Performance, USD)
- **Time range**: 31 January 2013 to 17 February 2023 (2,615 daily observations)
- **Data type**: Real-world financial time series data
- **Splits**: Chronologically ordered 70% training (31 Jan 2013–16 Feb 2020), 20% validation (17 Feb 2020–16 Feb 2022), 10% test (17 Feb 2022–17 Feb 2023)
- **Characteristics**: 
  - Index range: 109.80 to 158.99
  - Mean: 136.00, Std Dev: 9.65
  - Test set exhibits higher volatility (std dev 8.04) than training/validation sets (std dev ~5.3–5.7), making it a challenging out-of-sample prediction task.
- **Preprocessing**: No normalization or log transformation was applied, to preserve original scale and avoid assumptions about stationarity or seasonality.

### 5. Implementation Details

- **Programming language(s)**: Python
- **Key libraries/frameworks**:
  - TensorFlow and Keras (for model building and training)
  - Optuna (for hyperparameter optimization using TPE algorithm)
  - Statsmodels (for ACF/PACF and seasonal decomposition plots)
  - Matplotlib/Seaborn (for visualizations, though not explicitly named)
  - Google Colab (execution environment)

### 6. Model Architecture

#### N-BEATS Architecture (Generic Variant)

The N-BEATS model used in this study is a simplified version of the original architecture, configured as follows:

- **Stacks**: 1 stack (M=1) — the authors chose this to reduce training time (up to 4 hours) and complexity.
- **Blocks per stack**: 2 to 30 blocks (K), optimized via Optuna.
- **Block structure**:
  - **Fully connected section**: 1–4 hidden layers (each with 4–512 neurons), using ReLU (or other activation functions from a list: elu, gelu, linear, relu, sigmoid, swish, tanh). Each hidden layer is followed by a linear projection layer to generate expansion coefficients for backcast (θᵇ) and forecast (θᶠ).
  - **Basis layer**: Maps θᵇ and θᶠ to outputs via dense layers (no basis functions for trend/seasonality were used — generic mode). Output layer size equals input window + output horizon (e.g., 1+1=2 for 1-day window).
- **Doubly residual stacking**: Each block outputs a backcast residual (x̂ₗ = xₗ₋₁ - x̂ₗ₋₁) and a partial forecast (ŷₗ). The global forecast is the sum of all partial forecasts: ŷ = Σŷₗ.
- **Regularization**: L2 regularization applied to hidden layers (search space: 1e-5 to 1e-1).
- **Loss & Optimizer**: MAE loss with Adam optimizer (learning rate: 1e-5 to 1e-1).

#### DNN, CNN, LSTM, GRU Architectures (Extended Input Windows)

These models were retrained with 2-day and 5-day input windows (previously only 1-day). Architectures are identical to those in the initial paper (Dey, 2024):

- **DNN**: Feedforward networks with 1, 2, or 3 dense hidden layers (e.g., DNN0, DNN1, DNN2).
- **CNN**: 1D convolutional layer (kernel size=2 or 5) followed by optional dense layer (CNN0, CNN1).
- **LSTM/GRU**: Single LSTM or GRU layer (return_sequences=False) with optional dense layer (e.g., LSTM_0HL_F, LSTM_1HL_F, GRU_0HL_F, GRU_1HL_F).

All models use MAE loss, Adam optimizer, and L2 regularization. Hyperparameters (neurons, layers, activations, learning rate) were tuned via Optuna (30 trials, 400 epochs, batch size=128).

### 7. Technical Content

This paper extends prior work on forecasting the S&P Green Bond Index using neural networks by introducing the N-BEATS architecture and expanding input windows for existing models (DNN, CNN, LSTM, GRU). The goal is to improve prediction accuracy over a simple baseline (yesterday’s value = today’s value) for 1-day-ahead forecasts.

#### Background and Motivation

Green, Social, and Sustainability (GSS) bonds are a rapidly growing asset class (USD 939 billion issued in 2023), yet research on their time series behavior is limited. The authors aim to explore whether advanced neural architectures can capture patterns in this relatively new financial instrument. The study deliberately excludes traditional methods (e.g., ARIMA) and multivariate analysis to focus on pure neural network performance and interpretability. The S&P Green Bond Index (2013–2023) is used as the target variable, split chronologically into training (70%), validation (20%), and test (10%) sets to preserve temporal order.

#### N-BEATS Implementation and Results

N-BEATS was chosen for its state-of-the-art performance in time series forecasting (notably, 3% better than M4 winner) and its interpretable design. However, the authors implemented a “generic” variant without explicit trend/seasonality decomposition to maintain neutrality. The model was trained with input windows of 1, 2, and 5 days, predicting 1 day ahead.

Key implementation details:
- **Architecture**: 1 stack, 2–30 blocks, 1–4 hidden layers per block, 4–512 neurons per layer, ReLU or other activations, L2 regularization (1e-5 to 1e-1), MAE loss, Adam optimizer (lr 1e-5 to 1e-1).
- **Training**: 400 epochs, 30 Optuna trials, batch size 128, Google Colab environment.
- **Complexity**: The final model (1-day window) had over 4 million parameters — vastly more than DNNs (~625) or LSTMs (~47,629).

Results (Table 3):
- **MAE**: Baseline = 0.610; N-BEATS (1-day) = 0.620 (+0.010); N-BEATS (2-day) = 0.662 (+0.052); N-BEATS (5-day) = 0.657 (+0.047).
- **MAPE**: Baseline = 0.497%; N-BEATS (1-day) = 0.505% (+0.008%); N-BEATS (2-day) = 0.539% (+0.042%); N-BEATS (5-day) = 0.536% (+0.039%).

All N-BEATS variants underperformed the baseline. The 1-day window model was the best among N-BEATS, suggesting that adding more historical data did not help. The authors note that the baseline’s strong performance (MAPE ~0.5%) may reflect the index’s low volatility (coefficient of variation ~7.1%) or a “random walk” nature, making it hard to beat.

#### Extended Input Windows for DNN, CNN, LSTM, GRU

The authors retrained models from their initial paper with 2-day and 5-day input windows (previously only 1-day). This was motivated by ACF/PACF analysis (Section 4.2), which suggested potential 2-day lags, and prior literature suggesting 5-day lags. Seasonal decomposition of log returns showed weak trend/seasonality, supporting the use of longer windows.

Results (Tables 4–6):
- **Overall**: No model improved over the baseline (MAE 0.610, MAPE 0.497%) when using 2- or 5-day windows. In fact, most models performed worse.
- **Best performer**: GRU_1HL_F (Model 12) with 2-day window had MAE 0.623 (vs. 0.638 for 1-day) — a slight improvement, but still worse than baseline. Its MAPE was 0.507% (vs. 0.519% for 1-day), also better but not baseline-beating.
- **Worst performers**: DNN2 (Model 3) with 2-day window had MAE 1.320 (+0.700 over 1-day) and MAPE 1.085% (+0.580%).
- **Trends**: 
  - GRU and LSTM models generally outperformed DNN and CNN.
  - Extending to 5 days sometimes improved over 2 days (e.g., Models 3, 6, 10), but never over baseline.
  - The 5-day window often produced higher errors than 2-day, suggesting overfitting or noise amplification.

#### Interpretation and Limitations

The consistent failure to beat the baseline suggests that:
1. The S&P Green Bond Index may follow a near-random walk, where past values offer little predictive power beyond persistence.
2. Univariate analysis may be insufficient — the models lack external context (e.g., market indices, oil prices) that could provide meaningful signals.
3. The N-BEATS architecture, while powerful in general, may not be well-suited to this specific dataset or problem setup (e.g., generic mode without trend/seasonality, single stack).

The authors acknowledge that ensemble techniques (used by N-BEATS original authors) or multivariate inputs (e.g., as in Wang et al., 2021 with CEEMDAN-LSTM) could improve performance. They also note that longer windows (7 or 148 days) were tested but performed worse than 5 days.

#### Conclusions and Next Steps

The paper concludes that:
- N-BEATS and extended-window DNN/CNN/LSTM/GRU models did not materially outperform the naive baseline (differences up to +0.04% MAPE for N-BEATS, +0.3% for others).
- Results are inconclusive — the baseline’s simplicity and accuracy make it a tough benchmark.
- Future work will explore:
  1. Multivariate analysis (incorporating stock market, oil prices, etc.).
  2. Broader GSS bond indices and longer time ranges.
  3. Other architectures (e.g., TFT, Graph Neural Networks).
  4. Longer forecast horizons (1 week or 1 month).

In summary, while the paper rigorously tests advanced neural architectures on a novel financial time series, it finds no clear advantage over a simple persistence model. This highlights the challenge of forecasting GSS bond indices and suggests that future research should incorporate external variables or more sophisticated ensemble methods.

**Word count**: ~1500


================================================
FILE: data/summaries_30B_samples/Key_findings_from_the_data_science_thematic_review_summary_20260125_150113.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** Key findings from the data science thematic review  
**Authors:** Alan Marshall, David Gordon, Neil Buckley (IFoA)  
**Date of publication:** 2024-03-19 (webinar date); report published 2024-02-26  
**Topic:** Actuarial science and data science/AI integration  
**Sub-topic:** Risk, regulation, and professional practice in actuarial data science  
**URL:** Not available (webinar hosted internally by IFoA; recording available via Virtual Learning Environment — access restricted)  

---

## Summary

### 1. Modeling Techniques

The document does not describe specific machine learning or artificial intelligence modeling techniques (e.g., neural networks, random forests, gradient boosting, etc.). Instead, it focuses on the *application context* of data science and AI within actuarial practice. The review examines how actuaries are *using* or *considering using* data science tools — not the technical specifications of the models themselves. No algorithms, architectures, or model types are detailed. The emphasis is on governance, ethics, and professional standards rather than technical implementation.

**Not specified** — The document is a thematic review of professional practice, not a technical paper on model development.

### 2. Code Availability

No implementation code is mentioned or made available. The document is a high-level professional review and webinar summary, not a research paper or software publication. There is no reference to GitHub repositories, code sharing platforms, or open-source implementations.

**Not available**

### 3. Learning Type

The document does not discuss any specific machine learning paradigm (supervised, unsupervised, self-supervised, reinforcement learning). It does not describe training procedures, label availability, or learning objectives. The focus is on the *role of actuaries* in data science projects, not on the learning mechanisms of models.

**Not specified**

### 4. Dataset

No datasets are referenced. The review does not mention any specific data sources, whether real-world or synthetic, nor does it describe data characteristics, size, structure, or provenance. The discussion is centered on professional practice, regulatory considerations, and case studies of actuarial involvement in data science — not on data inputs or empirical analysis.

**Not specified**

### 5. Implementation Details

- **Programming language(s):** Not mentioned. No coding languages are referenced in the context of implementation.
- **Key libraries/frameworks:** Not mentioned. There is no reference to TensorFlow, PyTorch, scikit-learn, or any other software tools used in model development.

The document is not technical in nature — it does not address implementation details. It is a policy and practice-oriented review.

### 6. Model Architecture

No model architectures are described. The document does not discuss neural network layers, ensemble structures, feature engineering pipelines, or model fusion techniques. The focus is on the *professional and regulatory environment* surrounding the use of AI/data science by actuaries, not on model design or construction.

**Not specified**

### 7. Technical Content

The document is a summary of a thematic review conducted by the Institute and Faculty of Actuaries (IFoA) titled *“Actuaries using data science and artificial intelligence techniques.”* It was presented in a webinar on March 19, 2024, and the full report was published on February 26, 2024. The review is not a technical paper on modeling or algorithmic innovation but rather a professional and regulatory assessment of how actuaries are engaging with data science and AI in practice.

#### Purpose and Scope of the Review

The IFoA conducts regular thematic reviews to evaluate how actuaries perform their work in real-world settings. This particular review focuses on the growing intersection of actuarial science with data science and artificial intelligence. It aims to:

- Identify how actuaries are currently applying or planning to apply data science and AI in their work.
- Examine the risks and challenges associated with these applications.
- Assess the evolving regulatory and standards landscape globally.
- Provide guidance to actuaries, regulators, and employers on best practices and professional responsibilities.

The review is not intended to be a technical manual but rather a risk and governance framework for professionals navigating the adoption of AI and data science tools.

#### Key Findings from the Review

The webinar and report highlight several key areas of concern and opportunity for actuaries engaging with data science and AI:

##### 1. Actuarial Involvement in Data Science Projects

Actuaries are increasingly involved in data science initiatives, particularly in insurance, pensions, and financial services. Their traditional skills in statistical modeling, risk assessment, and financial forecasting are highly transferable to data science roles. However, the review notes that actuaries often participate in these projects in supporting or advisory capacities rather than as lead data scientists.

- Actuaries are frequently involved in model validation, governance, and interpretation rather than model building.
- Many actuaries are upskilling in programming, machine learning, and data visualization to remain competitive.
- There is a growing demand for actuaries who can bridge the gap between technical data science teams and business stakeholders.

##### 2. Areas of Potential Risk

The review identifies several risks associated with the use of AI and data science by actuaries:

- **Model Risk:** Actuaries may not fully understand the inner workings of complex machine learning models (e.g., deep learning, ensemble methods), leading to potential misinterpretation or misuse.
- **Bias and Fairness:** AI models can inadvertently perpetuate or amplify biases present in training data, which is particularly concerning in insurance pricing and underwriting.
- **Transparency and Explainability:** Many AI models are “black boxes,” making it difficult for actuaries to explain decisions to regulators, clients, or auditors — a core requirement in actuarial practice.
- **Regulatory Compliance:** Actuaries must ensure that AI-driven decisions comply with local and international regulations (e.g., GDPR, Solvency II, IFRS 17).
- **Professional Accountability:** Actuaries must take responsibility for the outcomes of AI models they endorse or use, even if they did not build them.

##### 3. Regulatory and Standards Landscape

The review emphasizes that the regulatory environment for AI and data science is rapidly evolving. Key developments include:

- **Global Standards:** Organizations such as the International Actuarial Association (IAA) and the International Organization of Securities Commissions (IOSCO) are developing guidelines for the use of AI in financial services.
- **National Regulations:** Countries are introducing AI-specific regulations (e.g., EU AI Act, UK AI Regulation White Paper) that may impact actuarial work.
- **Professional Standards:** The IFoA is updating its Code of Conduct and Technical Standards to address AI and data science. Actuaries are expected to adhere to principles of fairness, transparency, and accountability when using these technologies.

##### 4. Case Studies

The review includes several case studies (though not detailed in the webinar summary) illustrating how actuaries are applying data science and AI in practice. These include:

- **Insurance Pricing:** Using machine learning to refine risk segmentation and pricing models.
- **Claims Prediction:** Applying predictive analytics to estimate future claims and reserve requirements.
- **Customer Behavior Analysis:** Leveraging AI to understand policyholder behavior and improve retention.
- **Fraud Detection:** Implementing anomaly detection algorithms to identify suspicious claims.

In each case, the review highlights the importance of actuarial oversight, model validation, and ethical considerations.

##### 5. Recommendations for Actuaries

The IFoA provides several recommendations for actuaries engaging with data science and AI:

- **Upskill Continuously:** Actuaries should develop foundational knowledge in data science, machine learning, and programming (e.g., Python, R).
- **Collaborate Across Disciplines:** Work closely with data scientists, IT professionals, and legal/compliance teams to ensure robust model development and governance.
- **Prioritize Explainability:** Choose or adapt models that can be explained to stakeholders, even if they are less accurate than “black box” alternatives.
- **Document Thoroughly:** Maintain clear documentation of model assumptions, data sources, validation procedures, and limitations.
- **Engage with Regulators:** Proactively communicate with regulators about AI usage and seek guidance when necessary.

##### 6. Future Outlook

The review concludes that the integration of data science and AI into actuarial practice is inevitable and beneficial — but must be managed responsibly. Actuaries have a unique opportunity to lead in the ethical and professional use of these technologies. The IFoA plans to continue monitoring this space and updating its guidance as the landscape evolves.

#### Webinar Structure and Speakers

The webinar featured three key speakers:

- **Alan Marshall (IFoA Review Actuary):** Led the thematic review and presented the key findings.
- **David Gordon (IFoA Senior Review Actuary):** Provided additional insights on regulatory and governance issues.
- **Neil Buckley (IFoA Regulatory Board Chair):** Discussed the broader regulatory implications and future direction.

The webinar was recorded and made available on the IFoA’s Virtual Learning Environment (VLE), though access is restricted to IFoA members or registered users.

#### Limitations of the Document

It is important to note that this document is not a technical report on AI or data science modeling. It does not provide:

- Technical details on algorithms or architectures.
- Code or implementation examples.
- Dataset descriptions or empirical results.
- Performance metrics or evaluation methods.

Instead, it serves as a professional and regulatory guide for actuaries navigating the use of AI and data science in their work.

#### Implications for Undergraduate STEM Students

For undergraduate students in STEM fields (particularly those interested in actuarial science, data science, or financial engineering), this review highlights several important trends:

- **Interdisciplinary Skills Are Essential:** Future actuaries will need to combine traditional actuarial knowledge with data science skills.
- **Ethics and Governance Matter:** Technical proficiency alone is not sufficient — understanding regulatory, ethical, and professional responsibilities is critical.
- **Lifelong Learning Is Required:** The rapid evolution of AI and data science means continuous learning and adaptation are necessary.
- **Communication Is Key:** Being able to explain complex models to non-technical stakeholders is a valuable skill.

Students should consider courses in machine learning, programming, ethics, and regulatory compliance to prepare for careers at the intersection of actuarial science and data science.

#### Conclusion

The IFoA’s thematic review on actuaries using data science and AI is a timely and important contribution to the professional discourse. While it does not delve into technical modeling details, it provides a comprehensive overview of the risks, challenges, and opportunities associated with the use of these technologies in actuarial practice. The review underscores the need for actuaries to embrace data science while maintaining the highest standards of professionalism, ethics, and accountability.

For technical researchers, this document serves as a reminder that the adoption of AI and data science is not just a technical challenge — it is also a professional, ethical, and regulatory one. Future research should consider these dimensions alongside algorithmic innovation.

---

**Word count (Summary section only):** ~1520 words


================================================
FILE: data/summaries_30B_samples/Modular_Framework_of_Machine_Learning_Pipeline__281_29_summary_20260125_150113.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** Modular Framework of Machine Learning Pipeline  
**Authors:** John Ng MA FIA BPharm  
**Date of publication:** 2020-09-14  
**Topic:** Machine Learning Pipeline Design  
**Sub-topic:** Modular ML Framework for Actuarial and Insurance Applications  
**URL:** Not available  

---

## Summary

### 1. Modeling Techniques

The document does not focus on a single modeling technique but instead presents a **modular framework** that can accommodate a wide variety of machine learning and statistical algorithms. The modeling module is designed to be flexible and agnostic to the specific algorithm used. The following techniques are explicitly mentioned as potential candidates within the modeling module:

- **Supervised Learning Algorithms:**
  - Linear Regression
  - Generalized Linear Models (GLM) with Regularization (e.g., Lasso)
  - Decision Trees
  - Random Forest
  - Extreme Gradient Boosting (XGBoost)
  - Gradient Boosted Machines (GBM)
  - Support Vector Machines (SVM)
  - Artificial Neural Networks (ANN)
  - Survival Modeling (Cox Proportional Hazards, Deep Survival Analysis)
  - Natural Language Processing (NLP) models for text classification and sentiment analysis
  - K-Nearest Neighbors (KNN)
  - Custom models (user-defined)

- **Unsupervised Learning Algorithms:**
  - K-means Clustering

- **Feature Engineering Techniques:**
  - Manual transformations (logarithmic, polynomial, splines, fractional polynomial, ratios, one-hot encoding, binning, aggregation)
  - Automated feature engineering via PCA, interaction extraction from Random Forest, autoencoders
  - Representation learning

- **Model Selection and Optimization:**
  - Model cataloging
  - Hyperparameter tuning
  - Performance metric selection (e.g., AUC, custom value-based metrics)
  - “No Free Lunch Theorem” acknowledgment — no single algorithm is universally optimal

The document emphasizes **algorithmic diversity** and **model experimentation**, advocating for a “league” approach where multiple models are trained and compared to select the best performer for a given business problem. It also highlights the importance of **custom performance metrics** tailored to business value (e.g., in fraud detection: maximizing net savings = true positive savings minus false positive investigation costs).

### 2. Code Availability

**Not available.** The document is a conceptual and strategic presentation by John Ng, aimed at actuaries and data science practitioners in insurance. It does not reference any public GitHub repository, codebase, or open-source implementation. The focus is on framework design, governance, and application rather than code-level implementation.

### 3. Learning Type

The framework supports **supervised learning** as the primary paradigm, especially for use cases like fraud detection (binary classification), risk modeling (regression or classification), and customer lifetime value prediction (regression or probabilistic modeling). 

**Unsupervised learning** is mentioned for clustering applications (e.g., customer segmentation via K-means). 

**Self-supervised learning** is not discussed.

The document does not explicitly mention reinforcement learning or semi-supervised approaches, though the modular nature of the pipeline could theoretically accommodate them.

### 4. Dataset

The document does not reference any specific, named dataset. Instead, it describes **generic data sources** relevant to insurance applications:

- **Real-world data** is implied throughout, including:
  - Claims history (frequency, amount)
  - Policyholder attributes (demographics, behavior)
  - Policy and risk characteristics
  - Fraudulent claim labels (for supervised learning)
  - Unstructured text data (emails, reports, social media for NLP)
  - Wearables, genomic, and search engine data (for mortality modeling)
  - Customer interaction logs (for CLV and churn prediction)

Datasets are described as potentially **highly imbalanced** (e.g., fraud cases are rare), requiring special handling such as class balancing or custom metrics.

Data is expected to be sourced from enterprise systems (data warehouses, CRM, claims databases) and may require **data engineering** to connect, clean, and prepare.

### 5. Implementation Details

- **Programming language(s):** Not specified. However, given the context (actuarial science, insurance, and modern ML), **Python** is strongly implied as the de facto standard for implementing such pipelines. R is also plausible in actuarial contexts, but not mentioned.

- **Key libraries/frameworks:** Not explicitly listed. However, based on the algorithms mentioned, the following are likely candidates:
  - `scikit-learn` (for traditional ML: Random Forest, SVM, GLM, KNN)
  - `XGBoost`, `LightGBM`, or `CatBoost` (for gradient boosting)
  - `TensorFlow` or `PyTorch` (for neural networks and deep learning)
  - `pandas`, `numpy`, `matplotlib`, `seaborn` (for data manipulation and EDA)
  - `imblearn` (for handling imbalanced datasets)
  - `SHAP`, `LIME` (for explainability)
  - `MLflow`, `DVC`, or `Weights & Biases` (for pipeline logging and versioning — implied by “automated logging, audit trail, version control”)

The document emphasizes **enterprise integration**, suggesting that the pipeline should be compatible with existing IT infrastructure, possibly using containerization (Docker), orchestration (Airflow, Kubeflow), or cloud platforms (AWS SageMaker, Azure ML).

### 6. Model Architecture

The document does not describe a single composite model architecture. Instead, it presents a **modular pipeline architecture** with five interconnected stages:

1. **Business Problem Module**  
   - Define problem scope, constraints, success metrics
   - Engage stakeholders and domain experts

2. **Data Module**  
   - Data sourcing, cleaning, EDA, feature engineering, imputation, data segregation (train/validation/test split)
   - Feature store for reusable features
   - Techniques: manual + automated feature engineering, PCA, autoencoders, MICE, KNN imputation

3. **Modeling Module**  
   - Algorithm selection from catalog (Random Forest, XGBoost, GLM, ANN, etc.)
   - Model training, validation, hyperparameter tuning
   - Performance evaluation using business-aligned metrics
   - Model comparison via “league” or A/B testing

4. **Deployment Module**  
   - Integration into production systems (online/offline)
   - Deployment as recommender systems, APIs, or batch prediction engines
   - Versioned models, automated retraining triggers

5. **Monitoring Module**  
   - Champion-challenger testing (A/B testing)
   - Performance degradation detection
   - Model refresh when metrics fall below threshold
   - Continuous monitoring of actual vs. expected performance

This architecture is **iterative and cyclical**, mirroring the Actuarial Control Cycle (Define → Develop → Monitor → Repeat). It supports **automated retraining**, **model versioning**, and **governance** (explainability, fairness, audit trails).

### 7. Technical Content

The document presents a comprehensive, **modular machine learning pipeline framework** tailored for actuarial and insurance applications. It is structured to address the unique challenges of the insurance industry — such as data imbalance, regulatory compliance, ethical pricing, and business impact measurement — while leveraging modern data science techniques.

#### Core Philosophy: “SPEEDS” Framework

The pipeline is designed around five key principles:

- **S**calability: Ability to handle large datasets and growing model complexity.
- **P**erformance: Focus on predictive accuracy and business value, not just statistical metrics.
- **E**thics & **R**isk Management: Incorporation of fairness, explainability, and regulatory compliance.
- **I**ntegration: Seamless connection with enterprise systems and business processes.
- **S**peed: Rapid iteration, automation, and deployment to accelerate time-to-insight.

This “SPEEDS” acronym serves as a guiding principle for evaluating and designing ML pipelines in insurance contexts.

#### Pipeline Modules in Detail

**1. Business Problem Module**

This is the foundational step. It requires:

- Clear problem definition (e.g., reduce fraud, predict mortality, optimize CLV)
- Identification of key stakeholders (claims managers, actuaries, marketers)
- Definition of success metrics that align with business outcomes (e.g., net savings from fraud detection, increase in CLV, reduction in lapse rate)
- Resource and time constraints

The document stresses that **business context is paramount** — a technically superior model is useless if it doesn’t solve the right problem or deliver measurable value.

**2. Data Module**

This is the most labor-intensive phase, involving:

- **Data Sourcing & Engineering**: Connecting to internal databases, APIs, or external data providers. Building data pipelines to ingest, store, and preprocess data.
- **Data Cleaning**: Handling missing values (via MICE, KNN, or fixed imputation), removing post-event information, correcting formatting, detecting outliers.
- **Exploratory Data Analysis (EDA)**: Visualizing distributions, correlations, and patterns. Generating summary statistics and reports.
- **Feature Engineering**: The “secret sauce” of ML. Includes:
  - Manual feature creation (domain knowledge-driven: e.g., creating ratios, interactions, time-based features)
  - Automated feature generation (PCA, autoencoders, interaction terms from tree models)
  - Feature selection (removing redundant or low-importance features)
- **Data Segregation**: Splitting data into train, validation, and test sets using random or stratified sampling to ensure representative evaluation.
- **Feature Store**: A centralized repository for reusable features, enabling consistency across models and teams.

The document notes that **feature engineering is often the most impactful step** in improving model performance, especially when domain expertise is encoded into the feature space.

**3. Modeling Module**

This module is algorithm-agnostic and designed for experimentation:

- **Algorithm Selection**: A catalog of models is maintained, including traditional (GLM, decision trees) and modern (XGBoost, neural networks) techniques.
- **Model Training**: Models are trained on the prepared dataset.
- **Validation & Tuning**: Hyperparameters are optimized using cross-validation or grid/random search. Models are evaluated using appropriate metrics (AUC, F1, custom business metrics).
- **Model Comparison**: Multiple models are trained and compared — the “league” approach. The best model is selected based on performance and business impact.
- **“No Free Lunch” Principle**: Emphasizes that no single algorithm is universally best — the choice depends on data, problem, and constraints.

The document advocates for **custom performance metrics** that reflect business value. For example, in fraud detection, the goal is not to maximize accuracy (which is misleading in imbalanced datasets) but to maximize **net savings**:  
`Net Savings = (True Positive Savings) - (False Positive Investigation Costs)`

This approach ensures that ML models are evaluated on **economic impact**, not just statistical performance.

**4. Deployment Module**

Once a model is validated, it must be deployed into production:

- **Online vs. Offline**: Models can be deployed for real-time prediction (online) or batch processing (offline).
- **Integration**: Models are embedded into business systems — e.g., as a recommender system for fraud investigators, a pricing engine, or a CLV calculator.
- **Versioning**: Models are versioned (e.g., using Git or MLflow) to track changes and enable rollback.
- **Automation**: Deployment is automated to reduce manual effort and ensure consistency.

The document highlights the importance of **seamless integration** with existing workflows — e.g., a fraud detection model should output scores that can be consumed by a claims management system.

**5. Monitoring Module**

Models degrade over time due to concept drift, data drift, or changing business conditions. The monitoring module ensures ongoing performance:

- **Champion-Challenger Testing**: A new model (challenger) is tested against the current best model (champion) using A/B testing. If the challenger performs better, it becomes the new champion.
- **Performance Degradation Detection**: Metrics are continuously monitored. If performance drops below a threshold, the model is flagged for retraining.
- **Audit & Explainability**: Models are audited for fairness, bias, and regulatory compliance. Explainability tools (SHAP, LIME) are used to interpret predictions.
- **Refresh Cycle**: Models are retrained periodically or triggered by performance drops.

This module closes the loop, ensuring that the pipeline is **adaptive and self-correcting**.

#### Governance and Ethics

The framework includes strong governance components:

- **Ethics & Fairness**: Models must be fair and unbiased, especially in pricing and underwriting.
- **Regulatory Compliance**: Adherence to data protection laws (e.g., GDPR) and industry regulations.
- **Explainability (XAI)**: Use of SHAP, LIME, permutation importance to explain model decisions.
- **Data Lineage**: Tracking data sources and transformations for auditability.
- **Access Control**: Secure access to models and data.

These elements are critical in insurance, where models directly impact customers and are subject to regulatory scrutiny.

#### Applications in Insurance

The document illustrates the framework with five key applications:

**1. Fraud Control**

- **Problem**: Detect fraudulent claims to reduce losses.
- **Challenge**: Highly imbalanced data (few fraud cases).
- **Solution**: Binary classifier with custom metric (net savings).
- **Benefits**: Faster detection, reduced investigation costs, improved profitability.

**2. Risk Modeling / Pricing**

- **Problem**: Predict risk and set accurate prices.
- **Solution**: Use ML to compare models, automate experimentation, and measure lift from external data.
- **Benefit**: More accurate pricing, freed-up time for interpretability and ethics.

**3. Customer Lifetime Value (CLV)**

- **Problem**: Predict the net present value of a customer.
- **Solution**: Modular pipeline with multiple sub-models (acquisition, churn, cross-sell, claim probability).
- **Benefit**: Targeted marketing, personalized products, optimized pricing.

**4. Mortality Modeling**

- **Problem**: Predict mortality risk using granular factors.
- **Solution**: Move beyond GLM to XGBoost, survival models, or deep learning.
- **Benefit**: Incorporate new data sources (wearables, genomics), improve underwriting.

**5. Unstructured Data (NLP)**

- **Problem**: Extract insights from text (emails, reports, social media).
- **Solution**: Use NLP transformers (tokenizers, embeddings) to convert text to structured features.
- **Benefit**: Automate claims handling, sentiment analysis, improve customer service.

#### Getting Started

The document concludes with a practical roadmap for implementation:

1. **Identify Opportunities**: Partner with business champions to find high-impact problems.
2. **Quick Wins**: Start with low-effort, high-impact projects to build momentum.
3. **Build MVP**: Create a Minimum Viable Product that is scalable and deployable.
4. **Monitor & Communicate**: Track performance, report results, and iterate.
5. **Scale & Maintain**: Expand to other use cases, automate maintenance.

It encourages actuaries — with their domain expertise and statistical background — to lead this transformation, becoming “Revolutionary” by embracing data science.

#### Conclusion

This modular ML pipeline framework provides a **structured, scalable, and business-aligned approach** to deploying machine learning in insurance. It balances technical rigor with practical considerations — from data engineering to model monitoring — while emphasizing governance, ethics, and business value. The framework is not prescriptive about algorithms or code but provides a **blueprint for building robust, enterprise-grade ML systems** that deliver real-world impact.

By adopting this framework, insurers can move from ad-hoc, siloed ML projects to a **coordinated, repeatable, and governed ML lifecycle** — accelerating innovation while managing risk and ensuring compliance. The modular design allows for flexibility, experimentation, and continuous improvement, making it a powerful tool for actuaries and data scientists alike.

The document serves as both a **strategic guide** for executives and a **technical roadmap** for practitioners, bridging the gap between actuarial science and modern data science. It is a call to action for actuaries to harness the power of ML and lead data-driven transformation in their organizations.


================================================
FILE: data/summaries_30B_samples/primer-generative-ai_summary_20260125_150113.md
================================================
## Document Metadata

**S/N:** 1  
**Title:** A Primer on Generative AI for Actuaries  
**Authors:** Stephen Carlin, FIA; Stephan Mathys, FSA  
**Date of publication:** 2024-02  
**Topic:** Generative Artificial Intelligence  
**Sub-topic:** Applications and Practical Implementation for Actuaries  
**URL:** Not available  

---

## Summary

### 1. Modeling Techniques

The document provides a high-level overview of several foundational modeling techniques used in Generative AI, without delving into implementation specifics. The techniques discussed include:

- **Neural Networks and Deep Learning**: Used broadly as the backbone of most Generative AI systems. The document references Convolutional Neural Networks (CNNs) for image tasks and Recurrent Neural Networks (RNNs), including Long Short-Term Memory (LSTM) networks, for sequential data such as text or time series.

- **Transformer Models**: Highlighted as the dominant architecture for Large Language Models (LLMs). Transformers rely on self-attention mechanisms to process entire sequences of data simultaneously, enabling them to understand context more effectively than sequential models like RNNs. GPT models (Generative Pre-trained Transformer) are cited as examples.

- **Variational Autoencoders (VAEs)**: Described as probabilistic generative models that encode input data into a latent space (compressing it) and then decode it back to reconstruct the original data. VAEs are noted for their ability to generate new data by sampling from the learned latent distribution. They are particularly useful for anomaly detection, synthetic data generation, and dimensionality reduction.

- **Generative Adversarial Networks (GANs)**: Composed of two competing neural networks — a generator that creates synthetic data and a discriminator that tries to distinguish real from fake data. The adversarial training process improves both networks iteratively. GANs are used for image and video generation, style transfer, super-resolution, and data augmentation.

The document does not specify hybrid or composite model architectures beyond these foundational techniques. Instead, it emphasizes how these models are applied in actuarial contexts via platforms like ChatGPT, GitHub Copilot, and AWS Bedrock, which abstract away the underlying model complexity.

### 2. Code Availability

No implementation code is provided in the document. The paper is conceptual and explanatory, aimed at introducing actuaries to the capabilities and considerations of Generative AI. While it references tools such as GPT-4, GitHub Copilot, and Julius.ai, it does not link to GitHub repositories or provide code snippets for replication. The authors mention using GPT-4 to assist in drafting parts of the report, but no source code for the prompts or workflows is shared.

### 3. Learning Type

The document does not specify a single learning paradigm but implies a mix of approaches depending on context:

- **Supervised Learning**: Implicitly referenced in the context of training models on labeled datasets (e.g., for classification, code generation, or text summarization).

- **Self-Supervised Learning**: Suggested in the training of large language models like GPT, which are pre-trained on vast unlabeled text corpora using next-token prediction tasks.

- **Unsupervised Learning**: Mentioned in the context of anomaly detection using VAEs and clustering algorithms for model point compression.

The paper does not discuss reinforcement learning in depth, though it references Reinforcement Learning from Human Feedback (RLHF) as a technique used to refine models like Anthropic’s Claude.

### 4. Dataset

The document does not describe any specific dataset used for training or evaluation. It discusses:

- **Real-world data**: Used in actuarial applications such as claims processing, underwriting, and scenario analysis. Examples include policyholder data, economic scenario files, claims photos, and financial reports.

- **Synthetic data**: Generated via VAEs or GANs for testing, data augmentation, or privacy-preserving analysis. The authors note that synthetic data should not be used for experience analysis as it can misrepresent statistical reliability.

- **Training data for foundational models**: Referenced as large-scale, publicly available text corpora (e.g., hundreds of billions of words for GPT-3), though exact sources are not specified. The paper warns that such data may contain limited actuarial-specific content.

No named datasets (e.g., Kaggle, UCI, proprietary insurer datasets) are mentioned.

### 5. Implementation Details

- **Programming language(s):** Not explicitly specified. The document references tools that support multiple languages (e.g., GitHub Copilot for Python, R, SQL, etc.), but no single language is emphasized.

- **Key libraries/frameworks:** Not listed. The focus is on commercial platforms (ChatGPT, AWS Bedrock, Microsoft Azure AI Studio) rather than open-source libraries like TensorFlow or PyTorch. The paper assumes users interact with these platforms via APIs or web interfaces rather than coding directly with frameworks.

### 6. Model Architecture

The document does not describe custom or composite model architectures in technical detail. Instead, it outlines standard architectures used in Generative AI:

- **Transformer-based LLMs (e.g., GPT-4)**: Described as using self-attention to process entire input sequences in parallel. No layer counts, embedding dimensions, or parameter counts are provided.

- **VAEs**: Composed of an encoder (maps input to latent space parameters — mean and variance) and a decoder (reconstructs data from sampled latent points). The latent space is probabilistic, enabling diverse generation.

- **GANs**: Two-network adversarial system — generator creates data from noise; discriminator classifies real vs. fake. Training is iterative and competitive.

The paper emphasizes architectural choices in deployment (e.g., RAG vs. fine-tuning) rather than model internals. It notes that models can be accessed via APIs or integrated into platforms like Microsoft 365 or AWS Bedrock.

### 7. Technical Content

Generative AI (GenAI) represents a paradigm shift in how actuaries can approach tasks ranging from coding and documentation to claims processing and underwriting. Unlike traditional AI, which classifies or predicts based on input data, GenAI creates new content — text, code, images, or synthetic data — by learning patterns from vast training datasets. This capability stems from advanced neural network architectures, particularly Transformers, VAEs, and GANs, which enable systems like ChatGPT and DALL-E to generate human-like outputs.

The paper is structured to guide actuaries through the conceptual foundations, practical applications, and implementation considerations of GenAI. It begins by defining key terms and techniques, then explores use cases across actuarial workflows, and concludes with a checklist for responsible deployment.

#### Foundational Techniques

At the core of GenAI are deep learning models. **Transformers**, introduced in 2017, revolutionized natural language processing by replacing sequential RNNs with parallel processing and attention mechanisms. Attention allows the model to weigh the relevance of all input tokens when generating each output token, enabling superior context understanding. This architecture underpins LLMs like GPT-3 and GPT-4, which can summarize documents, generate code, and answer complex questions.

**VAEs** offer a probabilistic approach to generation. They compress input data into a latent space — a lower-dimensional representation capturing essential features — and then decode it back to reconstruct the original. By sampling from the latent distribution, VAEs generate new, similar data. This makes them ideal for anomaly detection: data points that reconstruct poorly (high reconstruction error) are likely outliers. In actuarial contexts, VAEs can validate datasets, detect fraudulent claims patterns, or generate synthetic test data for model validation.

**GANs** consist of two networks in competition: a generator that creates synthetic data from random noise, and a discriminator that tries to distinguish real from fake. Through adversarial training, both networks improve — the generator produces more realistic outputs, and the discriminator becomes better at detection. GANs are widely used in image generation (e.g., creating realistic faces) and can augment scarce datasets in insurance, such as generating additional claims images for training computer vision models.

#### Applications for Actuaries

The paper identifies seven key areas where GenAI can enhance actuarial work:

1. **General Productivity**: Tools like Fireflies.ai or Microsoft Teams AI can transcribe meetings, summarize discussions, and extract action items, saving time on note-taking. LLMs can also summarize lengthy regulatory documents or compare draft versions of policies, though users must verify outputs due to potential hallucinations.

2. **Coding and Software Development**: Copilot tools (GitHub Copilot, Amazon CodeWhisperer) suggest code, review syntax, generate comments, and write unit tests. These tools accelerate development for both experienced coders and those with limited programming skills. Code conversion (e.g., from legacy APL to Python) is also feasible, though human oversight is needed for efficiency and copyright compliance.

3. **Model Documentation and Governance**: GenAI can automatically generate technical documentation from code, translating complex logic into plain language. When integrated via APIs with Retrieval Augmented Generation (RAG), it can produce consistent, up-to-date documentation as models evolve. This addresses a common pain point in actuarial governance, where documentation often lags behind code changes.

4. **Data Enrichment and Analysis**: GenAI can generate synthetic datasets to test model scalability or validate assumptions. For example, it can expand a 10,000-row model point file to 100,000 rows. It can also manipulate data formats — e.g., reorganizing economic scenario files from long to wide format — using natural language prompts. For analysis, VAEs can flag anomalies in claims or policy data, while LLMs can automate report generation, though actuaries must validate outputs.

5. **Scenario Analysis**: Beyond traditional sensitivity testing (e.g., +/-10% mortality), GenAI can generate complex, multi-variable scenarios by learning from historical financial data, news, and external factors. It can also propose strategic responses to emerging scenarios and rank them by feasibility, helping actuaries understand not just impacts but likelihoods and trade-offs.

6. **Automation and Efficiency**: GenAI can automate repetitive tasks like summarizing financial results or documenting data lineage. For instance, it can trace upstream and downstream dependencies in spreadsheet chains, identify redundant calculations, and suggest streamlined workflows. This reduces manual effort and minimizes errors in complex reporting processes.

7. **Claims and Underwriting**: In claims, GenAI can analyze photos to estimate damage, validate policy coverage, connect to repair shops, and track claim progress. In underwriting, it can cross-reference applicant data with public records to detect inconsistencies (e.g., birth year errors) or synthesize risk profiles for unique coverages by learning from internal and external data. For group underwriting, it can evaluate new groups against existing portfolios and suggest targeted questions.

#### Practical Implementation

Deploying GenAI requires careful consideration of architecture, data, and governance. The paper distinguishes between:

- **Platforms** (e.g., ChatGPT, AWS Bedrock) that offer pre-trained models via APIs.
- **Specialist Applications** (e.g., GitHub Copilot for coding, Julius.ai for data manipulation).
- **Custom Solutions** built around foundational models.

Key implementation strategies include:

- **Prompt Engineering**: Crafting precise, context-rich prompts to guide LLM outputs. Small changes can significantly alter results, so testing and refinement are essential. For example, prompting GPT-4 to explain a Universal Life policy yields different outputs based on specificity and constraints.

- **Transfer Learning**: Fine-tuning a pre-trained model on domain-specific data (e.g., actuarial code or claims reports) to improve performance on specialized tasks. This requires additional training data and computational resources.

- **Retrieval Augmented Generation (RAG)**: Enhancing LLMs with external, task-specific data at query time. For instance, an actuarial documentation tool might retrieve function definitions from a code library before generating explanations. RAG is preferred for dynamic, data-sensitive applications over fine-tuning.

- **Chaining**: Combining multiple AI models or steps in sequence for complex tasks (e.g., first extracting data from a PDF, then summarizing it, then generating a report). This is necessary for end-to-end actuarial workflows.

#### Limitations and Risks

The paper emphasizes that GenAI is not a replacement for actuarial judgment. Key limitations include:

- **Hallucinations**: LLMs may generate plausible but incorrect outputs. Users must verify critical results.
- **Repeatability**: Outputs can vary for identical inputs due to randomness or model updates. This challenges auditability.
- **Data Privacy**: Uploading sensitive data to public platforms risks exposure. Private deployment or RAG with on-premises data is recommended.
- **Bias**: Models trained on biased data may perpetuate or amplify those biases, especially in underwriting or claims decisions.
- **Copyright**: Training on public data raises legal questions about ownership of generated content.
- **Prompt Engineering Skill**: Poorly crafted prompts yield poor results, requiring a learning curve.

Ethical considerations include job displacement, accountability for AI-generated errors, and regulatory compliance (e.g., EU AI Act).

#### Checklist for Deployment

The paper concludes with a practical checklist for organizations:

- Define the task, required data, and user interaction.
- Choose between third-party platforms or custom solutions.
- Assess if pretrained models suffice or if fine-tuning/RAG is needed.
- Evaluate data sensitivity and ensure compliance.
- Establish quality control, review processes, and governance.
- Secure stakeholder approvals and budget for costs (API tokens, licenses, infrastructure).

In summary, GenAI offers transformative potential for actuaries — from automating mundane tasks to enhancing complex modeling and decision-making. However, its successful adoption hinges on understanding its technical foundations, limitations, and ethical implications. Actuaries must act as stewards, combining AI’s computational power with their domain expertise to ensure responsible, effective, and auditable applications. The field is evolving rapidly, and this primer serves as a starting point for ongoing exploration and innovation.

