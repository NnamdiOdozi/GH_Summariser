[digest]
output_dir = "git_summaries"
max_summaries = 20           # keep only the N most recent digest pairs; older ones are deleted
default_word_count = 750
default_max_size = 10485760  # 10MB
default_exclude_patterns = [
    # Binary/media files
    "**/*.pdf", "**/*.csv", "**/*.jpg", "**/*.jpeg", "**/*.png", "**/*.gif", "**/*.bmp", "**/*.svg", "**/*.ico",
    "**/*.webp", "**/*.avif", "**/*.jfif", "**/*.tiff", "**/*.tif", "**/*.heic", "**/*.psd",
    "**/*.mp3", "**/*.mp4", "**/*.wav",
    # Archives
    "**/*.zip", "**/*.tar", "**/*.gz", "**/*.rar", "**/*.7z",
    # Executables/libs
    "**/*.exe", "**/*.dll", "**/*.so", "**/*.bin",
    # Data files
    "**/*.dat", "**/*.db", "**/*.sqlite", "**/*.xls", "**/*.xlsx", "**/*.parquet",
    # Documents (binary formats)
    "**/*.docx", "**/*.doc", "**/*.pptx", "**/*.ppt", "**/*.odt", "**/*.odp",
    # ML model weights
    "**/*.pickle", "**/*.pkl", "**/*.h5", "**/*.hdf5", "**/*.npy", "**/*.npz",
    "**/*.pth", "**/*.pt", "**/*.onnx", "**/*.tflite", "**/*.weights",
    # Lockfiles and package manager noise
    "**/*.lock", "**/*lock.yaml", "**/package-lock.json",
    # Minified, compiled and generated files
    "**/*.min.js", "**/*.min.css", "**/*.map",
    "**/*.snap", "**/*.generated.*", "**/*.g.ts",
    # Styles (non-source)
    "**/*.css",
    # Build and cache output
    "**/dist/**", "**/build/**", "**/.next/**", "**/.turbo/**", "**/.parcel-cache/**", "**/.cache/**",
    # Python cache
    "**/__pycache__/**", "**/*.pyc",
    # i18n / translation files
    "**/locales/**", "**/translations/**",
    # Data/output directories
    "**/data*/**",
    # Package manager and virtual environment directories
    "**/node_modules/**",
    "**/.venv/**",
    "**/venv/**",
    # Logs and runtime output
    "**/*.log", "**/logs/**",
    # AI agent config/context dirs (excluded only the non-skills config; skills handled by triage)
    # NOTE: .claude/, .gemini/, .codex/ intentionally NOT excluded — they contain high-signal
    # skills/instructions and are classified into the triage 'skills' tier instead.
    # Git internals
    "**/.git/**",
    # Dev container config
    "**/.devcontainer/**",
    # Misc binary artifacts
    "**/*.stackdump",
]

[triage]
enabled = true
# token_threshold is the ONLY value that controls how much content is sent to the LLM.
# context_window values in each [llm.provider] section are purely informational (not read by code).
# Set token_threshold conservatively so no model is under context pressure when generating.
# Per-model safe values (if you want to use more of a large context window):
#   MiniMax-M2.1 (Nebius):  178000   GLM-4.7-FP8 (Nebius): 165000
#   Kimi-K2.5 (Nebius):     240000   gpt-4.1-mini (OpenAI): 100000 (tier-1 TPM rate limit)
#   Qwen3-30B (Doubleword): 240000
token_threshold = 100000

[triage.layers]
docs_contract   = true   # API specs, OpenAPI, ADRs, PRDs, requirements, schemas — highest signal docs
docs_narrative  = true   # READMEs, CONTRIBUTING, CHANGELOG, tutorials — kept but dropped before contract docs
skills          = true   # files/folders with "skill" in name (agent instructions)
build_deps      = true   # pyproject.toml, package.json, Dockerfile, requirements.txt etc.
entrypoints     = true   # main.py, app.py, server.ts, index.ts etc.
config_surfaces = true   # anything with "config" or "settings" in name, .env.example
domain_model    = true   # models/, schemas/, routes/, services/, controllers/ etc.
ci              = true   # .github/workflows/, deploy/ etc.
tests           = false  # test files — off by default (verbose, lower signal per token)

[logging]
level = "INFO"  # DEBUG, INFO, WARNING, ERROR
log_dir = "logs"
log_file = "api.log"
max_log_bytes = 150000       # rotate at ~150KB (~1000 lines at ~150 chars/line)
backup_count = 5             # keep api.log + 5 rotated files (api.log.1 … api.log.5)

[llm]
provider = "nebius"  # "doubleword", "openai", or "nebius"
frequency_penalty = 0.3
timeout = 300  # seconds

# ── Provider sections ────────────────────────────────────────────────────────
# All providers share: base_url, model, model_env, auth_env, context_window, response_format
# context_window is informational only — it is never read by code. Only token_threshold (above) controls triage.
# response_format = "json_schema" is supported by all three providers and improves output reliability.
#
# Nebius-only keys (not present on doubleword/openai — would cause API errors if passed):
#   max_output_tokens — Nebius reasoning models share the output token budget between chain-of-thought
#                       and the actual response. Without a high cap (>=8000), the response can be null.
#                       Doubleword and OpenAI are not reasoning models so the default word_count×2 suffices.
#   reasoning_effort  — Nebius-specific parameter to reduce reasoning tokens. Not supported by other providers.

[llm.doubleword]
base_url = "https://api.doubleword.ai/v1"
model = "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8"  # fallback; overridden by DOUBLEWORD_MODEL env var if set
model_env = "DOUBLEWORD_MODEL"
auth_env = "DOUBLEWORD_AUTH_TOKEN"
context_window = 262000          # informational only — not read by code
response_format = "json_schema"  # enforces structured JSON output via OpenAI SDK
use_autobatcher = false          # doubleword only: queue via Autobatcher for 50-80% cost saving; results delayed by SLA
completion_window = "1h"         # doubleword autobatcher only: "1h" or "24h" (24h is cheaper)

[llm.openai]
base_url = "https://api.openai.com/v1"
model = "gpt-4.1-mini"  # fallback; overridden by OPENAI_MODEL env var if set
model_env = "OPENAI_MODEL"
auth_env = "OPENAI_API_KEY"
context_window = 1000000         # informational only — not read by code; effective limit ~100K due to tier-1 TPM rate limit
response_format = "json_schema"  # enforces structured JSON output via OpenAI SDK

[llm.nebius]
base_url = "https://api.tokenfactory.nebius.com/v1/"
model = "MiniMaxAI/MiniMax-M2.1"  # fallback; overridden by NEBIUS_MODEL env var if set
model_env = "NEBIUS_MODEL"
auth_env = "NEBIUS_API_KEY"
context_window = 196000          # informational only — not read by code; update alongside model when switching
max_output_tokens = 8000         # reasoning model only: reserves output tokens for the response, not chain-of-thought
response_format = "json_schema"  # enforces structured JSON output via OpenAI SDK
reasoning_effort = "low"         # reasoning model only: reduces chain-of-thought tokens; values: none, minimal, low, medium, high

# Nebius model options — update model + context_window together (context_window is for reference only):
# model = "MiniMaxAI/MiniMax-M2.1"  context_window = 196000  (reasoning model)
# model = "moonshotai/Kimi-K2.5"    context_window = 256000  (reasoning model; variable latency)
# model = "zai-org/GLM-4.7-FP8"     context_window = 200000  (reasoning model; degrades above ~100K input tokens)
