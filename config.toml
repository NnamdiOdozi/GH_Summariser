[digest]
output_dir = "git_summaries"
max_summaries = 20           # keep only the N most recent digest pairs; older ones are deleted
default_word_count = 750
default_max_size = 10485760  # 10MB
default_exclude_patterns = [
    # Binary/media files
    "**/*.pdf", "**/*.csv", "**/*.jpg", "**/*.jpeg", "**/*.png", "**/*.gif", "**/*.bmp", "**/*.svg", "**/*.ico",
    "**/*.webp", "**/*.avif", "**/*.jfif", "**/*.tiff", "**/*.tif", "**/*.heic", "**/*.psd",
    "**/*.mp3", "**/*.mp4", "**/*.wav",
    # Archives
    "**/*.zip", "**/*.tar", "**/*.gz", "**/*.rar", "**/*.7z",
    # Executables/libs
    "**/*.exe", "**/*.dll", "**/*.so", "**/*.bin",
    # Data files
    "**/*.dat", "**/*.db", "**/*.sqlite", "**/*.xls", "**/*.xlsx", "**/*.parquet",
    # Documents (binary formats)
    "**/*.docx", "**/*.doc", "**/*.pptx", "**/*.ppt", "**/*.odt", "**/*.odp",
    # ML model weights
    "**/*.pickle", "**/*.pkl", "**/*.h5", "**/*.hdf5", "**/*.npy", "**/*.npz",
    "**/*.pth", "**/*.pt", "**/*.onnx", "**/*.tflite", "**/*.weights",
    # Lockfiles and package manager noise
    "**/*.lock", "**/*lock.yaml", "**/package-lock.json",
    # Minified, compiled and generated files
    "**/*.min.js", "**/*.min.css", "**/*.map",
    "**/*.snap", "**/*.generated.*", "**/*.g.ts",
    # Styles (non-source)
    "**/*.css",
    # Build and cache output
    "**/dist/**", "**/build/**", "**/.next/**", "**/.turbo/**", "**/.parcel-cache/**", "**/.cache/**",
    # Python cache
    "**/__pycache__/**", "**/*.pyc",
    # i18n / translation files
    "**/locales/**", "**/translations/**",
    # Data/output directories
    "**/data*/**",
    # Package manager and virtual environment directories
    "**/node_modules/**",
    "**/.venv/**",
    "**/venv/**",
    # Logs and runtime output
    "**/*.log", "**/logs/**",
    # AI agent config/context dirs (excluded only the non-skills config; skills handled by triage)
    # NOTE: .claude/, .gemini/, .codex/ intentionally NOT excluded — they contain high-signal
    # skills/instructions and are classified into the triage 'skills' tier instead.
    # Git internals
    "**/.git/**",
    # Dev container config
    "**/.devcontainer/**",
    # Misc binary artifacts
    "**/*.stackdump",
]

[triage]
enabled = true
token_threshold = 178000   # MiniMax-M2.1: 196k total - 339 prompt - 8000 output = 187k; -5% for tokenizer variance

[triage.layers]
docs            = true   # READMEs, CONTRIBUTING, CHANGELOG, ADRs, docs/ folders
skills          = true   # files/folders with "skill" in name (agent instructions)
build_deps      = true   # pyproject.toml, package.json, Dockerfile, requirements.txt etc.
entrypoints     = true   # main.py, app.py, server.ts, index.ts etc.
config_surfaces = true   # anything with "config" or "settings" in name, .env.example
domain_model    = true   # models/, schemas/, routes/, services/, controllers/ etc.
ci              = true   # .github/workflows/, deploy/ etc.
tests           = false  # test files — off by default (verbose, lower signal per token)

[logging]
level = "INFO"  # DEBUG, INFO, WARNING, ERROR
log_dir = "logs"
log_file = "api.log"
max_log_bytes = 150000       # rotate at ~150KB (~1000 lines at ~150 chars/line)
backup_count = 5             # keep api.log + 5 rotated files (api.log.1 … api.log.5)

[llm]
provider = "nebius"  # "doubleword", "openai", or "nebius"
frequency_penalty = 0.3
timeout = 300  # seconds

[llm.doubleword]
base_url = "https://api.doubleword.ai/v1"
model = "Qwen/Qwen3-VL-30B-A3B-Instruct-FP8"  # fallback; overridden by DOUBLEWORD_MODEL env var if set
model_env = "DOUBLEWORD_MODEL"
auth_env = "DOUBLEWORD_AUTH_TOKEN"
context_window = 262000

[llm.openai]
base_url = "https://api.openai.com/v1"
model = "gpt-4.1-mini"  # fallback; overridden by OPENAI_MODEL env var if set
model_env = "OPENAI_MODEL"
auth_env = "OPENAI_API_KEY"
context_window = 1000000

[llm.nebius]
base_url = "https://api.tokenfactory.nebius.com/v1/"
model = "MiniMaxAI/MiniMax-M2.1"  # fallback; overridden by NEBIUS_MODEL env var if set
model_env = "NEBIUS_MODEL"
auth_env = "NEBIUS_API_KEY"
context_window = 196000           # update together with model when switching
max_output_tokens = 8000         # reasoning model; safe cap for standard models too
response_format = "json_object"  # enforces valid JSON output via OpenAI SDK
reasoning_effort = "low"         # reduces reasoning tokens; values: none, minimal, low, medium, high

# Nebius model options — set model + context_window together, update token_threshold in [triage] to match:
# model = "MiniMaxAI/MiniMax-M2.1"  context_window = 196000  token_threshold = 178000  (reasoning model)
# model = "moonshotai/Kimi-K2.5"    context_window = 256000  token_threshold = 240000  (reasoning model)
# model = "zai-org/GLM-4.7-FP8"     context_window = 200000  token_threshold = 182000  (reasoning model)
